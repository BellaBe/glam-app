================================================================================
Directory Structure: /home/bellabe/glam-app/services/catalog-ai-analyzer
================================================================================

catalog-ai-analyzer/
src/
├── events/
│   ├── __init__.py
│   ├── listeners.py
│   │
│   │   ```py
│   │   # services/catalog-ai-analyzer/src/events/listeners.py
│   │   import time
│   │   from shared.messaging.listener import Listener
│   │   from shared.utils.exceptions import ValidationError
│   │   from ..schemas.events import CatalogAnalysisRequestedPayload
│   │
│   │   class CatalogAnalysisRequestedListener(Listener):
│   │       """Listen for catalog analysis requests"""
│   │
│   │       @property
│   │       def subject(self) -> str:
│   │           return "evt.catalog.ai.analysis.requested"
│   │
│   │       @property
│   │       def queue_group(self) -> str:
│   │           return "catalog-ai-analyzer-requests"
│   │
│   │       @property
│   │       def service_name(self) -> str:
│   │           return "catalog-ai-analyzer"
│   │
│   │       def __init__(self, js_client, publisher, service, logger):
│   │           super().__init__(js_client, logger)
│   │           self.publisher = publisher
│   │           self.service = service
│   │
│   │       async def on_message(self, data: dict) -> None:
│   │           """Process catalog analysis request"""
│   │           start_time = time.perf_counter()
│   │
│   │           try:
│   │               # Validate and parse payload
│   │               payload = CatalogAnalysisRequestedPayload(**data)
│   │
│   │               self.logger.info(
│   │                   f"Processing analysis request for merchant {payload.merchant_id}, "
│   │                   f"sync {payload.sync_id}, {len(payload.items)} items"
│   │               )
│   │
│   │               # Process batch
│   │               processed, failed, partial = await self.service.analyze_batch(
│   │                   merchant_id=payload.merchant_id,
│   │                   sync_id=payload.sync_id,
│   │                   correlation_id=payload.correlation_id,
│   │                   items=payload.items
│   │               )
│   │
│   │               # Publish batch completion
│   │               total_time_ms = int((time.perf_counter() - start_time) * 1000)
│   │
│   │               await self.publisher.batch_completed(
│   │                   payload={
│   │                       "merchant_id": payload.merchant_id,
│   │                       "sync_id": payload.sync_id,
│   │                       "correlation_id": payload.correlation_id,
│   │                       "processed": processed,
│   │                       "failed": failed,
│   │                       "partial": partial,
│   │                       "total_time_ms": total_time_ms
│   │                   },
│   │                   correlation_id=payload.correlation_id
│   │               )
│   │
│   │               self.logger.info(
│   │                   f"Batch completed: {processed} processed, {failed} failed, "
│   │                   f"{partial} partial in {total_time_ms}ms"
│   │               )
│   │
│   │           except ValidationError as e:
│   │               self.logger.error(f"Invalid analysis request: {e}")
│   │               # ACK invalid messages (don't retry)
│   │               return
│   │           except Exception as e:
│   │               self.logger.error(f"Analysis request processing failed: {e}", exc_info=True)
│   │               raise  # NACK for retry
│   │
│   │       async def on_error(self, error: Exception, data: dict) -> bool:
│   │           """Handle processing errors"""
│   │           if isinstance(error, ValidationError):
│   │               return True  # ACK validation errors
│   │
│   │           # Check retry count
│   │           if self.delivery_count >= self.max_deliver:
│   │               self.logger.error(f"Max retries exceeded, dropping message")
│   │               # Could send to DLQ here
│   │               return True
│   │
│   │           return False  # NACK for retry
│   │   ```
│   │
│   └── publishers.py
│
│       ```py
│       # services/catalog-ai-analyzer/src/events/publishers.py
│       from shared.messaging.publisher import Publisher
│       from ..schemas.events import (
│           CatalogAnalysisCompletedPayload,
│           CatalogBatchCompletedPayload
│       )
│
│       class CatalogAIPublisher(Publisher):
│           """Publish catalog AI analysis events"""
│
│           @property
│           def service_name(self) -> str:
│               return "catalog-ai-analyzer"
│
│           async def analysis_completed(
│               self,
│               payload: CatalogAnalysisCompletedPayload,
│               correlation_id: str
│           ) -> str:
│               """Publish individual item analysis completed"""
│               return await self.publish_event(
│                   subject="evt.catalog.ai.analysis.completed",
│                   data=payload.model_dump(mode="json"),
│                   correlation_id=correlation_id
│               )
│
│           async def batch_completed(
│               self,
│               payload: CatalogBatchCompletedPayload,
│               correlation_id: str
│           ) -> str:
│               """Publish batch processing completed"""
│               return await self.publish_event(
│                   subject="evt.catalog.ai.batch.completed",
│                   data=payload.model_dump(mode="json"),
│                   correlation_id=correlation_id
│               )
│       ```
│
├── external/
│   └── __init__.py
├── repositories/
│   ├── __init__.py
│   └── base_repository.py
├── schemas/
│   ├── __init__.py
│   ├── analysis.py
│   │
│   │   ```py
│   │   # services/catalog-ai-analyzer/src/schemas/analysis.py
│   │   from uuid import UUID
│   │   from typing import List, Optional, Dict, Any
│   │   from pydantic import BaseModel, Field
│   │   from datetime import datetime
│   │
│   │   # Analysis DTOs
│   │   class AnalysisItem(BaseModel):
│   │       """Single item for analysis"""
│   │       item_id: UUID
│   │       product_id: str  # Platform product ID (e.g., "8526062977266")
│   │       variant_id: str  # Platform variant ID (e.g., "46547096469746")
│   │       image_url: str
│   │       metadata: Optional[Dict[str, Any]] = None
│   │
│   │   class AnalysisConfig(BaseModel):
│   │       """Analysis configuration"""
│   │       enable_color: bool = True
│   │       enable_attributes: bool = True
│   │       max_colors: int = 5
│   │
│   │   class ColorResult(BaseModel):
│   │       """Color analysis result"""
│   │       name: str
│   │       confidence: float
│   │       hex: str
│   │
│   │   class PatternResult(BaseModel):
│   │       """Pattern analysis result"""
│   │       name: str
│   │       confidence: float
│   │
│   │   class AttributeResult(BaseModel):
│   │       """AI-extracted attributes"""
│   │       colors: List[ColorResult] = []
│   │       patterns: List[PatternResult] = []
│   │       styles: List[Dict[str, Any]] = []
│   │       materials: List[Dict[str, Any]] = []
│   │       season: List[str] = []
│   │       occasion: List[str] = []
│   │
│   │   class PreciseColors(BaseModel):
│   │       """MediaPipe precise color extraction"""
│   │       rgb_values: List[List[int]]
│   │       color_count: int
│   │       extraction_method: str = "mediapipe_lab_kmeans"
│   │
│   │   class AnalysisMetadata(BaseModel):
│   │       """Analysis metadata"""
│   │       analyzers_used: List[str]
│   │       quality_score: float
│   │       confidence_score: float
│   │       processing_times: Dict[str, int]
│   │
│   │   class ItemAnalysisResult(BaseModel):
│   │       """Complete analysis result for a single item"""
│   │       merchant_id: UUID
│   │       item_id: UUID
│   │       product_id: str
│   │       variant_id: str
│   │       correlation_id: str
│   │       service_version: str = "v1.0.0"
│   │       status: str  # success|partial|failed
│   │       category: Optional[str] = None
│   │       subcategory: Optional[str] = None
│   │       description: Optional[str] = None
│   │       gender: Optional[str] = None
│   │       attributes: Optional[AttributeResult] = None
│   │       precise_colors: Optional[PreciseColors] = None
│   │       analysis_metadata: AnalysisMetadata
│   │       error: Optional[str] = None
│   │   ```
│   │
│   └── events.py
│
│       ```py
│       # services/catalog-ai-analyzer/src/schemas/events.py
│       from uuid import UUID
│       from typing import List, Optional
│       from pydantic import BaseModel
│       from .analysis import AnalysisItem, AnalysisConfig, ItemAnalysisResult
│
│       # Input Events
│       class CatalogAnalysisRequestedPayload(BaseModel):
│           """evt.catalog.ai.analysis.requested payload"""
│           merchant_id: UUID
│           sync_id: UUID
│           correlation_id: str
│           items: List[AnalysisItem]
│           analysis_config: Optional[AnalysisConfig] = None
│
│       # Output Events
│       class CatalogAnalysisCompletedPayload(ItemAnalysisResult):
│           """evt.catalog.ai.analysis.completed payload"""
│           pass
│
│       class CatalogBatchCompletedPayload(BaseModel):
│           """evt.catalog.ai.batch.completed payload"""
│           merchant_id: UUID
│           sync_id: UUID
│           correlation_id: str
│           processed: int
│           failed: int
│           partial: int
│           total_time_ms: int
│       ```
│
├── services/
│   ├── __init__.py
│   ├── catalog_ai_service.py
│   │
│   │   ```py
│   │   # services/catalog-ai-analyzer/src/services/catalog_ai_service.py
│   │   import asyncio
│   │   import time
│   │   from typing import Optional, Dict
│   │   from uuid import UUID
│   │   import httpx
│   │   from shared.utils.logger import ServiceLogger
│   │   from ..config import ServiceConfig
│   │   from ..schemas.analysis import (
│   │       AnalysisItem, ItemAnalysisResult, AnalysisMetadata,
│   │       PreciseColors
│   │   )
│   │   from .mediapipe_analyzer import MediaPipeAnalyzer
│   │   from .openai_analyzer import OpenAIAnalyzer
│   │
│   │   from ..utils.image_downloader import ImageDownloader
│   │   from ..exceptions import (
│   │       ImageDownloadError,
│   │       AnalysisTimeoutError,
│   │       BothAnalyzersFailedError,
│   │       MissingProductIdentifiersError,
│   │   )
│   │
│   │   class CatalogAIService:
│   │       """Main orchestrator for catalog AI analysis"""
│   │
│   │       def __init__(
│   │           self,
│   │           config: ServiceConfig,
│   │           mediapipe: MediaPipeAnalyzer,
│   │           openai: OpenAIAnalyzer,
│   │           logger: ServiceLogger
│   │       ):
│   │           self.config = config
│   │           self.mediapipe = mediapipe
│   │           self.openai = openai
│   │           self.logger = logger
│   │
│   │           # Use the image downloader utility
│   │           self.image_downloader = ImageDownloader(
│   │               timeout=config.image_download_timeout,
│   │               max_retries=3,
│   │               logger=logger
│   │           )
│   │
│   │       async def analyze_single_item(
│   │           self,
│   │           merchant_id: UUID,
│   │           correlation_id: str,
│   │           item: AnalysisItem
│   │       ) -> ItemAnalysisResult:
│   │           """Analyze a single catalog item with proper error handling"""
│   │           start_time = time.perf_counter()
│   │           processing_times = {
│   │               "download_ms": 0,
│   │               "color_extraction_ms": 0,
│   │               "ai_analysis_ms": 0,
│   │               "total_ms": 0
│   │           }
│   │
│   │           try:
│   │               # Validate required IDs
│   │               if not item.product_id or not item.variant_id:
│   │                   raise MissingProductIdentifiersError(
│   │                       item_id=str(item.item_id)
│   │                   )
│   │
│   │               # Download image using the utility
│   │               download_start = time.perf_counter()
│   │               try:
│   │                   image_bytes = await self.image_downloader.download(item.image_url)
│   │               except Exception as e:
│   │                   raise ImageDownloadError(
│   │                       f"Failed to download image: {str(e)}",
│   │                       url=item.image_url
│   │                   )
│   │               processing_times["download_ms"] = int((time.perf_counter() - download_start) * 1000)
│   │
│   │               # Run parallel analysis with timeout
│   │               try:
│   │                   color_task = self._extract_colors_safe(image_bytes)
│   │                   ai_task = self._analyze_attributes_safe(image_bytes)
│   │
│   │                   color_result, ai_result = await asyncio.wait_for(
│   │                       asyncio.gather(color_task, ai_task),
│   │                       timeout=self.config.analysis_timeout_per_item
│   │                   )
│   │               except asyncio.TimeoutError:
│   │                   raise AnalysisTimeoutError(
│   │                       f"Analysis timeout after {self.config.analysis_timeout_per_item}s",
│   │                       timeout_seconds=self.config.analysis_timeout_per_item,
│   │                       item_id=str(item.item_id)
│   │                   )
│   │
│   │               # Check if both failed
│   │               if color_result is None and ai_result is None:
│   │                   raise BothAnalyzersFailedError(
│   │                       item_id=str(item.item_id),
│   │                       product_id=item.product_id,
│   │                       variant_id=item.variant_id
│   │                   )
│   │
│   │               # Calculate timings
│   │               processing_times["total_ms"] = int((time.perf_counter() - start_time) * 1000)
│   │
│   │               # Determine status
│   │               status = self._determine_status(color_result, ai_result)
│   │
│   │               return ItemAnalysisResult(
│   │                   merchant_id=merchant_id,
│   │                   item_id=item.item_id,
│   │                   product_id=item.product_id,
│   │                   variant_id=item.variant_id,
│   │                   correlation_id=correlation_id,
│   │                   status=status,
│   │                   category=ai_result.get("category") if ai_result else None,
│   │                   subcategory=ai_result.get("subcategory") if ai_result else None,
│   │                   description=ai_result.get("description") if ai_result else None,
│   │                   gender=ai_result.get("gender") if ai_result else None,
│   │                   attributes=ai_result.get("attributes") if ai_result else None,
│   │                   precise_colors=color_result,
│   │                   analysis_metadata=AnalysisMetadata(
│   │                       analyzers_used=self._get_analyzers_used(color_result, ai_result),
│   │                       quality_score=self._calculate_quality_score(color_result, ai_result),
│   │                       confidence_score=self._calculate_confidence_score(ai_result),
│   │                       processing_times=processing_times
│   │                   ),
│   │                   error=None if status != "failed" else "Analysis failed"
│   │               )
│   │
│   │           except Exception as e:
│   │               self.logger.error(
│   │                   f"Item analysis failed: {e}",
│   │                   extra={
│   │                       "item_id": str(item.item_id),
│   │                       "product_id": item.product_id,
│   │                       "variant_id": item.variant_id,
│   │                       "error_type": type(e).__name__
│   │                   },
│   │                   exc_info=True
│   │               )
│   │
│   │               processing_times["total_ms"] = int((time.perf_counter() - start_time) * 1000)
│   │
│   │               return ItemAnalysisResult(
│   │                   merchant_id=merchant_id,
│   │                   item_id=item.item_id,
│   │                   product_id=item.product_id,
│   │                   variant_id=item.variant_id,
│   │                   correlation_id=correlation_id,
│   │                   status="failed",
│   │                   analysis_metadata=AnalysisMetadata(
│   │                       analyzers_used=[],
│   │                       quality_score=0.0,
│   │                       confidence_score=0.0,
│   │                       processing_times=processing_times
│   │                   ),
│   │                   error=str(e)
│   │               )
│   │
│   │       async def _extract_colors_safe(self, image_bytes: bytes) -> Optional[PreciseColors]:
│   │           """Safe color extraction that returns None on error"""
│   │           try:
│   │               return await self.mediapipe.extract_colors(image_bytes)
│   │           except Exception as e:
│   │               self.logger.error(f"Color extraction failed: {e}")
│   │               return None
│   │
│   │       async def _analyze_attributes_safe(self, image_bytes: bytes) -> Optional[Dict]:
│   │           """Safe attribute analysis that returns None on error"""
│   │           try:
│   │               return await self.openai.analyze_attributes(image_bytes)
│   │           except Exception as e:
│   │               self.logger.error(f"OpenAI analysis failed: {e}")
│   │               return None
│   │
│   │       async def close(self):
│   │           """Cleanup resources"""
│   │           await self.image_downloader.close()
│   │   ```
│   │
│   ├── mediapipe_analyzer.py
│   │
│   │   ```py
│   │   # services/catalog-ai-analyzer/src/services/mediapipe_analyzer.py
│   │   import time
│   │   import cv2
│   │   import numpy as np
│   │   import mediapipe as mp
│   │   from sklearn.cluster import KMeans
│   │   from typing import Tuple, List, Optional
│   │   from shared.utils.logger import ServiceLogger
│   │   from ..config import ServiceConfig
│   │   from ..schemas.analysis import PreciseColors
│   │
│   │   class MediaPipeAnalyzer:
│   │       """MediaPipe-based color extraction from legacy code, adapted for URL-based processing"""
│   │
│   │       def __init__(self, config: ServiceConfig, logger: ServiceLogger):
│   │           self.config = config
│   │           self.logger = logger
│   │           self._segmenter = None
│   │
│   │       async def extract_colors(self, image_bytes: bytes) -> Optional[PreciseColors]:
│   │           """Extract precise colors using MediaPipe segmentation"""
│   │           try:
│   │               # Convert bytes to numpy array
│   │               nparr = np.frombuffer(image_bytes, np.uint8)
│   │               image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
│   │
│   │               if image is None:
│   │                   self.logger.error("Failed to decode image")
│   │                   return None
│   │
│   │               # Perform apparel segmentation (from legacy code)
│   │               _, _, _, bound_crop, bound_mask = self._segment_apparel(image)
│   │
│   │               if bound_crop is None or bound_mask is None:
│   │                   self.logger.warning("No apparel detected in image")
│   │                   return PreciseColors(
│   │                       rgb_values=[],
│   │                       color_count=0,
│   │                       extraction_method="mediapipe_lab_kmeans"
│   │                   )
│   │
│   │               # Extract color palette using LAB color space (from legacy)
│   │               dominant_colors = self._extract_apparel_palette_lab(
│   │                   mask=bound_mask,
│   │                   image=bound_crop,
│   │                   n_colors=self.config.default_colors,
│   │                   sample_size=self.config.sample_size,
│   │                   min_chroma=self.config.min_chroma
│   │               )
│   │
│   │               return PreciseColors(
│   │                   rgb_values=dominant_colors,
│   │                   color_count=len(dominant_colors),
│   │                   extraction_method="mediapipe_lab_kmeans"
│   │               )
│   │
│   │           except Exception as e:
│   │               self.logger.error(f"MediaPipe color extraction failed: {e}")
│   │               return None
│   │
│   │       def _get_segmenter(self):
│   │           """Create ImageSegmenter once - from legacy code"""
│   │           if self._segmenter is not None:
│   │               return self._segmenter
│   │
│   │           from mediapipe.tasks.python.core.base_options import BaseOptions
│   │           from mediapipe.tasks.python.vision.image_segmenter import (
│   │               ImageSegmenter,
│   │               ImageSegmenterOptions,
│   │               _RunningMode,
│   │           )
│   │
│   │           opts = ImageSegmenterOptions(
│   │               base_options=BaseOptions(model_asset_path=str(self.config.model_path)),
│   │               running_mode=_RunningMode.IMAGE,
│   │               output_category_mask=True,
│   │           )
│   │           self._segmenter = ImageSegmenter.create_from_options(opts)
│   │           return self._segmenter
│   │
│   │       def _segment_apparel(self, image: np.ndarray) -> Tuple:
│   │           """Run MediaPipe segmentation - from legacy code"""
│   │           segmenter = self._get_segmenter()
│   │
│   │           # Wrap NumPy BGR in mp.Image
│   │           mp_img = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)
│   │
│   │           # Run segmentation
│   │           result = segmenter.segment(mp_img)
│   │           class_mask = result.category_mask.numpy_view()
│   │
│   │           # Build binary apparel mask (class 4 = clothes/apparel)
│   │           apparel_mask = np.where(class_mask == 4, 255, 0).astype(np.uint8)
│   │
│   │           # Colored visualization mask
│   │           coloured_mask = self._make_colour_mask(class_mask)
│   │
│   │           # Raw crop
│   │           apparel_crop = cv2.bitwise_and(image, image, mask=apparel_mask)
│   │
│   │           # Find tight bounding box
│   │           ys, xs = np.where(apparel_mask == 255)
│   │           if xs.size == 0 or ys.size == 0:
│   │               return coloured_mask, apparel_mask, None, None, None
│   │
│   │           x0, x1 = xs.min(), xs.max() + 1
│   │           y0, y1 = ys.min(), ys.max() + 1
│   │
│   │           bound_crop = apparel_crop[y0:y1, x0:x1]
│   │           bound_mask = apparel_mask[y0:y1, x0:x1]
│   │
│   │           return coloured_mask, apparel_mask, apparel_crop, bound_crop, bound_mask
│   │
│   │       def _make_colour_mask(self, segment_mask: np.ndarray):
│   │           """Create colored visualization mask - from legacy code"""
│   │           lut = np.zeros((256, 1, 3), dtype=np.uint8)
│   │           lut[0] = (0, 0, 0)      # bg
│   │           lut[1] = (255, 255, 0)  # hair
│   │           lut[2] = (255, 0, 0)    # body-skin
│   │           lut[3] = (0, 0, 255)    # face-skin
│   │           lut[4] = (0, 255, 0)    # apparel/clothes
│   │           lut[5] = (255, 0, 255)  # other
│   │           return cv2.applyColorMap(segment_mask, lut)
│   │
│   │       def _extract_apparel_palette_lab(
│   │           self,
│   │           mask: np.ndarray,
│   │           image: np.ndarray,
│   │           n_colors: int = 5,
│   │           sample_size: int = 20000,
│   │           min_chroma: float = 5.0,
│   │       ) -> List[List[int]]:
│   │           """Extract color palette using LAB color space - from legacy code"""
│   │           # Get apparel pixels
│   │           coords = np.where(mask == 255)
│   │           pixels = image[coords]
│   │
│   │           if len(pixels) == 0:
│   │               return []
│   │
│   │           # Sample if too many pixels
│   │           if len(pixels) > sample_size:
│   │               idx = np.random.choice(len(pixels), sample_size, replace=False)
│   │               pixels = pixels[idx]
│   │
│   │           # Convert BGR→RGB→Lab
│   │           pixels_rgb = pixels[:, ::-1]
│   │           pixels_lab = cv2.cvtColor(
│   │               pixels_rgb.reshape(-1, 1, 3),
│   │               cv2.COLOR_RGB2LAB
│   │           ).reshape(-1, 3)
│   │
│   │           # K-Means clustering in Lab space
│   │           km = KMeans(n_clusters=n_colors * 2, n_init=8, random_state=0)
│   │           labels = km.fit_predict(pixels_lab)
│   │           centers_lab = km.cluster_centers_
│   │
│   │           # Compute counts & chroma, filter low-chroma
│   │           counts = np.bincount(labels)
│   │           chroma = np.linalg.norm(centers_lab[:, 1:], axis=1)
│   │           keep = [i for i in np.argsort(-counts) if chroma[i] >= min_chroma]
│   │
│   │           # Take top n_colors
│   │           chosen = keep[:n_colors]
│   │           palette_rgb = []
│   │           for i in chosen:
│   │               lab = np.uint8(centers_lab[i].reshape(1, 1, 3))
│   │               rgb = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB).reshape(3,)
│   │               palette_rgb.append([int(c) for c in rgb])
│   │
│   │           return palette_rgb
│   │   ```
│   │
│   └── openai_analyzer.py
│
│       ```py
│       # services/catalog-ai-analyzer/src/services/openai_analyzer.py
│       import base64
│       import json
│       import asyncio
│       from typing import Optional, Dict, Any
│       import httpx
│       from shared.utils.logger import ServiceLogger
│       from ..config import ServiceConfig
│       from ..schemas.analysis import AttributeResult, ColorResult, PatternResult
│       from ..exceptions import OpenAIAPIError, OpenAIRateLimitError
│
│       class OpenAIAnalyzer:
│           """OpenAI Vision API integration for semantic attribute analysis"""
│
│           def __init__(self, config: ServiceConfig, logger: ServiceLogger):
│               self.config = config
│               self.logger = logger
│               self.client = httpx.AsyncClient(timeout=config.openai_timeout_seconds)
│
│           async def analyze_attributes(self, image_bytes: bytes) -> Optional[Dict[str, Any]]:
│               """Analyze product attributes using OpenAI Vision API"""
│               try:
│                   base64_image = base64.b64encode(image_bytes).decode('utf-8')
│                   prompt = self._build_analysis_prompt()
│
│                   for attempt in range(self.config.openai_max_retries):
│                       try:
│                           response = await self._call_openai_api(base64_image, prompt)
│                           if response:
│                               return self._parse_openai_response(response)
│
│                       except httpx.HTTPStatusError as e:
│                           if e.response.status_code == 429:
│                               # Rate limited
│                               wait_time = 2 ** attempt
│                               if attempt == self.config.openai_max_retries - 1:
│                                   raise OpenAIRateLimitError(
│                                       retry_after=wait_time
│                                   )
│                               self.logger.warning(f"OpenAI rate limited, waiting {wait_time}s")
│                               await asyncio.sleep(wait_time)
│                           elif e.response.status_code >= 500:
│                               # Server error
│                               if attempt == self.config.openai_max_retries - 1:
│                                   raise OpenAIAPIError(
│                                       f"OpenAI server error: {e.response.status_code}",
│                                       status_code=e.response.status_code,
│                                       error_type="server_error"
│                                   )
│                               await asyncio.sleep(2 ** attempt)
│                           else:
│                               # Client error - don't retry
│                               raise OpenAIAPIError(
│                                   f"OpenAI client error: {e.response.status_code}",
│                                   status_code=e.response.status_code,
│                                   error_type="client_error"
│                               )
│                       except httpx.TimeoutException:
│                           if attempt == self.config.openai_max_retries - 1:
│                               raise OpenAIAPIError(
│                                   "OpenAI API timeout",
│                                   error_type="timeout"
│                               )
│                           await asyncio.sleep(2 ** attempt)
│
│                   return None
│
│               except (OpenAIAPIError, OpenAIRateLimitError):
│                   raise  # Re-raise our domain exceptions
│               except Exception as e:
│                   raise OpenAIAPIError(
│                       f"Unexpected OpenAI error: {str(e)}",
│                       error_type="unexpected"
│                   )
│
│           async def _call_openai_api(self, base64_image: str, prompt: str) -> Dict:
│               """Call OpenAI Vision API"""
│               headers = {
│                   "Authorization": f"Bearer {self.config.openai_api_key}",
│                   "Content-Type": "application/json"
│               }
│
│               payload = {
│                   "model": self.config.openai_model,
│                   "messages": [
│                       {
│                           "role": "user",
│                           "content": [
│                               {
│                                   "type": "text",
│                                   "text": prompt
│                               },
│                               {
│                                   "type": "image_url",
│                                   "image_url": {
│                                       "url": f"data:image/jpeg;base64,{base64_image}"
│                                   }
│                               }
│                           ]
│                       }
│                   ],
│                   "max_tokens": 1000
│               }
│
│               response = await self.client.post(
│                   "https://api.openai.com/v1/chat/completions",
│                   json=payload,
│                   headers=headers
│               )
│               response.raise_for_status()
│
│               return response.json()
│
│           def _build_analysis_prompt(self) -> str:
│               """Build structured prompt for OpenAI Vision"""
│               return """Analyze this product image and return a JSON object with the following structure:
│               {
│                   "category": "main product category (e.g., shirts, dresses, shoes)",
│                   "subcategory": "specific subcategory (e.g., casual-shirts, evening-dresses)",
│                   "description": "brief product description (max 100 chars)",
│                   "gender": "male|female|unisex",
│                   "attributes": {
│                       "colors": [
│                           {"name": "color name", "confidence": 0.0-1.0}
│                       ],
│                       "patterns": [
│                           {"name": "pattern name", "confidence": 0.0-1.0}
│                       ],
│                       "styles": [
│                           {"name": "style name", "confidence": 0.0-1.0}
│                       ],
│                       "materials": [
│                           {"name": "material name", "confidence": 0.0-1.0}
│                       ],
│                       "season": ["spring", "summer", "fall", "winter"],
│                       "occasion": ["casual", "formal", "business", "sport", "evening"]
│                   }
│               }
│
│               Focus on fashion/apparel attributes. Be precise and confident.
│               Return ONLY valid JSON, no additional text."""
│
│           def _parse_openai_response(self, response: Dict) -> Optional[Dict[str, Any]]:
│               """Parse OpenAI response and extract structured data"""
│               try:
│                   content = response['choices'][0]['message']['content']
│
│                   # Clean up response (remove markdown if present)
│                   content = content.strip()
│                   if content.startswith("```json"):
│                       content = content[7:]
│                   if content.endswith("```"):
│                       content = content[:-3]
│
│                   # Parse JSON
│                   data = json.loads(content.strip())
│
│                   # Convert to our schema
│                   return {
│                       "category": data.get("category"),
│                       "subcategory": data.get("subcategory"),
│                       "description": data.get("description"),
│                       "gender": data.get("gender"),
│                       "attributes": AttributeResult(
│                           colors=[ColorResult(**c) for c in data.get("attributes", {}).get("colors", [])],
│                           patterns=[PatternResult(**p) for p in data.get("attributes", {}).get("patterns", [])],
│                           styles=[{"name": s["name"], "confidence": s["confidence"]}
│                                   for s in data.get("attributes", {}).get("styles", [])],
│                           materials=[{"name": m["name"], "confidence": m["confidence"]}
│                                     for m in data.get("attributes", {}).get("materials", [])],
│                           season=data.get("attributes", {}).get("season", []),
│                           occasion=data.get("attributes", {}).get("occasion", [])
│                       )
│                   }
│
│               except Exception as e:
│                   self.logger.error(f"Failed to parse OpenAI response: {e}")
│                   return None
│
│           async def close(self):
│               """Close HTTP client"""
│               await self.client.aclose()
│       ```
│
├── utils/
│   └── image_downloader.py
│
│       ```py
│       # services/catalog-ai-analyzer/src/utils/image_downloader.py
│       import asyncio
│       from typing import Optional
│       import httpx
│       from shared.utils.logger import ServiceLogger
│       from shared.utils.exceptions import ValidationError, RequestTimeoutError, InfrastructureError
│
│       class ImageDownloader:
│           """Utility for downloading and validating product images"""
│
│           # Supported image formats
│           SUPPORTED_FORMATS = {
│               'image/jpeg', 'image/jpg', 'image/png',
│               'image/webp', 'image/gif', 'image/bmp'
│           }
│
│           # Maximum file size (10MB)
│           MAX_SIZE_BYTES = 10 * 1024 * 1024
│
│           def __init__(
│               self,
│               timeout: int = 10,
│               max_retries: int = 3,
│               logger: Optional[ServiceLogger] = None
│           ):
│               self.timeout = timeout
│               self.max_retries = max_retries
│               self.logger = logger
│               self.client = httpx.AsyncClient(
│                   timeout=httpx.Timeout(timeout),
│                   follow_redirects=True,
│                   limits=httpx.Limits(max_keepalive_connections=10)
│               )
│
│           async def download(self, url: str) -> bytes:
│               """
│               Download image with validation and retry logic
│
│               Args:
│                   url: Image URL to download
│
│               Returns:
│                   Image bytes
│
│               Raises:
│                   ValidationError: Invalid URL or image format
│                   RequestTimeoutError: Download timeout
│                   InfrastructureError: Network/server errors
│               """
│               if not url or not url.startswith(('http://', 'https://')):
│                   raise ValidationError(
│                       message="Invalid image URL",
│                       field="image_url",
│                       value=url
│                   )
│
│               last_error = None
│
│               for attempt in range(self.max_retries):
│                   try:
│                       if self.logger:
│                           self.logger.debug(f"Downloading image (attempt {attempt + 1}): {url}")
│
│                       # Make request
│                       response = await self.client.get(url)
│
│                       # Check status
│                       if response.status_code == 404:
│                           raise ValidationError(
│                               message=f"Image not found: {url}",
│                               field="image_url",
│                               value=url
│                           )
│
│                       response.raise_for_status()
│
│                       # Validate content type
│                       content_type = response.headers.get('content-type', '').lower()
│                       if content_type and not any(fmt in content_type for fmt in self.SUPPORTED_FORMATS):
│                           raise ValidationError(
│                               message=f"Unsupported image format: {content_type}",
│                               field="content_type",
│                               value=content_type
│                           )
│
│                       # Check size
│                       content_length = response.headers.get('content-length')
│                       if content_length and int(content_length) > self.MAX_SIZE_BYTES:
│                           raise ValidationError(
│                               message=f"Image too large: {int(content_length)} bytes (max {self.MAX_SIZE_BYTES})",
│                               field="content_length",
│                               value=content_length
│                           )
│
│                       # Get content
│                       content = response.content
│
│                       # Validate actual size
│                       if len(content) > self.MAX_SIZE_BYTES:
│                           raise ValidationError(
│                               message=f"Image too large: {len(content)} bytes",
│                               field="image_size",
│                               value=len(content)
│                           )
│
│                       # Validate it's actually an image (check magic bytes)
│                       if not self._validate_image_bytes(content):
│                           raise ValidationError(
│                               message="Invalid image data",
│                               field="image_data"
│                           )
│
│                       if self.logger:
│                           self.logger.debug(f"Successfully downloaded {len(content)} bytes from {url}")
│
│                       return content
│
│                   except httpx.TimeoutException as e:
│                       last_error = RequestTimeoutError(
│                           message=f"Image download timeout after {self.timeout}s",
│                           timeout_seconds=self.timeout,
│                           operation="image_download"
│                       )
│                       if attempt < self.max_retries - 1:
│                           await asyncio.sleep(2 ** attempt)  # Exponential backoff
│
│                   except httpx.HTTPStatusError as e:
│                       if e.response.status_code >= 500:
│                           last_error = InfrastructureError(
│                               message=f"Server error downloading image: {e.response.status_code}",
│                               service="image_server",
│                               retryable=True
│                           )
│                           if attempt < self.max_retries - 1:
│                               await asyncio.sleep(2 ** attempt)
│                       else:
│                           raise ValidationError(
│                               message=f"Failed to download image: HTTP {e.response.status_code}",
│                               field="image_url",
│                               value=url
│                           )
│
│                   except httpx.RequestError as e:
│                       last_error = InfrastructureError(
│                           message=f"Network error downloading image: {str(e)}",
│                           service="network",
│                           retryable=True
│                       )
│                       if attempt < self.max_retries - 1:
│                           await asyncio.sleep(2 ** attempt)
│
│                   except ValidationError:
│                       raise  # Don't retry validation errors
│
│                   except Exception as e:
│                       last_error = InfrastructureError(
│                           message=f"Unexpected error downloading image: {str(e)}",
│                           service="image_download"
│                       )
│                       if self.logger:
│                           self.logger.error(f"Unexpected download error: {e}", exc_info=True)
│                       break
│
│               # All retries exhausted
│               if last_error:
│                   raise last_error
│
│               raise InfrastructureError(
│                   message="Failed to download image after all retries",
│                   service="image_download"
│               )
│
│           def _validate_image_bytes(self, data: bytes) -> bool:
│               """Validate image data by checking magic bytes"""
│               if len(data) < 8:
│                   return False
│
│               # Check common image format signatures
│               signatures = {
│                   b'\xff\xd8\xff': 'jpeg',  # JPEG
│                   b'\x89PNG\r\n\x1a\n': 'png',  # PNG
│                   b'GIF87a': 'gif',  # GIF87a
│                   b'GIF89a': 'gif',  # GIF89a
│                   b'RIFF': 'webp',  # WebP (need more checks)
│                   b'BM': 'bmp',  # BMP
│               }
│
│               for signature, format_name in signatures.items():
│                   if data.startswith(signature):
│                       # Special case for WebP
│                       if format_name == 'webp' and len(data) > 12:
│                           return data[8:12] == b'WEBP'
│                       return True
│
│               return False
│
│           async def download_batch(
│               self,
│               urls: list[str],
│               max_concurrent: int = 5
│           ) -> list[tuple[str, Optional[bytes], Optional[Exception]]]:
│               """
│               Download multiple images concurrently
│
│               Returns:
│                   List of (url, image_bytes, error) tuples
│               """
│               results = []
│
│               for chunk in self._chunk_urls(urls, max_concurrent):
│                   tasks = [self._download_safe(url) for url in chunk]
│                   chunk_results = await asyncio.gather(*tasks)
│                   results.extend(chunk_results)
│
│               return results
│
│           async def _download_safe(self, url: str) -> tuple[str, Optional[bytes], Optional[Exception]]:
│               """Safe download that returns error instead of raising"""
│               try:
│                   image_bytes = await self.download(url)
│                   return (url, image_bytes, None)
│               except Exception as e:
│                   return (url, None, e)
│
│           def _chunk_urls(self, urls: list[str], chunk_size: int):
│               """Split URLs into chunks"""
│               for i in range(0, len(urls), chunk_size):
│                   yield urls[i:i + chunk_size]
│
│           async def close(self):
│               """Close HTTP client"""
│               await self.client.aclose()
│       ```
│
├── __init__.py
├── config.py
│
│   ```py
│   # services/catalog-ai-analyzer/src/config.py
│   import os
│   from functools import lru_cache
│   from pathlib import Path
│   from pydantic import BaseModel, Field, ConfigDict, model_validator
│   from shared.utils import load_root_env, ConfigurationError
│
│   class ServiceConfig(BaseModel):
│       """Catalog AI Analyzer service configuration"""
│       model_config = ConfigDict(
│           extra="ignore",
│           case_sensitive=False,
│           populate_by_name=True,
│       )
│
│       # Service identification
│       service_name: str = "catalog-ai-analyzer"
│       service_version: str = "v1.0.0"
│       service_description: str = "AI-powered catalog item analysis with MediaPipe and OpenAI Vision"
│       debug: bool = Field(default=False, alias="DEBUG")
│
│       # Required environment variables
│       environment: str = Field(..., alias="APP_ENV")
│       api_external_port: int = Field(default=8123, alias="CATALOG_AI_API_EXTERNAL_PORT")
│
│       # No database for this service (stateless)
│       database_enabled: bool = False
│
│       # Required secrets (from .env)
│       client_jwt_secret: str = Field(..., alias="CLIENT_JWT_SECRET")
│       internal_jwt_secret: str = Field(..., alias="INTERNAL_JWT_SECRET")
│       internal_api_keys: str = Field(..., alias="INTERNAL_API_KEYS")
│
│       # OpenAI Configuration
│       openai_api_key: str = Field(..., alias="OPENAI_API_KEY")
│       openai_model: str = Field(default="gpt-4-vision-preview", alias="OPENAI_VISION_MODEL")
│       openai_max_retries: int = Field(default=3, alias="OPENAI_MAX_RETRIES")
│       openai_timeout_seconds: int = Field(default=30, alias="OPENAI_TIMEOUT_SECONDS")
│
│       # MediaPipe Configuration
│       model_path: str = Field(
│           default="models/selfie_multiclass_256x256.tflite",
│           alias="MEDIAPIPE_MODEL_PATH"
│       )
│
│       # Processing Configuration
│       max_concurrent_items: int = Field(default=5, alias="MAX_CONCURRENT_ITEMS")
│       max_batch_size: int = Field(default=20, alias="MAX_BATCH_SIZE")
│       image_download_timeout: int = Field(default=10, alias="IMAGE_DOWNLOAD_TIMEOUT")
│       analysis_timeout_per_item: int = Field(default=30, alias="ANALYSIS_TIMEOUT_PER_ITEM")
│
│       # Color extraction settings (from legacy)
│       default_colors: int = Field(default=5, alias="DEFAULT_COLORS")
│       sample_size: int = Field(default=20000, alias="COLOR_SAMPLE_SIZE")
│       min_chroma: float = Field(default=5.0, alias="MIN_CHROMA")
│
│       # API configuration
│       api_host: str = "0.0.0.0"
│
│       # Logging
│       logging_level: str = "INFO"
│       logging_format: str = "json"
│
│       @property
│       def nats_url(self) -> str:
│           """NATS URL for event system"""
│           in_container = os.path.exists("/.dockerenv")
│           if in_container or self.environment in ["development", "production"]:
│               return "nats://nats:4222"
│           return "nats://localhost:4222"
│
│       @property
│       def api_port(self) -> int:
│           """Port based on environment"""
│           in_container = os.path.exists("/.dockerenv")
│           return 8000 if in_container else self.api_external_port
│
│       @model_validator(mode="after")
│       def validate_config(self):
│           # Validate model file exists
│           model_path = Path(self.model_path)
│           if not model_path.is_file():
│               raise ValueError(f"Model file missing at {model_path}")
│           return self
│
│   @lru_cache
│   def get_service_config() -> ServiceConfig:
│       """Load configuration once"""
│       try:
│           load_root_env()
│           return ServiceConfig(**os.environ)
│       except Exception as e:
│           raise ConfigurationError(
│               f"Failed to load config: {e}",
│               config_key="catalog-ai-analyzer"
│           )
│   ```
│
├── dependencies.py
├── exceptions.py
│
│   ```py
│   # services/catalog-ai-analyzer/src/exceptions.py
│   from typing import Optional, Any, Dict
│   from shared.utils.exceptions import (
│       DomainError,
│       InfrastructureError,
│       ValidationError
│   )
│
│   # ===============================
│   # Domain Errors (Business Logic)
│   # ===============================
│
│   class CatalogAnalysisError(DomainError):
│       """Base error for catalog analysis domain"""
│       code = "CATALOG_ANALYSIS_ERROR"
│
│   class ImageAnalysisError(CatalogAnalysisError):
│       """Error during image analysis"""
│       code = "CAT_IMG_ANALYSIS_FAILED"
│
│       def __init__(
│           self,
│           message: str,
│           *,
│           item_id: Optional[str] = None,
│           product_id: Optional[str] = None,
│           variant_id: Optional[str] = None,
│           analyzer: Optional[str] = None,
│           **kwargs
│       ):
│           super().__init__(message, **kwargs)
│           if item_id:
│               self.details["item_id"] = item_id
│           if product_id:
│               self.details["product_id"] = product_id
│           if variant_id:
│               self.details["variant_id"] = variant_id
│           if analyzer:
│               self.details["analyzer"] = analyzer
│
│   class ImageDownloadError(CatalogAnalysisError):
│       """Failed to download image"""
│       code = "CAT_IMG_DOWNLOAD_FAILED"
│
│       def __init__(
│           self,
│           message: str,
│           *,
│           url: Optional[str] = None,
│           status_code: Optional[int] = None,
│           **kwargs
│       ):
│           super().__init__(message, **kwargs)
│           if url:
│               self.details["url"] = url
│           if status_code:
│               self.details["status_code"] = status_code
│
│   class InvalidImageFormatError(ValidationError):
│       """Invalid image format"""
│       code = "CAT_IMG_INVALID_FORMAT"
│
│       def __init__(
│           self,
│           message: str,
│           *,
│           format_received: Optional[str] = None,
│           formats_supported: Optional[list] = None,
│           **kwargs
│       ):
│           super().__init__(message, **kwargs)
│           if format_received:
│               self.details["format_received"] = format_received
│           if formats_supported:
│               self.details["formats_supported"] = formats_supported
│
│   class ColorExtractionError(ImageAnalysisError):
│       """MediaPipe color extraction failed"""
│       code = "CAT_COLOR_EXTRACTION_FAILED"
│
│       def __init__(
│           self,
│           message: str = "Failed to extract colors from image",
│           **kwargs
│       ):
│           super().__init__(message, analyzer="mediapipe", **kwargs)
│
│   class MissingProductIdentifiersError(ValidationError):
│       """Missing required product/variant IDs"""
│       code = "CAT_MISSING_IDS"
│
│       def __init__(
│           self,
│           message: str = "Missing product_id or variant_id",
│           *,
│           item_id: Optional[str] = None,
│           **kwargs
│       ):
│           super().__init__(message, **kwargs)
│           if item_id:
│               self.details["item_id"] = item_id
│
│   class BatchSizeExceededError(ValidationError):
│       """Batch size exceeds maximum allowed"""
│       code = "CAT_BATCH_SIZE_EXCEEDED"
│
│       def __init__(
│           self,
│           message: str,
│           *,
│           batch_size: int,
│           max_size: int,
│           **kwargs
│       ):
│           super().__init__(message, **kwargs)
│           self.details["batch_size"] = batch_size
│           self.details["max_size"] = max_size
│
│   # ===============================
│   # Infrastructure Errors
│   # ===============================
│
│   class OpenAIAPIError(InfrastructureError):
│       """OpenAI API error"""
│       code = "CAT_OPENAI_API_ERROR"
│
│       def __init__(
│           self,
│           message: str,
│           *,
│           status_code: Optional[int] = None,
│           error_type: Optional[str] = None,
│           **kwargs
│       ):
│           super().__init__(message, service="openai", **kwargs)
│           if status_code:
│               self.details["status_code"] = status_code
│           if error_type:
│               self.details["error_type"] = error_type
│
│   class OpenAIRateLimitError(OpenAIAPIError):
│       """OpenAI rate limit exceeded"""
│       code = "CAT_OPENAI_RATE_LIMITED"
│
│       def __init__(
│           self,
│           message: str = "OpenAI API rate limit exceeded",
│           *,
│           retry_after: Optional[int] = None,
│           **kwargs
│       ):
│           super().__init__(message, retryable=True, **kwargs)
│           if retry_after:
│               self.details["retry_after"] = retry_after
│
│   class AnalysisTimeoutError(InfrastructureError):
│       """Analysis timeout"""
│       code = "CAT_ANALYSIS_TIMEOUT"
│
│       def __init__(
│           self,
│           message: str,
│           *,
│           timeout_seconds: int,
│           item_id: Optional[str] = None,
│           **kwargs
│       ):
│           super().__init__(
│               message,
│               service="analyzer",
│               retryable=True,
│               timeout_seconds=timeout_seconds,
│               **kwargs
│           )
│           if item_id:
│               self.details["item_id"] = item_id
│
│   class BothAnalyzersFailedError(ImageAnalysisError):
│       """Both MediaPipe and OpenAI analyzers failed"""
│       code = "CAT_BOTH_ANALYZERS_FAILED"
│
│       def __init__(
│           self,
│           message: str = "Complete analysis failure - both analyzers failed",
│           *,
│           mediapipe_error: Optional[str] = None,
│           openai_error: Optional[str] = None,
│           **kwargs
│       ):
│           super().__init__(message, **kwargs)
│           self.details["failures"] = {}
│           if mediapipe_error:
│               self.details["failures"]["mediapipe"] = mediapipe_error
│           if openai_error:
│               self.details["failures"]["openai"] = openai_error
│
│   # ===============================
│   # Utility Functions
│   # ===============================
│
│   def is_retryable_error(error: Exception) -> bool:
│       """Check if an error should be retried"""
│       if isinstance(error, InfrastructureError):
│           return error.retryable
│
│       # Specific error types that should be retried
│       retryable_types = (
│           OpenAIRateLimitError,
│           AnalysisTimeoutError,
│           ImageDownloadError,
│       )
│
│       return isinstance(error, retryable_types)
│
│   def get_error_code(error: Exception) -> str:
│       """Extract error code from exception"""
│       if hasattr(error, 'code'):
│           return error.code
│       return "UNKNOWN_ERROR"
│   ```
│
├── lifecycle.py
│
│   ```py
│   # services/catalog-ai-analyzer/src/lifecycle.py
│   from typing import Optional, List
│   import asyncio
│   from shared.messaging.jetstream_client import JetStreamClient
│   from shared.utils.logger import ServiceLogger
│   from .config import ServiceConfig
│   from .services.catalog_ai_service import CatalogAIService
│   from .services.mediapipe_analyzer import MediaPipeAnalyzer
│   from .services.openai_analyzer import OpenAIAnalyzer
│   from .events.publishers import CatalogAIPublisher
│   from .events.listeners import CatalogAnalysisRequestedListener
│
│   class ServiceLifecycle:
│       """Manages catalog AI analyzer service lifecycle"""
│
│       def __init__(self, config: ServiceConfig, logger: ServiceLogger):
│           self.config = config
│           self.logger = logger
│
│           # Connections
│           self.messaging_client: Optional[JetStreamClient] = None
│
│           # Components
│           self.mediapipe_analyzer: Optional[MediaPipeAnalyzer] = None
│           self.openai_analyzer: Optional[OpenAIAnalyzer] = None
│           self.catalog_ai_service: Optional[CatalogAIService] = None
│           self.event_publisher: Optional[CatalogAIPublisher] = None
│
│           # Listeners
│           self._listeners: List = []
│
│       async def startup(self) -> None:
│           """Initialize all components"""
│           try:
│               self.logger.info("Starting catalog AI analyzer service...")
│
│               # 1. Messaging
│               await self._init_messaging()
│
│               # 2. Analyzers
│               self._init_analyzers()
│
│               # 3. Main service
│               self._init_service()
│
│               # 4. Event listeners
│               await self._init_listeners()
│
│               self.logger.info("Catalog AI analyzer service started successfully")
│
│           except Exception as e:
│               self.logger.critical("Service startup failed", exc_info=True)
│               await self.shutdown()
│               raise
│
│       async def shutdown(self) -> None:
│           """Graceful shutdown"""
│           self.logger.info("Shutting down catalog AI analyzer service")
│
│           # Stop listeners
│           for listener in self._listeners:
│               try:
│                   await listener.stop()
│               except Exception:
│                   self.logger.error("Listener stop failed", exc_info=True)
│
│           # Close analyzers
│           if self.openai_analyzer:
│               await self.openai_analyzer.close()
│
│           if self.catalog_ai_service:
│               await self.catalog_ai_service.close()
│
│           # Close messaging
│           if self.messaging_client:
│               try:
│                   await self.messaging_client.close()
│               except Exception:
│                   self.logger.error("Messaging close failed", exc_info=True)
│
│           self.logger.info("Shutdown complete")
│
│       async def _init_messaging(self) -> None:
│           """Initialize NATS/JetStream"""
│           self.messaging_client = JetStreamClient(self.logger)
│           await self.messaging_client.connect([self.config.nats_url])
│           await self.messaging_client.ensure_stream("GLAM_EVENTS", ["evt.*", "cmd.*"])
│
│           # Initialize publisher
│           self.event_publisher = CatalogAIPublisher(
│               jetstream_client=self.messaging_client,
│               logger=self.logger
│           )
│
│           self.logger.info("Messaging initialized")
│
│       def _init_analyzers(self) -> None:
│           """Initialize MediaPipe and OpenAI analyzers"""
│           self.mediapipe_analyzer = MediaPipeAnalyzer(
│               config=self.config,
│               logger=self.logger
│           )
│
│           self.openai_analyzer = OpenAIAnalyzer(
│               config=self.config,
│               logger=self.logger
│           )
│
│           self.logger.info("Analyzers initialized")
│
│       def _init_service(self) -> None:
│           """Initialize main service"""
│           self.catalog_ai_service = CatalogAIService(
│               config=self.config,
│               mediapipe=self.mediapipe_analyzer,
│               openai=self.openai_analyzer,
│               logger=self.logger
│           )
│
│           # Wrap service to publish events after each item
│           original_analyze = self.catalog_ai_service.analyze_single_item
│
│           async def wrapped_analyze(merchant_id, correlation_id, item):
│               result = await original_analyze(merchant_id, correlation_id, item)
│
│               # Publish analysis completed event
│               await self.event_publisher.analysis_completed(
│                   payload=result,
│                   correlation_id=correlation_id
│               )
│
│               return result
│
│           self.catalog_ai_service.analyze_single_item = wrapped_analyze
│
│           self.logger.info("Catalog AI service initialized")
│
│       async def _init_listeners(self) -> None:
│           """Initialize event listeners"""
│           listener = CatalogAnalysisRequestedListener(
│               js_client=self.messaging_client,
│               publisher=self.event_publisher,
│               service=self.catalog_ai_service,
│               logger=self.logger
│           )
│
│           await listener.start()
│           self._listeners.append(listener)
│
│           self.logger.info("Event listeners started")
│   ```
│
└── main.py

    ```py
    # services/catalog-ai-analyzer/src/main.py
    from contextlib import asynccontextmanager
    from fastapi import FastAPI
    from shared.api import setup_middleware, create_health_router
    from shared.utils import create_logger
    from .config import get_service_config
    from .lifecycle import ServiceLifecycle

    # Create singletons
    config = get_service_config()
    logger = create_logger(config.service_name)
    lifecycle = ServiceLifecycle(config, logger)

    @asynccontextmanager
    async def lifespan(app: FastAPI):
        """Lifespan management"""
        app.state.lifecycle = lifecycle
        app.state.config = config
        app.state.logger = logger  # REQUIRED for middleware

        try:
            await lifecycle.startup()
            yield
        finally:
            await lifecycle.shutdown()

    def create_application() -> FastAPI:
        """Create FastAPI app"""
        app = FastAPI(
            title=config.service_name,
            version=config.service_version,
            description=config.service_description,
            lifespan=lifespan
        )

        # Setup shared middleware
        setup_middleware(app, service_name=config.service_name)

        # Add health check
        app.include_router(create_health_router(config.service_name))

        return app

    app = create_application()

    if __name__ == "__main__":
        import uvicorn
        uvicorn.run(
            "src.main:app",
            host=config.api_host,
            port=config.api_port,
            reload=config.debug
        )
    ```

tests/
├── integration/
│   ├── __init__.py
│   └── test_api.py
├── unit/
│   ├── __init__.py
│   └── test_service.py
├── __init__.py
└── conftest.py
Dockerfile

```
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Install Poetry
RUN pip install poetry

# Copy dependency files
COPY pyproject.toml poetry.lock* ./
COPY ../shared /shared

# Install dependencies
RUN poetry config virtualenvs.create false \
    && poetry install --no-interaction --no-ansi

# Copy application code
COPY . .

# Generate Prisma client
RUN poetry run prisma generate

EXPOSE 8000

CMD ["poetry", "run", "uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

poetry.lock
poetry.toml

```toml
[virtualenvs]
in-project = true
```

pyproject.toml

```toml
[tool.poetry]
name = "catalog-ai-analyzer"
version = "0.1.0"
description = "catalog-ai-analyzer microservice for GLAM platform"
authors = ["GLAM Team"]
readme = "README.md"
package-mode = false


[tool.poetry.dependencies]
python = "^3.11"


[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```


================================================================================
Output includes file contents
================================================================================
