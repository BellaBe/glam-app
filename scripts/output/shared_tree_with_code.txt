================================================================================
Directory Structure: /home/bellabe/glam-app/shared
================================================================================

shared/
shared/
├── api/
│   ├── __init__.py
│   │   
│   │   ```py
│   │   # -------------------------------
│   │   # shared/api/__init__.py
│   │   # -------------------------------
│   │   
│   │   """
│   │   Unified API response models and utilities for glam-app microservices.
│   │   
│   │   This module provides a single, consistent approach to API responses
│   │   across all services.
│   │   """
│   │   
│   │   from .models import (
│   │       # Core models
│   │       ApiResponse,
│   │       Meta,
│   │       Pagination,
│   │       Links,
│   │       ErrorDetail,
│   │       T,  # Generic type
│   │   )
│   │   
│   │   from .responses import (
│   │       # Response helpers
│   │       create_response,
│   │       success_response,
│   │       error_response,
│   │       paginated_response,
│   │   )
│   │   
│   │   from .dependencies import (
│   │       # FastAPI dependencies
│   │       PaginationDep,
│   │       RequestContextDep,
│   │       CorrelationIdDep,  # Re-exported from correlation
│   │   )
│   │   
│   │   from .middleware import (
│   │       # Middleware
│   │       APIMiddleware,
│   │       setup_middleware,
│   │   )
│   │   
│   │   from .correlation import (
│   │       # Correlation utilities
│   │       get_correlation_id,
│   │       set_correlation_context,
│   │       get_correlation_context,
│   │       add_correlation_header,
│   │       add_correlation_to_event,
│   │       extract_correlation_from_event,
│   │       
│   │   )
│   │   from .tracing import (
│   │       set_trace_context,
│   │       get_trace_context,
│   │       TracingMiddleware,
│   │   )   
│   │   
│   │   __all__ = [
│   │       # Models
│   │       "ApiResponse",
│   │       "Meta",
│   │       "Pagination",
│   │       "Links",
│   │       "ErrorDetail",
│   │       "T",
│   │       
│   │       # Response helpers
│   │       "create_response",
│   │       "success_response",
│   │       "error_response",
│   │       "paginated_response",
│   │       
│   │       # Dependencies
│   │       "PaginationDep",
│   │       "RequestContextDep",
│   │       "CorrelationIdDep",
│   │       
│   │       # Correlation
│   │       "get_correlation_id",
│   │       "set_correlation_context",
│   │       "get_correlation_context",
│   │       "add_correlation_header",
│   │       "add_correlation_to_event",
│   │       "extract_correlation_from_event",
│   │       # Tracing
│   │       "set_trace_context",
│   │       "get_trace_context",
│   │       "TracingMiddleware",
│   │       
│   │       # Middleware
│   │       "APIMiddleware",
│   │       "setup_middleware",
│   │   ]
│   │   ```
│   │   
│   ├── correlation.py
│   │   
│   │   ```py
│   │   # File: shared/api/correlation.py
│   │   
│   │   """
│   │   Simplified correlation ID support for distributed tracing.
│   │   
│   │   Focuses on the essential functionality needed for request tracing
│   │   across services without over-engineering.
│   │   """
│   │   
│   │   from typing import Optional, Annotated
│   │   from contextvars import ContextVar
│   │   from fastapi import Request, Depends
│   │   import uuid
│   │   
│   │   # Context variable for async operations
│   │   _correlation_context: ContextVar[Optional[str]] = ContextVar(
│   │       "correlation_id", default=None
│   │   )
│   │   
│   │   
│   │   def get_correlation_id(request: Request) -> str:
│   │       """
│   │       Get or generate correlation ID for the current request.
│   │   
│   │       Priority:
│   │       1. Request state (set by middleware)
│   │       2. X-Correlation-ID header (from upstream service)
│   │       3. Generate new one (originating request)
│   │       """
│   │       # Check request state first
│   │       if hasattr(request.state, "correlation_id"):
│   │           return request.state.correlation_id
│   │   
│   │       # Check headers from upstream service
│   │       correlation_id = request.headers.get("X-Correlation-ID")
│   │       if correlation_id:
│   │           return correlation_id
│   │   
│   │       # Generate new one
│   │       return f"corr_{uuid.uuid4().hex[:12]}"
│   │   
│   │   
│   │   # FastAPI dependency
│   │   CorrelationIdDep = Annotated[str, Depends(get_correlation_id)]
│   │   
│   │   
│   │   def set_correlation_context(correlation_id: str) -> None:
│   │       """Set correlation ID in async context."""
│   │       _correlation_context.set(correlation_id)
│   │   
│   │   
│   │   def get_correlation_context() -> Optional[str]:
│   │       """Get correlation ID from async context."""
│   │       return _correlation_context.get()
│   │   
│   │   
│   │   # Essential integrations only
│   │   
│   │   
│   │   def add_correlation_header(headers: dict) -> dict:
│   │       """
│   │       Add correlation ID to outgoing HTTP headers.
│   │   
│   │       Usage:
│   │           headers = add_correlation_header({"Content-Type": "application/json"})
│   │           response = await client.get(url, headers=headers)
│   │       """
│   │       correlation_id = get_correlation_context()
│   │       if correlation_id:
│   │           headers["X-Correlation-ID"] = correlation_id
│   │       return headers
│   │   
│   │   
│   │   def add_correlation_to_event(event_data: dict) -> dict:
│   │       """
│   │       Add correlation ID to message bus events.
│   │   
│   │       Usage:
│   │           event_data = {"subject": "ORDER_CREATED", "data": {...}}
│   │           event_with_correlation = add_correlation_to_event(event_data)
│   │       """
│   │       correlation_id = get_correlation_context()
│   │       if correlation_id:
│   │           if "metadata" not in event_data:
│   │               event_data["metadata"] = {}
│   │           event_data["metadata"]["correlation_id"] = correlation_id
│   │       return event_data
│   │   
│   │   
│   │   def extract_correlation_from_event(event_data: dict) -> Optional[str]:
│   │       """Extract correlation ID from event data."""
│   │       return event_data.get("metadata", {}).get("correlation_id")
│   │   ```
│   │   
│   ├── dependencies.py
│   │   
│   │   ```py
│   │   # File: shared/api/dependencies.py
│   │   
│   │   """
│   │   FastAPI dependencies for standardized API behavior.
│   │   
│   │   Simplified to focus on commonly used dependencies.
│   │   """
│   │   
│   │   from typing import Annotated
│   │   from fastapi import Query, Request, Depends
│   │   from pydantic import BaseModel, Field
│   │   from .correlation import get_correlation_id
│   │   
│   │   
│   │   class PaginationParams(BaseModel):
│   │       """Standard pagination parameters."""
│   │       
│   │       page: int = Field(default=1, ge=1)
│   │       limit: int = Field(default=50, ge=1, le=1000)
│   │       
│   │       @property
│   │       def offset(self) -> int:
│   │           """Calculate offset for database queries."""
│   │           return (self.page - 1) * self.limit
│   │   
│   │   
│   │   def get_pagination_params(
│   │       page: int = Query(1, ge=1, description="Page number"),
│   │       limit: int = Query(50, ge=1, le=1000, description="Items per page")
│   │   ) -> PaginationParams:
│   │       """
│   │       FastAPI dependency for pagination parameters.
│   │       
│   │       Usage:
│   │           @app.get("/items")
│   │           async def list_items(pagination: PaginationDep):
│   │               items = await db.query(offset=pagination.offset, limit=pagination.limit)
│   │       """
│   │       return PaginationParams(page=page, limit=limit)
│   │   
│   │   
│   │   def get_request_id(request: Request) -> str:
│   │       """
│   │       Get request ID from middleware-set state.
│   │       
│   │       Raises error if middleware hasn't run, ensuring proper initialization.
│   │       """
│   │       if not hasattr(request.state, "request_id"):
│   │           raise RuntimeError(
│   │               "Request ID not found. Ensure APIMiddleware is properly configured."
│   │           )
│   │       return request.state.request_id
│   │   
│   │   
│   │   # Type aliases for clean dependency injection
│   │   RequestIdDep = Annotated[str, Depends(get_request_id)]
│   │   PaginationDep = Annotated[PaginationParams, Depends(get_pagination_params)]
│   │   CorrelationIdDep = Annotated[str, Depends(get_correlation_id)]  # Re-export for convenience
│   │   
│   │   
│   │   # Optional: Simplified request context for logging
│   │   class RequestContext(BaseModel):
│   │       """Essential request context for logging/auditing."""
│   │       
│   │       request_id: str
│   │       trace_id: str
│   │       correlation_id: str
│   │       method: str
│   │       path: str
│   │       
│   │       @classmethod
│   │       def from_request(cls, request: Request) -> "RequestContext":
│   │           """Create context from FastAPI request."""
│   │           return cls(
│   │               request_id=get_request_id(request),
│   │               correlation_id=get_correlation_id(request),
│   │               trace_id=request.state.trace_id,
│   │               method=request.method,
│   │               path=str(request.url.path)
│   │           )
│   │   
│   │   
│   │   def get_request_context(request: Request) -> RequestContext:
│   │       """Get essential request context."""
│   │       return RequestContext.from_request(request)
│   │   
│   │   
│   │   RequestContextDep = Annotated[RequestContext, Depends(get_request_context)]
│   │   
│   │   def get_client_ip(request: Request) -> str:
│   │       """
│   │       Extract client IP address.
│   │       Only add if needed for rate limiting or security.
│   │       """
│   │       forwarded_for = request.headers.get("X-Forwarded-For")
│   │       if forwarded_for:
│   │           return forwarded_for.split(",")[0].strip()
│   │       return request.client.host if request.client else "unknown"
│   │   
│   │   
│   │   ClientIpDep = Annotated[str, Depends(get_client_ip)]
│   │   ```
│   │   
│   ├── health.py
│   │   
│   │   ```py
│   │   # glam-app/shared/api/health.py
│   │   
│   │   from fastapi import APIRouter, Request
│   │   from datetime import datetime, timezone
│   │   from shared.api.responses import success_response
│   │   
│   │   
│   │   def create_health_router(service_name: str) -> APIRouter:
│   │       router = APIRouter()
│   │   
│   │       @router.get("/health", tags=["Health"])
│   │       async def health_check(request: Request):
│   │           """Basic health check endpoint with service name and timestamp"""
│   │           return success_response(
│   │               data={
│   │                   "status": "healthy",
│   │                   "service": service_name,
│   │                   "timestamp": datetime.now(timezone.utc).isoformat(),
│   │               },
│   │               request_id=getattr(request.state, "request_id", None),
│   │               correlation_id=getattr(request.state, "correlation_id", None),
│   │           )
│   │   
│   │       return router
│   │   ```
│   │   
│   ├── middleware.py
│   │   
│   │   ```py
│   │   # -------------------------------
│   │   # shared/api/middleware.py
│   │   # -------------------------------
│   │   
│   │   """Simplified API middleware."""
│   │   
│   │   import time
│   │   import uuid
│   │   import logging
│   │   from typing import Callable
│   │   
│   │   from fastapi import Request, Response
│   │   from fastapi import FastAPI
│   │   from fastapi.responses import JSONResponse
│   │   from starlette.middleware.base import BaseHTTPMiddleware
│   │   from fastapi.exceptions import RequestValidationError, HTTPException
│   │   
│   │   from ..errors import GlamBaseError
│   │   from ..metrics import PrometheusMiddleware, metrics_endpoint
│   │   
│   │   from .models import ErrorDetail
│   │   from .responses import error_response
│   │   from .correlation import get_correlation_id, set_correlation_context
│   │   from .tracing import TracingMiddleware
│   │   
│   │   logger = logging.getLogger(__name__)
│   │   
│   │   
│   │   class APIMiddleware(BaseHTTPMiddleware):
│   │       """Unified middleware for request/response handling."""
│   │       
│   │       def __init__(self, app, *, service_name: str = "glam-service"):
│   │           super().__init__(app)
│   │           self.service_name = service_name
│   │       
│   │       async def dispatch(self, request: Request, call_next: Callable) -> Response:
│   │           # Generate IDs
│   │           request_id = request.headers.get("X-Request-ID", f"req_{uuid.uuid4().hex[:12]}")
│   │           
│   │           # Get correlation ID (this will check headers and generate if needed)
│   │           correlation_id = get_correlation_id(request)
│   │           
│   │           # Store in request state for easy access in the request
│   │           request.state.request_id = request_id
│   │           request.state.correlation_id = correlation_id
│   │           
│   │           # IMPORTANT: Set correlation context for async operations
│   │           # This makes correlation_id available throughout the request lifecycle
│   │           set_correlation_context(correlation_id)
│   │           
│   │           # Track timing
│   │           start_time = time.perf_counter()
│   │           
│   │           try:
│   │               response = await call_next(request)
│   │               
│   │               # Add standard headers
│   │               response.headers["X-Request-ID"] = request_id
│   │               response.headers["X-Correlation-ID"] = correlation_id
│   │               response.headers["X-Service-Name"] = self.service_name
│   │               
│   │               return response
│   │               
│   │           except Exception as exc:
│   │               # Convert to standard error response
│   │               error_resp = self._handle_exception(exc, request_id, correlation_id)
│   │               
│   │               # Determine status code
│   │               status_code = 500
│   │               if isinstance(exc, GlamBaseError):
│   │                   status_code = exc.status
│   │               elif isinstance(exc, HTTPException):
│   │                   status_code = exc.status_code
│   │               elif isinstance(exc, RequestValidationError):
│   │                   status_code = 422
│   │               
│   │               # Log error
│   │               duration_ms = (time.perf_counter() - start_time) * 1000
│   │               logger.error(
│   │                   "Request failed",
│   │                   extra={
│   │                       "request_id": request_id,
│   │                       "correlation_id": correlation_id,
│   │                       "method": request.method,
│   │                       "path": request.url.path,
│   │                       "status": status_code,
│   │                       "duration_ms": round(duration_ms, 2),
│   │                       "error_code": error_resp.error.code if error_resp.error else "UNKNOWN",
│   │                       "service": self.service_name
│   │                   }
│   │               )
│   │               
│   │               response = JSONResponse(
│   │                   content=error_resp.model_dump(mode="json", exclude_none=True),
│   │                   status_code=status_code
│   │               )
│   │               
│   │               # Add standard headers
│   │               response.headers["X-Request-ID"] = request_id
│   │               response.headers["X-Correlation-ID"] = correlation_id
│   │               response.headers["X-Service-Name"] = self.service_name
│   │               
│   │               return response
│   │       
│   │       def _handle_exception(self, exc: Exception, request_id: str, correlation_id: str):
│   │           """Convert exception to error response."""
│   │           
│   │           if isinstance(exc, GlamBaseError):
│   │               return error_response(
│   │                   code=exc.code,
│   │                   message=exc.message,
│   │                   details=exc.details,
│   │                   request_id=request_id,
│   │                   correlation_id=correlation_id
│   │               )
│   │           
│   │           elif isinstance(exc, RequestValidationError):
│   │               validation_errors = []
│   │               for error in exc.errors():
│   │                   field_path = ".".join(str(loc) for loc in error["loc"])
│   │                   validation_errors.append({
│   │                       "field": field_path,
│   │                       "message": error["msg"],
│   │                       "type": error["type"]
│   │                   })
│   │               
│   │               return error_response(
│   │                   code="VALIDATION_ERROR",
│   │                   message="Request validation failed",
│   │                   details={"validation_errors": validation_errors},
│   │                   request_id=request_id,
│   │                   correlation_id=correlation_id
│   │               )
│   │           
│   │           elif isinstance(exc, HTTPException):
│   │               return error_response(
│   │                   code=f"HTTP_{exc.status_code}",
│   │                   message=exc.detail,
│   │                   request_id=request_id,
│   │                   correlation_id=correlation_id
│   │               )
│   │           
│   │           else:
│   │               logger.exception(
│   │                   "Unhandled exception",
│   │                   extra={
│   │                       "request_id": request_id,
│   │                       "correlation_id": correlation_id,
│   │                       "error_type": type(exc).__name__
│   │                   }
│   │               )
│   │               
│   │               return error_response(
│   │                   code="INTERNAL_ERROR",
│   │                   message="An unexpected error occurred",
│   │                   request_id=request_id,
│   │                   correlation_id=correlation_id
│   │               )
│   │   
│   │   
│   │   def setup_middleware(
│   │       app: FastAPI,
│   │       *,
│   │       service_name: str,
│   │       enable_tracing: bool = True,
│   │       enable_metrics: bool = True,
│   │       metrics_path: str = "/metrics",
│   │   ):
│   │       """
│   │       Set up all standard middleware for a service.
│   │       
│   │       This sets up middleware in the correct order:
│   │       1. Prometheus metrics (if enabled) - captures all requests
│   │       2. API middleware - handles responses and errors
│   │       
│   │       Args:
│   │           app: FastAPI application
│   │           service_name: Name of the service
│   │           enable_metrics: Whether to enable Prometheus metrics
│   │           metrics_path: Path for metrics endpoint
│   │           debug: Whether to include error details in responses
│   │       """
│   │       # Add Prometheus middleware FIRST (captures all requests)
│   │       if enable_metrics:
│   │           app.add_middleware(PrometheusMiddleware, service_name=service_name)
│   │           
│   │           # Add metrics endpoint
│   │           app.add_api_route(
│   │               metrics_path,
│   │               metrics_endpoint,
│   │               methods=["GET"],
│   │               include_in_schema=False,
│   │               tags=["monitoring"]
│   │           )
│   │   
│   │       if enable_tracing:
│   │           app.add_middleware(TracingMiddleware)
│   │   
│   │       # Add API middleware for standardized responses
│   │       app.add_middleware(APIMiddleware, service_name=service_name)
│   │   ```
│   │   
│   ├── models.py
│   │   
│   │   ```py
│   │   # -------------------------------
│   │   # shared/api/models.py
│   │   # -------------------------------
│   │   
│   │   """
│   │   Unified API response models for glam-app services.
│   │   Consolidates all response structures into a single, consistent pattern.
│   │   """
│   │   
│   │   from typing import TypeVar, Generic, Optional, Any, Dict, List
│   │   from datetime import datetime, timezone
│   │   from pydantic import BaseModel, Field, ConfigDict
│   │   import uuid
│   │   
│   │   # Generic type for response data
│   │   T = TypeVar("T")
│   │   
│   │   
│   │   class Meta(BaseModel):
│   │       """Metadata included in all responses."""
│   │       request_id: str = Field(description="Unique request identifier")
│   │       correlation_id: Optional[str] = Field(None, description="Distributed tracing ID")
│   │       timestamp: datetime = Field(
│   │           default_factory=lambda: datetime.now(timezone.utc),
│   │           description="Response timestamp in UTC"
│   │       )
│   │       
│   │       model_config = ConfigDict(
│   │           json_encoders={datetime: lambda v: v.isoformat()}
│   │       )
│   │   
│   │   
│   │   class Pagination(BaseModel):
│   │       """Pagination metadata for list responses."""
│   │       page: int = Field(ge=1)
│   │       limit: int = Field(ge=1, le=1000)
│   │       total: int = Field(ge=0)
│   │       pages: int = Field(ge=0)
│   │       has_next: bool
│   │       has_previous: bool
│   │       
│   │       @classmethod
│   │       def create(cls, page: int, limit: int, total: int) -> "Pagination":
│   │           """Create pagination from parameters."""
│   │           pages = (total + limit - 1) // limit if total > 0 else 0
│   │           return cls(
│   │               page=page,
│   │               limit=limit,
│   │               total=total,
│   │               pages=pages,
│   │               has_next=page < pages,
│   │               has_previous=page > 1
│   │           )
│   │   
│   │   
│   │   class Links(BaseModel):
│   │       """HATEOAS links for resource navigation."""
│   │       self: str
│   │       next: Optional[str] = None
│   │       previous: Optional[str] = None
│   │       first: Optional[str] = None
│   │       last: Optional[str] = None
│   │       
│   │       @classmethod
│   │       def create_paginated(
│   │           cls, 
│   │           base_url: str, 
│   │           page: int, 
│   │           limit: int, 
│   │           pages: int,
│   │           **query_params
│   │       ) -> "Links":
│   │           """Create pagination links."""
│   │           def build_url(page_num: int) -> str:
│   │               params = {**query_params, "page": page_num, "limit": limit}
│   │               query = "&".join(f"{k}={v}" for k, v in params.items())
│   │               return f"{base_url}?{query}"
│   │           
│   │           return cls(
│   │               self=build_url(page),
│   │               next=build_url(page + 1) if page < pages else None,
│   │               previous=build_url(page - 1) if page > 1 else None,
│   │               first=build_url(1) if pages > 0 else None,
│   │               last=build_url(pages) if pages > 0 else None
│   │           )
│   │   
│   │   
│   │   class ErrorDetail(BaseModel):
│   │       """Error information."""
│   │       code: str
│   │       message: str
│   │       details: Optional[Dict[str, Any]] = None
│   │   
│   │   
│   │   class ApiResponse(BaseModel, Generic[T]):
│   │       """
│   │       Unified API response structure.
│   │       Used for both success and error responses.
│   │       """
│   │       # For success responses
│   │       data: Optional[T] = None
│   │       
│   │       # For error responses
│   │       error: Optional[ErrorDetail] = None
│   │       
│   │       # Always present
│   │       meta: Meta
│   │       
│   │       # Optional for paginated responses
│   │       pagination: Optional[Pagination] = None
│   │       links: Optional[Links] = None
│   │       
│   │       model_config = ConfigDict(
│   │           json_encoders={datetime: lambda v: v.isoformat()}
│   │       )
│   │   ```
│   │   
│   ├── responses.py
│   │   
│   │   ```py
│   │   # -------------------------------
│   │   # shared/api/responses.py
│   │   # -------------------------------
│   │   
│   │   """Response helper functions."""
│   │   
│   │   from typing import Optional, Dict, Any, List, Tuple
│   │   import uuid
│   │   from .models import ApiResponse, Meta, ErrorDetail, Pagination, Links, T
│   │   
│   │   
│   │   def create_response(
│   │       data: Optional[T] = None,
│   │       error: Optional[ErrorDetail] = None,
│   │       request_id: Optional[str] = None,
│   │       correlation_id: Optional[str] = None,
│   │       pagination: Optional[Pagination] = None,
│   │       links: Optional[Links] = None
│   │   ) -> ApiResponse[T]:
│   │       """Create a unified API response."""
│   │       if request_id is None:
│   │           request_id = f"req_{uuid.uuid4().hex[:12]}"
│   │       
│   │       meta = Meta(request_id=request_id, correlation_id=correlation_id)
│   │       
│   │       return ApiResponse(
│   │           data=data,
│   │           error=error,
│   │           meta=meta,
│   │           pagination=pagination,
│   │           links=links
│   │       )
│   │   
│   │   
│   │   def success_response(
│   │       data: T,
│   │       request_id: Optional[str] = None,
│   │       correlation_id: Optional[str] = None,
│   │       links: Optional[Links] = None
│   │   ) -> ApiResponse[T]:
│   │       """Create a success response."""
│   │       return create_response(
│   │           data=data,
│   │           request_id=request_id,
│   │           correlation_id=correlation_id,
│   │           links=links
│   │       )
│   │   
│   │   
│   │   def error_response(
│   │       code: str,
│   │       message: str,
│   │       details: Optional[Dict[str, Any]] = None,
│   │       request_id: Optional[str] = None,
│   │       correlation_id: Optional[str] = None
│   │   ) -> ApiResponse[None]:
│   │       """Create an error response."""
│   │       error = ErrorDetail(code=code, message=message, details=details)
│   │       return create_response(
│   │           error=error,
│   │           request_id=request_id,
│   │           correlation_id=correlation_id
│   │       )
│   │   
│   │   
│   │   def paginated_response(
│   │       data: List[T],
│   │       page: int,
│   │       limit: int,
│   │       total: int,
│   │       base_url: str,
│   │       request_id: Optional[str] = None,
│   │       correlation_id: Optional[str] = None,
│   │       **query_params
│   │   ) -> ApiResponse[List[T]]:
│   │       """Create a paginated response."""
│   │       pagination = Pagination.create(page, limit, total)
│   │       links = Links.create_paginated(base_url, page, limit, pagination.pages, **query_params)
│   │       
│   │       return create_response(
│   │           data=data,
│   │           request_id=request_id,
│   │           correlation_id=correlation_id,
│   │           pagination=pagination,
│   │           links=links
│   │       )
│   │   ```
│   │   
│   └── tracing.py
│       
│       ```py
│       # shared/api/tracing.py
│       from typing import Optional
│       from fastapi import Request
│       from contextvars import ContextVar
│       from uuid7 import uuid7
│       from starlette.middleware.base import BaseHTTPMiddleware
│       from .correlation import set_correlation_context
│       
│       _trace_ctx: ContextVar[Optional[str]] = ContextVar("trace_id", default=None)
│                                       
│       
│       def set_trace_context(trace_id: Optional[str]) -> None:
│           _trace_ctx.set(trace_id)
│       
│       def get_trace_context() -> Optional[str]:
│           return _trace_ctx.get()
│       
│       class TracingMiddleware(BaseHTTPMiddleware):
│           """Middleware to handle trace_id and correlation_id for all requests"""
│           
│           async def dispatch(self, request: Request, call_next):
│               # 🆕 Extract or generate trace_id from W3C traceparent
│               trace_id = self._extract_trace_id(request) or str(uuid7())
│               
│               # Extract or generate correlation_id
│               correlation_id = request.headers.get("x-correlation-id") or trace_id
│               
│               # Set context for this request
│               set_trace_context(trace_id)
│               set_correlation_context(correlation_id)
│               
│               # Add to request state
│               request.state.trace_id = trace_id
│               request.state.correlation_id = correlation_id
│               
│               # Process request
│               response = await call_next(request)
│               
│               # Add headers to response
│               response.headers["x-trace-id"] = trace_id
│               response.headers["x-correlation-id"] = correlation_id
│               
│               return response
│           
│           def _extract_trace_id(self, request: Request) -> Optional[str]:
│               """Extract trace ID from W3C traceparent header"""
│               traceparent = request.headers.get("traceparent")
│               if traceparent:
│                   # W3C traceparent format: version-trace_id-parent_id-flags
│                   parts = traceparent.split("-")
│                   if len(parts) >= 2:
│                       return parts[1]  # Return trace_id part
│               return None
│       
│       
│       
│       ```
│       
├── config/
│   ├── __init__.py
│   └── loader.py
│       
│       ```py
│       from __future__ import annotations
│       from pathlib import Path
│       from typing import Any, Dict
│       
│       import os
│       import yaml
│       from dotenv import load_dotenv
│       
│       
│       _REPO_ROOT = Path(__file__).resolve()    
│       
│       while _REPO_ROOT.name != "glam-app":
│           if _REPO_ROOT.parent == _REPO_ROOT:
│               raise RuntimeError("Unable to locate glam-app root directory")
│           _REPO_ROOT = _REPO_ROOT.parent
│       
│       _CONFIG_DIR = _REPO_ROOT / "config"                     # ./config
│       _SHARED_CONFIG = _CONFIG_DIR / "shared.yml"            # ./config/shared.yml
│       _SVC_CFG_DIR = _CONFIG_DIR / "services"                 # ./config/services
│       _ENV_FILE = _REPO_ROOT / ".env"                         # optional
│       
│       
│       # Check if files exist
│       print(f"\nFile existence check:")
│       print(f"  .env exists: {_ENV_FILE.exists()}")
│       print(f"  shared.yml exists: {_SHARED_CONFIG.exists()}")
│       print(f"  config/services/ exists: {_SVC_CFG_DIR.exists()}")
│       
│       # Load .env once so os.environ is ready (local runs)
│       if _ENV_FILE.exists():
│           load_dotenv(_ENV_FILE)
│       
│       
│       def _load_yaml_file(path: Path) -> Dict[str, Any]:
│           """Load a YAML file and return dict"""
│           if not path.is_file():
│               return {}
│           with path.open() as f:
│               return yaml.safe_load(f) or {}
│       
│       
│       def _deep_merge(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
│           """Deep merge two dictionaries, override takes precedence"""
│           result = base.copy()
│           
│           for key, value in override.items():
│               if (key in result and 
│                   isinstance(result[key], dict) and 
│                   isinstance(value, dict)):
│                   result[key] = _deep_merge(result[key], value)
│               else:
│                   result[key] = value
│           
│           return result
│       
│       
│       def merged_config(service: str, *, env_prefix: str) -> Dict[str, Any]:
│           """
│           Load configuration in order of precedence:
│           1. config/shared.yml              -> baseline shared config
│           2. config/services/{service}.yml  -> service-specific config  
│           3. Environment variables           -> runtime overrides
│           
│           YAML                      -> baseline
│           (prefixed) env variables  -> override keys in YAML
│           RESULT                    -> dict ready for Pydantic
│           """
│           
│           # 1. Load shared configuration (baseline)
│           cfg = _load_yaml_file(_SHARED_CONFIG)
│           
│           # 2. Load service-specific configuration and merge
│           service_config_path = _SVC_CFG_DIR / f"{service}.yml"
│           if not service_config_path.is_file():
│               raise FileNotFoundError(f"Service config not found: {service_config_path}")
│           
│           service_config = _load_yaml_file(service_config_path)
│           cfg = _deep_merge(cfg, service_config)
│           
│           # 3. Apply environment variable overrides
│           prefix = f"{env_prefix.upper()}_"
│           for key, val in os.environ.items():
│               if key.startswith(prefix):
│                   yaml_key = key[len(prefix):].lower()
│                   
│                   # Handle nested keys with double underscore
│                   if "__" in yaml_key:
│                       parts = yaml_key.split("__")
│                       current = cfg
│                       
│                       # Navigate/create nested structure
│                       for part in parts[:-1]:
│                           if part not in current:
│                               current[part] = {}
│                           current = current[part]
│                       
│                       # Set the final value
│                       current[parts[-1]] = val
│                   else:
│                       cfg[yaml_key] = val
│           
│           return cfg
│       
│       def flatten_config(data: dict, parent_key: str = '', sep: str = '.') -> dict:
│           """Flatten nested dict for Pydantic validation_alias to work"""
│           items = []
│           for k, v in data.items():
│               new_key = f"{parent_key}{sep}{k}" if parent_key else k
│               if isinstance(v, dict):
│                   items.extend(flatten_config(v, new_key, sep=sep).items())
│               else:
│                   items.append((new_key, v))
│           return dict(items)
│       ```
│       
├── database/
│   ├── __init__.py
│   │   
│   │   ```py
│   │   # glam-app/shared/database/__init__.py
│   │   """
│   │   Shared database utilities for GLAM microservices.
│   │   
│   │   This package provides:
│   │   - Base SQLAlchemy models and mixins
│   │   - Async session management
│   │   - Generic repository pattern
│   │   - FastAPI dependencies
│   │   - Alembic migration utilities
│   │   - Database configuration
│   │   """
│   │   
│   │   from .base import Base, TimestampedMixin, SoftDeleteMixin
│   │   from .session import DatabaseSessionManager
│   │   from .repository import Repository
│   │   from .dependencies import (
│   │       DBSessionDep,
│   │       get_db_session,
│   │       set_database_manager,
│   │       get_database_manager,
│   │       get_database_health,
│   │   )
│   │   from .config import DatabaseConfig, create_database_config
│   │   from .migrations import MigrationManager, create_alembic_env_template
│   │   
│   │   __all__ = [
│   │       # Base classes
│   │       "Base",
│   │       "TimestampedMixin",
│   │       "SoftDeleteMixin",
│   │       
│   │       # Session management
│   │       "DatabaseSessionManager",
│   │       
│   │       # Repository pattern
│   │       "Repository",
│   │       
│   │       # FastAPI dependencies
│   │       "DBSessionDep",
│   │       "get_db_session",
│   │       "set_database_manager",
│   │       "get_database_manager",
│   │       "get_database_health",
│   │       
│   │       # Configuration
│   │       "DatabaseConfig",
│   │       "create_database_config",
│   │       
│   │       # Migrations
│   │       "MigrationManager",
│   │       "create_alembic_env_template",
│   │   ]
│   │   ```
│   │   
│   ├── base.py
│   │   
│   │   ```py
│   │   
│   │   # glam-app/shared/database/base.py
│   │   from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column
│   │   from sqlalchemy.ext.asyncio import AsyncAttrs
│   │   from sqlalchemy.dialects.postgresql import UUID as PGUUID
│   │   from sqlalchemy import DateTime, String, func, Index, MetaData
│   │   from datetime import datetime
│   │   from uuid import UUID
│   │   
│   │   
│   │   class Base(AsyncAttrs, DeclarativeBase):
│   │       __abstract__ = True            # <- prevents accidental table mapping
│   │   
│   │       # optional: naming convention for Alembic
│   │       metadata = MetaData(naming_convention={
│   │           "ix": "ix_%(column_0_label)s",
│   │           "uq": "uq_%(table_name)s_%(column_0_name)s",
│   │           "ck": "ck_%(table_name)s_%(constraint_name)s",
│   │           "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
│   │           "pk": "pk_%(table_name)s"
│   │       })
│   │   
│   │   class MerchantMixin:
│   │       """Mixin to add merchant_id to any model"""
│   │       merchant_id: Mapped[UUID] = mapped_column(
│   │           PGUUID(as_uuid=True), 
│   │           nullable=False, 
│   │           index=True
│   │       )
│   │       merchant_domain: Mapped[str] = mapped_column(
│   │           String(255), 
│   │           nullable=False, 
│   │           index=True
│   │       )
│   │       __table_args__ = (
│   │       Index("idx_merchant_id_domain", "merchant_id", "merchant_domain"),)
│   │   
│   │   
│   │   class TimestampedMixin:
│   │       """Mixin to add created_at and updated_at to any model"""
│   │       created_at = mapped_column( DateTime(timezone=True), server_default=func.now(), nullable=False, index=True )
│   │       updated_at = mapped_column( DateTime(timezone=True), server_default=func.now(), server_onupdate=func.now(), nullable=False )
│   │   
│   │   
│   │   class SoftDeleteMixin:
│   │       """Mixin to add soft delete functionality"""
│   │       deleted_at: Mapped[datetime | None] = mapped_column(
│   │           DateTime(timezone=True),
│   │           default=None)
│   │       is_deleted: Mapped[bool] = mapped_column(default=False, index=True)
│   │   ```
│   │   
│   ├── config.py
│   │   
│   │   ```py
│   │   # glam-app/shared/database/config.py
│   │   from __future__ import annotations
│   │   from pydantic_settings import BaseSettings, SettingsConfigDict
│   │   from pydantic import Field
│   │   from typing import Any, Dict
│   │   
│   │   
│   │   class DatabaseConfig(BaseSettings):
│   │       # ── connection ─────────────────────────────────────────────
│   │       DB_HOST: str
│   │       DB_PORT: int = 5432
│   │       DB_PORT_EXTERNAL: int | None = None
│   │       DB_NAME: str
│   │       DB_USER: str
│   │       DB_PASSWORD: str
│   │       DB_ENABLED: bool = True
│   │   
│   │       # ── pool / driver ──────────────────────────────────────────
│   │       DB_POOL_SIZE: int = 5
│   │       DB_MAX_OVERFLOW: int = 10
│   │       DB_POOL_PRE_PING: bool = True
│   │       DB_POOL_RECYCLE: int = 3600
│   │       DB_ASYNC_DRIVER: str = "asyncpg"
│   │       DB_ECHO: bool = False
│   │   
│   │       # defaults: `.env` at repo root, strict case match
│   │       model_config = SettingsConfigDict(
│   │           env_file=".env",
│   │           env_file_encoding="utf-8",
│   │           case_sensitive=True,          # "CREDIT_DB_HOST" must match exactly
│   │           populate_by_name=True,
│   │       )
│   │   
│   │       # ── helpers ────────────────────────────────────────────────
│   │       def model_post_init(self, _ctx: Any) -> None:
│   │           if self.DB_PORT is None:
│   │               if self.DB_HOST in {"localhost", "127.0.0.1", "host.docker.internal"}:
│   │                   self.DB_PORT = self.DB_PORT_EXTERNAL or 5432
│   │               else:
│   │                   self.DB_PORT = 5432
│   │   
│   │       @property
│   │       def effective_port(self) -> int:
│   │           """Return host-side port when talking to localhost, else the container port."""
│   │           if self.DB_HOST in {"localhost", "127.0.0.1", "host.docker.internal"}:
│   │               return self.DB_PORT_EXTERNAL or self.DB_PORT
│   │           return self.DB_PORT
│   │       
│   │       @property
│   │       def database_url(self) -> str:
│   │           return (
│   │               f"postgresql+{self.DB_ASYNC_DRIVER}://"
│   │               f"{self.DB_USER}:{self.DB_PASSWORD}@"
│   │               f"{self.DB_HOST}:{self.effective_port}/{self.DB_NAME}"
│   │           )
│   │   
│   │       def engine_kwargs(self) -> Dict[str, Any]:
│   │           return dict(
│   │               echo=self.DB_ECHO,
│   │               pool_size=self.DB_POOL_SIZE,
│   │               max_overflow=self.DB_MAX_OVERFLOW,
│   │               pool_pre_ping=self.DB_POOL_PRE_PING,
│   │               pool_recycle=self.DB_POOL_RECYCLE,
│   │           )
│   │   
│   │   
│   │   def create_database_config(prefix: str) -> DatabaseConfig:
│   │       """Factory that applies the per-service prefix (CREDIT_, NOTIFICATION_, …)."""
│   │       class Prefixed(DatabaseConfig):
│   │           model_config = SettingsConfigDict(
│   │               env_prefix=prefix,         # CREDIT_DB_HOST, etc.
│   │               env_file=".env",
│   │               case_sensitive=True,
│   │               populate_by_name=True,
│   │           )
│   │       return Prefixed() # type: ignore[call-arg]
│   │   ```
│   │   
│   ├── dependencies.py
│   │   
│   │   ```py
│   │   # glam-app/shared/database/dependencies.py
│   │   from typing import Annotated, AsyncGenerator
│   │   from fastapi import Depends
│   │   from sqlalchemy.ext.asyncio import AsyncSession
│   │   from .session import DatabaseSessionManager
│   │   
│   │   # Global database manager instance - each service will set this
│   │   from typing import Optional
│   │   
│   │   _db_manager: Optional[DatabaseSessionManager] = None
│   │   
│   │   
│   │   def set_database_manager(manager: DatabaseSessionManager):
│   │       """Set the global database manager for the service"""
│   │       global _db_manager
│   │       _db_manager = manager
│   │   
│   │   
│   │   def get_database_manager() -> DatabaseSessionManager:
│   │       """Get the current database manager"""
│   │       if _db_manager is None:
│   │           raise RuntimeError(
│   │               "Database manager not initialized. "
│   │               "Call set_database_manager() during app startup."
│   │           )
│   │       return _db_manager
│   │   
│   │   
│   │   async def get_db_session() -> AsyncGenerator[AsyncSession, None]:
│   │       """FastAPI dependency to get a database session"""
│   │       manager = get_database_manager()
│   │       async with manager.session() as session:
│   │           yield session
│   │           
│   │   async def get_database_health() -> bool:
│   │       """Check if the database is healthy"""
│   │       manager = get_database_manager()
│   │       try:
│   │           async with manager.session() as session:
│   │               # Perform a simple query to check connectivity
│   │               from sqlalchemy.sql import text
│   │               await session.execute(text("SELECT 1"))
│   │           return True
│   │       except Exception as e:
│   │           # Log the error or handle it as needed
│   │           print(f"Database health check failed: {e}")
│   │           return False
│   │   
│   │   
│   │   # Type alias for dependency injection
│   │   DBSessionDep = Annotated[AsyncSession, Depends(get_db_session)]
│   │   ```
│   │   
│   ├── migrations.py
│   │   
│   │   ```py
│   │   # glam-app/shared/database/migrations.py
│   │   import os
│   │   from pathlib import Path
│   │   from alembic import command
│   │   from alembic.config import Config
│   │   from sqlalchemy import text
│   │   from sqlalchemy.ext.asyncio import AsyncEngine
│   │   import logging
│   │   
│   │   logger = logging.getLogger(__name__)
│   │   
│   │   
│   │   class MigrationManager:
│   │       """Manages Alembic migrations for a microservice"""
│   │       
│   │       def __init__(
│   │           self,
│   │           service_name: str,
│   │           alembic_ini_path: str,
│   │           migrations_path: str,
│   │           database_url: str
│   │       ):
│   │           self.service_name = service_name
│   │           self.alembic_ini_path = Path(alembic_ini_path)
│   │           self.migrations_path = Path(migrations_path)
│   │           self.database_url = database_url
│   │           
│   │           # Verify paths exist
│   │           if not self.alembic_ini_path.exists():
│   │               raise FileNotFoundError(f"Alembic config not found: {alembic_ini_path}")
│   │           
│   │           # Create migrations directory if it doesn't exist
│   │           self.migrations_path.mkdir(parents=True, exist_ok=True)
│   │       
│   │       def get_alembic_config(self) -> Config:
│   │           """Get Alembic configuration"""
│   │           config = Config(str(self.alembic_ini_path))
│   │           config.set_main_option("sqlalchemy.url", self.database_url)
│   │           config.set_main_option("script_location", str(self.migrations_path))
│   │           return config
│   │       
│   │       def init_alembic(self):
│   │           """Initialize Alembic for the service (run once)"""
│   │           config = self.get_alembic_config()
│   │           command.init(config, str(self.migrations_path))
│   │           logger.info(f"Initialized Alembic for {self.service_name}")
│   │       
│   │       def create_migration(self, message: str):
│   │           """Create a new migration"""
│   │           config = self.get_alembic_config()
│   │           command.revision(config, message=message, autogenerate=True)
│   │           logger.info(f"Created migration: {message}")
│   │       
│   │       def upgrade(self, revision: str = "head"):
│   │           """Apply migrations up to a specific revision"""
│   │           config = self.get_alembic_config()
│   │           command.upgrade(config, revision)
│   │           logger.info(f"Upgraded database to {revision}")
│   │       
│   │       def downgrade(self, revision: str):
│   │           """Downgrade to a specific revision"""
│   │           config = self.get_alembic_config()
│   │           command.downgrade(config, revision)
│   │           logger.info(f"Downgraded database to {revision}")
│   │       
│   │       def get_current_revision(self) -> str:
│   │           """Get the current migration revision"""
│   │           config = self.get_alembic_config()
│   │           # This would require more implementation
│   │           return "Not implemented"
│   │       
│   │       async def ensure_schema_exists(self, engine: AsyncEngine, schema_name: str):
│   │           """Ensure a database schema exists (PostgreSQL specific)"""
│   │           async with engine.connect() as conn:
│   │               await conn.execute(
│   │                   text(f"CREATE SCHEMA IF NOT EXISTS {schema_name}")
│   │               )
│   │               await conn.commit()
│   │           logger.info(f"Ensured schema exists: {schema_name}")
│   │   
│   │   
│   │   def create_alembic_env_template(service_name: str, base_module: str) -> str:
│   │       """Generate env.py template for a service"""
│   │       return f'''"""Alembic environment script for {service_name}"""
│   │   from logging.config import fileConfig
│   │   from sqlalchemy import engine_from_config, pool
│   │   from alembic import context
│   │   
│   │   # Import your service's Base metadata
│   │   from {base_module} import Base
│   │   
│   │   config = context.config
│   │   
│   │   if config.config_file_name is not None:
│   │       fileConfig(config.config_file_name)
│   │   
│   │   target_metadata = Base.metadata
│   │   
│   │   
│   │   def run_migrations_offline() -> None:
│   │       """Run migrations in 'offline' mode."""
│   │       url = config.get_main_option("sqlalchemy.url")
│   │       context.configure(
│   │           url=url,
│   │           target_metadata=target_metadata,
│   │           literal_binds=True,
│   │           dialect_opts={{"paramstyle": "named"}},
│   │       )
│   │   
│   │       with context.begin_transaction():
│   │           context.run_migrations()
│   │   
│   │   
│   │   def run_migrations_online() -> None:
│   │       """Run migrations in 'online' mode."""
│   │       connectable = engine_from_config(
│   │           config.get_section(config.config_ini_section),
│   │           prefix="sqlalchemy.",
│   │           poolclass=pool.NullPool,
│   │       )
│   │   
│   │       with connectable.connect() as connection:
│   │           context.configure(
│   │               connection=connection,
│   │               target_metadata=target_metadata
│   │           )
│   │   
│   │           with context.begin_transaction():
│   │               context.run_migrations()
│   │   
│   │   
│   │   if context.is_offline_mode():
│   │       run_migrations_offline()
│   │   else:
│   │       run_migrations_online()
│   │   '''
│   │   ```
│   │   
│   ├── repository.py
│   │   
│   │   ```py
│   │   # glam-app/shared/database/repository.py
│   │   from sqlalchemy import select
│   │   from typing import TypeVar, Generic, Type, AsyncIterator
│   │   from uuid import UUID
│   │   from sqlalchemy.ext.asyncio import async_sessionmaker
│   │   from sqlalchemy.ext.asyncio import AsyncSession
│   │   from .base import Base
│   │   
│   │   T = TypeVar("T", bound=Base)
│   │   
│   │   
│   │   class Repository(Generic[T]):
│   │       """
│   │       Generic repository providing basic CRUD operations.
│   │       Services can extend this for specific domain needs.
│   │       """
│   │       
│   │       def __init__(self, model: Type[T], session_factory: async_sessionmaker[AsyncSession]):
│   │           self.model = model
│   │           self.session_factory = session_factory
│   │   
│   │       # helper used by child methods
│   │       async def _session(self) -> AsyncIterator[AsyncSession]:
│   │           async with self.session_factory() as session:
│   │               yield session
│   │   
│   │       async def save(self, instance: T) -> T | None:
│   │           """Save an instance to the database"""
│   │           async for session in self._session():
│   │               session.add(instance)
│   │               await session.commit()
│   │               return instance
│   │       
│   │       async def update(self, instance: T) -> T | None:
│   │           """Update an existing instance"""
│   │           async for session in self._session():
│   │               await session.merge(instance)
│   │               await session.commit()
│   │               return instance
│   │           
│   │       async def delete(self, instance: T) -> None:
│   │           """Delete an instance from the database"""
│   │           async for session in self._session():
│   │               await session.delete(instance)
│   │               await session.commit()
│   │       
│   │       async def delete_by_id(self, id: str | UUID) -> None:
│   │           """Delete an instance by its ID"""
│   │           async for session in self._session():
│   │               instance = await session.get(self.model, id)
│   │               if instance:
│   │                   await session.delete(instance)
│   │                   await session.commit()
│   │           
│   │       async def find_by_id(self, id: str | UUID) -> T | None:
│   │           """Find an instance by its ID"""
│   │           async for session in self._session():
│   │               result = await session.get(self.model, id)
│   │               return result
│   │           
│   │       async def find_all(
│   │           self,
│   │           * ,
│   │           limit: int | None = None,
│   │           offset: int | None = None,
│   │           **filters
│   │       ) -> list[T] | None:
│   │           """
│   │           Return a list of model instances.
│   │           Optional keyword filters map column names to values (exact match).
│   │           You can also page results with limit/offset.
│   │           """
│   │           async for session in self._session():
│   │               stmt = select(self.model)
│   │   
│   │               # Apply column == value filters
│   │               for col, val in filters.items():
│   │                   try:
│   │                       stmt = stmt.where(getattr(self.model, col) == val)
│   │                   except AttributeError:
│   │                       raise ValueError(f"{col!r} is not a valid column on {self.model.__name__}")
│   │   
│   │               # Pagination
│   │               if offset is not None:
│   │                   stmt = stmt.offset(offset)
│   │               if limit is not None:
│   │                   stmt = stmt.limit(limit)
│   │   
│   │               result = await session.execute(stmt)
│   │               return list(result.scalars().all())
│   │   ```
│   │   
│   └── session.py
│       
│       ```py
│       # glam-app/shared/database/session.py
│       from sqlalchemy.ext.asyncio import (
│           create_async_engine,
│           AsyncSession,
│           async_sessionmaker,
│           AsyncEngine
│       )
│       from contextlib import asynccontextmanager
│       from typing import AsyncGenerator, Optional
│       import logging
│       
│       logger = logging.getLogger(__name__)
│       
│       
│       class DatabaseSessionManager:
│           """
│           Manages database connections and sessions for a microservice.
│           Each service creates its own instance with its specific configuration.
│           """
│           
│           def __init__(
│               self,
│               database_url: str,
│               echo: bool = False,
│               pool_size: int = 5,
│               max_overflow: int = 10,
│               pool_pre_ping: bool = True,
│               pool_recycle: int = 3600
│           ):
│               self.database_url = database_url
│               self._engine: Optional[AsyncEngine] = None
│               self._session_factory: Optional[async_sessionmaker[AsyncSession]] = None
│               
│               # Engine configuration
│               self.engine_config = {
│                   "echo": echo,
│                   "pool_size": pool_size,
│                   "max_overflow": max_overflow,
│                   "pool_pre_ping": pool_pre_ping,
│                   "pool_recycle": pool_recycle,
│               }
│           
│           async def init(self):
│               """Initialize the database engine and session factory"""
│               if self._engine is not None:
│                   raise RuntimeError("Database session manager already initialized")
│               
│               self._engine = create_async_engine(
│                   self.database_url,
│                   **self.engine_config
│               )
│               
│               self._session_factory = async_sessionmaker(
│                   bind=self._engine,
│                   class_=AsyncSession,
│                   autocommit=False,
│                   autoflush=False,
│                   expire_on_commit=False
│               )
│               
│               logger.info(f"Database engine initialized with URL: {self.database_url}")
│           
│           async def close(self):
│               """Close the database engine"""
│               if self._engine is None:
│                   raise RuntimeError("Database session manager not initialized")
│               
│               await self._engine.dispose()
│               self._engine = None
│               self._session_factory = None
│               logger.info("Database engine closed")
│           
│           @asynccontextmanager
│           async def get_session(self) -> AsyncGenerator[AsyncSession, None]:
│               if self._session_factory is None:
│                   raise RuntimeError("Database session manager not initialized")
│               
│               async with self._session_factory() as session:
│                   try:
│                       yield session
│                       await session.commit()
│                   except Exception:
│                       await session.rollback()
│                       raise
│                   finally:
│                       await session.close()
│           
│           @property
│           def engine(self) -> AsyncEngine:
│               """Get the underlying SQLAlchemy engine"""
│               if self._engine is None:
│                   raise RuntimeError("Database session manager not initialized")
│               return self._engine
│           
│           @property
│           def session_factory(self) -> async_sessionmaker[AsyncSession]:
│               """Get the session factory"""
│               if self._session_factory is None:
│                   raise RuntimeError("Database session manager not initialized")
│               return self._session_factory
│       ```
│       
├── errors/
│   ├── __init__.py
│   │   
│   │   ```py
│   │   # -------------------------------
│   │   # shared/errors/__init__.py
│   │   # -------------------------------
│   │   
│   │   """
│   │   Shared error handling module for glam-app microservices.
│   │   
│   │   This module provides a consistent error hierarchy and handling patterns
│   │   across all services, following the three-tier model:
│   │   - BaseError (root)
│   │   - InfrastructureError (external failures)
│   │   - DomainError (business logic failures)
│   │   """
│   │   
│   │   from .base import (
│   │       GlamBaseError,
│   │       InfrastructureError,
│   │       DomainError,
│   │       ValidationError,
│   │       NotFoundError,
│   │       ConflictError,
│   │       UnauthorizedError,
│   │       ForbiddenError,
│   │       RateLimitedError,
│   │       ServiceUnavailableError,
│   │       RequestTimeoutError,
│   │       InternalError,
│   │   )
│   │   
│   │   from .catalog import (
│   │       SyncInProgressError,
│   │       SyncNotFoundError,
│   │       SyncNotResumableError,
│   │       SyncNotCancellableError,
│   │       ItemNotFoundError,
│   │       ParentSyncNotFoundError,
│   │   )
│   │   
│   │   from .profile import (
│   │       ProfileNotFoundError,
│   │       ProfileAlreadyExistsError,
│   │       ProfileCreationFailedError,
│   │   )
│   │   
│   │   from .analysis import (
│   │       AnalysisInProgressError,
│   │       AnalysisNotFoundError,
│   │       AnalysisNotCancellableError,
│   │       NoCurrentAnalysisError,
│   │   )
│   │   
│   │   from .selfie import (
│   │       SelfieNotFoundError,
│   │       InvalidImageFormatError,
│   │       ImageTooLargeError,
│   │       ImageTooSmallError,
│   │       NoFaceDetectedError,
│   │       MultipleFacesDetectedError,
│   │       PoorImageQualityError,
│   │   )
│   │   
│   │   from .notification import (
│   │       NotificationNotFoundError,
│   │       TemplateNotFoundError,
│   │       TemplateRenderError,
│   │       InvalidRecipientError,
│   │       PreferencesNotFoundError,
│   │       EmailProviderError,
│   │       UnsubscribedError,
│   │   )
│   │   
│   │   from .infrastructure import (
│   │       DatabaseError,
│   │       RedisError,
│   │       S3Error,
│   │       UpstreamServiceError,
│   │       CircuitOpenError,
│   │       MessageBusError,
│   │   )
│   │   
│   │   
│   │   from .utils import (
│   │       wrap_external_error,
│   │       classify_http_error,
│   │       is_retryable_error,
│   │   )
│   │   
│   │   __all__ = [
│   │       # Base errors
│   │       "GlamBaseError",
│   │       "InfrastructureError",
│   │       "DomainError",
│   │       # Common domain errors
│   │       "ValidationError",
│   │       "NotFoundError",
│   │       "ConflictError",
│   │       "UnauthorizedError",
│   │       "ForbiddenError",
│   │       "RateLimitedError",
│   │       "ServiceUnavailableError",
│   │       "InternalError",
│   │       # Catalog errors
│   │       "SyncInProgressError",
│   │       "SyncNotFoundError",
│   │       "SyncNotResumableError",
│   │       "SyncNotCancellableError",
│   │       "ItemNotFoundError",
│   │       "ParentSyncNotFoundError",
│   │       # Profile errors
│   │       "ProfileNotFoundError",
│   │       "ProfileAlreadyExistsError",
│   │       "ProfileCreationFailedError",
│   │       # Analysis errors
│   │       "AnalysisInProgressError",
│   │       "AnalysisNotFoundError",
│   │       "AnalysisNotCancellableError",
│   │       "NoCurrentAnalysisError",
│   │       # Selfie errors
│   │       "SelfieNotFoundError",
│   │       "InvalidImageFormatError",
│   │       "ImageTooLargeError",
│   │       "ImageTooSmallError",
│   │       "NoFaceDetectedError",
│   │       "MultipleFacesDetectedError",
│   │       "PoorImageQualityError",
│   │       # Notification errors
│   │       "NotificationNotFoundError",
│   │       "TemplateNotFoundError",
│   │       "TemplateRenderError",
│   │       "InvalidRecipientError",
│   │       "PreferencesNotFoundError",
│   │       "EmailProviderError",
│   │       "UnsubscribedError",
│   │       # Infrastructure errors
│   │       "DatabaseError",
│   │       "RedisError",
│   │       "S3Error",
│   │       "UpstreamServiceError",
│   │       "CircuitOpenError",
│   │       "MessageBusError",
│   │       # Handlers and utilities
│   │       "wrap_external_error",
│   │       "classify_http_error",
│   │       "is_retryable_error",
│   │   ]
│   │   ```
│   │   
│   ├── analysis.py
│   │   
│   │   ```py
│   │   # -------------------------------
│   │   # shared/errors/analysis.py
│   │   # -------------------------------
│   │   
│   │   """Analysis service specific errors."""
│   │   
│   │   from typing import Optional
│   │   from .base import ConflictError, NotFoundError
│   │   
│   │   
│   │   class AnalysisInProgressError(ConflictError):
│   │       """Another analysis is already in progress."""
│   │       
│   │       code = "ANALYSIS_IN_PROGRESS"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str = "Another analysis is already in progress",
│   │           *,
│   │           current_analysis_id: Optional[str] = None,
│   │           user_id: Optional[str] = None,
│   │           started_at: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │           
│   │           if current_analysis_id:
│   │               self.details["current_analysis_id"] = current_analysis_id
│   │           if user_id:
│   │               self.details["user_id"] = user_id
│   │           if started_at:
│   │               self.details["started_at"] = started_at
│   │   
│   │   
│   │   class AnalysisNotFoundError(NotFoundError):
│   │       """Analysis not found."""
│   │       
│   │       code = "ANALYSIS_NOT_FOUND"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           analysis_id: Optional[str] = None,
│   │           user_id: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, resource="analysis", resource_id=analysis_id, **kwargs)
│   │           
│   │           if user_id:
│   │               self.details["user_id"] = user_id
│   │   
│   │   
│   │   class AnalysisNotCancellableError(ConflictError):
│   │       """Analysis cannot be cancelled in its current state."""
│   │       
│   │       code = "ANALYSIS_NOT_CANCELLABLE"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           analysis_id: Optional[str] = None,
│   │           current_status: Optional[str] = None,
│   │           reason: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │           
│   │           if analysis_id:
│   │               self.details["analysis_id"] = analysis_id
│   │           if current_status:
│   │               self.details["current_status"] = current_status
│   │           if reason:
│   │               self.details["reason"] = reason
│   │   
│   │   
│   │   class NoCurrentAnalysisError(NotFoundError):
│   │       """No completed analysis available."""
│   │       
│   │       code = "NO_CURRENT_ANALYSIS"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str = "No completed analysis available",
│   │           *,
│   │           user_id: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, resource="current_analysis", **kwargs)
│   │           
│   │           if user_id:
│   │               self.details["user_id"] = user_id
│   │   ```
│   │   
│   ├── base.py
│   │   
│   │   ```py
│   │   # -------------------------------
│   │   # shared/errors/base.py
│   │   # -------------------------------
│   │   
│   │   """
│   │   Base error classes for the glam-app error hierarchy.
│   │   
│   │   This module defines the fundamental error types that all other
│   │   errors inherit from, following a three-tier model:
│   │   1. GlamBaseError - Root of all application errors
│   │   2. InfrastructureError - External system failures
│   │   3. DomainError - Business logic violations
│   │   """
│   │   
│   │   from typing import Any, Dict, Optional
│   │   
│   │   
│   │   class GlamBaseError(Exception):
│   │       """
│   │       Base class for all glam-app errors.
│   │   
│   │       Attributes:
│   │           code: Stable error code for clients (e.g., "VALIDATION_ERROR")
│   │           status: HTTP status code (default 500)
│   │           message: Human-readable error message
│   │           details: Additional error context
│   │           __cause__: Original exception if wrapped
│   │       """
│   │   
│   │       code: str = "INTERNAL_ERROR"
│   │       status: int = 500
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           code: Optional[str] = None,
│   │           status: Optional[int] = None,
│   │           details: Optional[Dict[str, Any]] = None,
│   │           cause: Optional[Exception] = None
│   │       ):
│   │           super().__init__(message)
│   │   
│   │           if code is not None:
│   │               self.code = code
│   │           if status is not None:
│   │               self.status = status
│   │   
│   │           self.message = message
│   │           self.details = details or {}
│   │   
│   │           # Preserve the original exception chain
│   │           if cause is not None:
│   │               self.__cause__ = cause
│   │   
│   │       def to_dict(self) -> Dict[str, Any]:
│   │           """Convert error to dictionary for JSON serialization."""
│   │           result: Dict[str, Any] = {
│   │               "code": self.code,
│   │               "message": self.message,
│   │           }
│   │   
│   │           if self.details:
│   │               result["details"] = self.details
│   │   
│   │           return result
│   │   
│   │   
│   │   class InfrastructureError(GlamBaseError):
│   │       """
│   │       Infrastructure/external system errors.
│   │   
│   │       These are failures in external dependencies like databases,
│   │       APIs, message queues, etc. They may be retryable.
│   │       """
│   │   
│   │       code = "INFRASTRUCTURE_ERROR"
│   │       status = 503  # Service Unavailable
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           service: Optional[str] = None,
│   │           retryable: bool = True,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │   
│   │           if service:
│   │               self.details["service"] = service
│   │   
│   │           self.details["retryable"] = retryable
│   │           self.retryable = retryable
│   │   
│   │   
│   │   class DomainError(GlamBaseError):
│   │       """
│   │       Domain/business logic errors.
│   │   
│   │       These represent violations of business rules or invalid
│   │       operations within the application domain.
│   │       """
│   │   
│   │       code = "DOMAIN_ERROR"
│   │       status = 400  # Bad Request
│   │   
│   │   
│   │   # Common domain errors used across services
│   │   
│   │   
│   │   class ValidationError(DomainError):
│   │       """Invalid request data or parameters."""
│   │   
│   │       code = "VALIDATION_ERROR"
│   │       status = 422  # Unprocessable Entity
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           field: Optional[str] = None,
│   │           value: Optional[Any] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │   
│   │           if field:
│   │               self.details["field"] = field
│   │           if value is not None:
│   │               self.details["value"] = str(value)
│   │   
│   │   
│   │   class NotFoundError(DomainError):
│   │       """Requested resource not found."""
│   │   
│   │       code = "NOT_FOUND"
│   │       status = 404
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           resource: Optional[str] = None,
│   │           resource_id: Optional[Any] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │   
│   │           if resource:
│   │               self.details["resource"] = resource
│   │           if resource_id is not None:
│   │               self.details["resource_id"] = str(resource_id)
│   │   
│   │   
│   │   class ConflictError(DomainError):
│   │       """Operation conflicts with current state."""
│   │   
│   │       code = "CONFLICT"
│   │       status = 409
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           conflicting_resource: Optional[str] = None,
│   │           current_state: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │   
│   │           if conflicting_resource:
│   │               self.details["conflicting_resource"] = conflicting_resource
│   │           if current_state:
│   │               self.details["current_state"] = current_state
│   │   
│   │   
│   │   class UnauthorizedError(DomainError):
│   │       """Authentication required or failed."""
│   │   
│   │       code = "UNAUTHORIZED"
│   │       status = 401
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str = "Authentication required",
│   │           *,
│   │           auth_type: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │   
│   │           if auth_type:
│   │               self.details["auth_type"] = auth_type
│   │   
│   │   
│   │   class ForbiddenError(DomainError):
│   │       """Authenticated but insufficient permissions."""
│   │   
│   │       code = "FORBIDDEN"
│   │       status = 403
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str = "Insufficient permissions",
│   │           *,
│   │           required_permission: Optional[str] = None,
│   │           resource: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │   
│   │           if required_permission:
│   │               self.details["required_permission"] = required_permission
│   │           if resource:
│   │               self.details["resource"] = resource
│   │   
│   │   
│   │   class RateLimitedError(DomainError):
│   │       """Too many requests."""
│   │   
│   │       code = "RATE_LIMITED"
│   │       status = 429
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str = "Rate limit exceeded",
│   │           *,
│   │           limit: Optional[int] = None,
│   │           window: Optional[str] = None,
│   │           retry_after: Optional[int] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │   
│   │           if limit:
│   │               self.details["limit"] = limit
│   │           if window:
│   │               self.details["window"] = window
│   │           if retry_after:
│   │               self.details["retry_after"] = retry_after
│   │   
│   │   
│   │   class ServiceUnavailableError(InfrastructureError):
│   │       """Service temporarily unavailable."""
│   │   
│   │       code = "SERVICE_UNAVAILABLE"
│   │       status = 503
│   │   
│   │   
│   │   class RequestTimeoutError(InfrastructureError):
│   │       """Operation timed out."""
│   │   
│   │       code = "TIMEOUT"
│   │       status = 504
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           timeout_seconds: Optional[float] = None,
│   │           operation: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │   
│   │           if timeout_seconds:
│   │               self.details["timeout_seconds"] = timeout_seconds
│   │           if operation:
│   │               self.details["operation"] = operation
│   │   
│   │   
│   │   class InternalError(GlamBaseError):
│   │       """Unexpected internal server error."""
│   │   
│   │       code = "INTERNAL_ERROR"
│   │       status = 500
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str = "An unexpected error occurred",
│   │           *,
│   │           error_id: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           # Never expose internal details in production
│   │           super().__init__(message, **kwargs)
│   │   
│   │           if error_id:
│   │               self.details["error_id"] = error_id
│   │   ```
│   │   
│   ├── catalog.py
│   │   
│   │   ```py
│   │   
│   │   # -------------------------------
│   │   # shared/errors/catalog.py
│   │   # -------------------------------
│   │   
│   │   """Catalog service specific errors."""
│   │   
│   │   from typing import Optional
│   │   from .base import ConflictError, NotFoundError
│   │   
│   │   
│   │   class SyncInProgressError(ConflictError):
│   │       """Another sync operation is already running."""
│   │       
│   │       code = "SYNC_IN_PROGRESS"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str = "Another sync is already in progress",
│   │           *,
│   │           current_sync_id: Optional[str] = None,
│   │           merchant_id: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │           
│   │           if current_sync_id:
│   │               self.details["current_sync_id"] = current_sync_id
│   │           if merchant_id:
│   │               self.details["merchant_id"] = merchant_id
│   │   
│   │   
│   │   class SyncNotFoundError(NotFoundError):
│   │       """Sync operation not found."""
│   │       
│   │       code = "SYNC_NOT_FOUND"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           sync_id: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, resource="sync", resource_id=sync_id, **kwargs)
│   │   
│   │   
│   │   class SyncNotResumableError(ConflictError):
│   │       """Sync cannot be resumed in its current state."""
│   │       
│   │       code = "SYNC_NOT_RESUMABLE"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           sync_id: Optional[str] = None,
│   │           sync_status: Optional[str] = None,
│   │           reason: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │           
│   │           if sync_id:
│   │               self.details["sync_id"] = sync_id
│   │           if sync_status:
│   │               self.details["sync_status"] = sync_status
│   │           if reason:
│   │               self.details["reason"] = reason
│   │   
│   │   
│   │   class SyncNotCancellableError(ConflictError):
│   │       """Sync cannot be cancelled in its current state."""
│   │       
│   │       code = "SYNC_NOT_CANCELLABLE"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           sync_id: Optional[str] = None,
│   │           sync_status: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │           
│   │           if sync_id:
│   │               self.details["sync_id"] = sync_id
│   │           if sync_status:
│   │               self.details["sync_status"] = sync_status
│   │   
│   │   
│   │   class ItemNotFoundError(NotFoundError):
│   │       """Item not found in catalog."""
│   │       
│   │       code = "ITEM_NOT_FOUND"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           item_id: Optional[str] = None,
│   │           merchant_id: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, resource="item", resource_id=item_id, **kwargs)
│   │           
│   │           if merchant_id:
│   │               self.details["merchant_id"] = merchant_id
│   │   
│   │   
│   │   class ParentSyncNotFoundError(NotFoundError):
│   │       """Parent sync operation not found for resume."""
│   │       
│   │       code = "PARENT_SYNC_NOT_FOUND"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           parent_sync_id: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(
│   │               message,
│   │               resource="parent_sync",
│   │               resource_id=parent_sync_id,
│   │               **kwargs
│   │           )
│   │   ```
│   │   
│   ├── infrastructure.py
│   │   
│   │   ```py
│   │   # -------------------------------
│   │   # shared/errors/infrastructure.py
│   │   # -------------------------------
│   │   
│   │   """Infrastructure-specific error classes."""
│   │   
│   │   from typing import Optional
│   │   from .base import InfrastructureError
│   │   
│   │   
│   │   class DatabaseError(InfrastructureError):
│   │       """Database operation failed."""
│   │   
│   │       code = "DATABASE_ERROR"
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           operation: Optional[str] = None,
│   │           table: Optional[str] = None,
│   │           error_code: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, service="database", **kwargs)
│   │   
│   │           if operation:
│   │               self.details["operation"] = operation
│   │           if table:
│   │               self.details["table"] = table
│   │           if error_code:
│   │               self.details["error_code"] = error_code
│   │   
│   │   
│   │   class RedisError(InfrastructureError):
│   │       """Redis operation failed."""
│   │   
│   │       code = "REDIS_ERROR"
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           operation: Optional[str] = None,
│   │           key: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, service="redis", **kwargs)
│   │   
│   │           if operation:
│   │               self.details["operation"] = operation
│   │           if key:
│   │               self.details["key"] = key
│   │   
│   │   
│   │   class S3Error(InfrastructureError):
│   │       """S3 operation failed."""
│   │   
│   │       code = "S3_ERROR"
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           operation: Optional[str] = None,
│   │           bucket: Optional[str] = None,
│   │           key: Optional[str] = None,
│   │           error_code: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, service="s3", **kwargs)
│   │   
│   │           if operation:
│   │               self.details["operation"] = operation
│   │           if bucket:
│   │               self.details["bucket"] = bucket
│   │           if key:
│   │               self.details["key"] = key
│   │           if error_code:
│   │               self.details["error_code"] = error_code
│   │   
│   │   
│   │   class UpstreamServiceError(InfrastructureError):
│   │       """Upstream service call failed."""
│   │   
│   │       code = "UPSTREAM_SERVICE_ERROR"
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           upstream_service: Optional[str] = None,
│   │           upstream_status: Optional[int] = None,
│   │           upstream_error: Optional[str] = None,
│   │           endpoint: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, service=upstream_service, **kwargs)
│   │   
│   │           if upstream_status:
│   │               self.details["upstream_status"] = upstream_status
│   │           if upstream_error:
│   │               self.details["upstream_error"] = upstream_error
│   │           if endpoint:
│   │               self.details["endpoint"] = endpoint
│   │   
│   │   
│   │   class CircuitOpenError(InfrastructureError):
│   │       """Circuit breaker is open."""
│   │   
│   │       code = "CIRCUIT_OPEN"
│   │       status = 503
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           service_name: Optional[str] = None,
│   │           failure_count: Optional[int] = None,
│   │           open_until: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(
│   │               message,
│   │               service=service_name,
│   │               retryable=False,  # Don't retry when circuit is open
│   │               **kwargs
│   │           )
│   │   
│   │           if failure_count:
│   │               self.details["failure_count"] = failure_count
│   │           if open_until:
│   │               self.details["open_until"] = open_until
│   │   
│   │   
│   │   class MessageBusError(InfrastructureError):
│   │       """Message bus operation failed."""
│   │   
│   │       code = "MESSAGE_BUS_ERROR"
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           operation: Optional[str] = None,
│   │           stream: Optional[str] = None,
│   │           subject: Optional[str] = None,
│   │           subject: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, service="nats", **kwargs)
│   │   
│   │           if operation:
│   │               self.details["operation"] = operation
│   │           if stream:
│   │               self.details["stream"] = stream
│   │           if subject:
│   │               self.details["subject"] = subject
│   │           if subject:
│   │               self.details["subject"] = subject
│   │   ```
│   │   
│   ├── notification.py
│   │   
│   │   ```py
│   │   # -------------------------------
│   │   # shared/errors/notification.py
│   │   # -------------------------------
│   │   
│   │   from uuid import UUID
│   │   
│   │   """Notification service specific errors."""
│   │   
│   │   from typing import Optional
│   │   from .base import NotFoundError, ValidationError, InfrastructureError, ConflictError
│   │   
│   │   
│   │   class NotificationNotFoundError(NotFoundError):
│   │       """Notification not found."""
│   │   
│   │       code = "NOTIFICATION_NOT_FOUND"
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           notification_id: Optional[UUID] = None,
│   │           merchant_id: Optional[UUID] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(
│   │               message, resource="notification", resource_id=notification_id, **kwargs
│   │           )
│   │   
│   │           if merchant_id:
│   │               self.details["merchant_id"] = merchant_id
│   │   
│   │   
│   │   class TemplateNotFoundError(NotFoundError):
│   │       """Email template not found."""
│   │   
│   │       code = "TEMPLATE_NOT_FOUND"
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           template_name: Optional[str] = None,
│   │           template_type: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(
│   │               message, resource="template", resource_id=template_name, **kwargs
│   │           )
│   │   
│   │           if template_type:
│   │               self.details["template_type"] = template_type
│   │   
│   │   
│   │   class TemplateRenderError(ValidationError):
│   │       """Failed to render email template."""
│   │   
│   │       code = "TEMPLATE_RENDER_ERROR"
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           template_name: Optional[str] = None,
│   │           missing_variables: Optional[list] = None,
│   │           render_error: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │   
│   │           if template_name:
│   │               self.details["template_name"] = template_name
│   │           if missing_variables:
│   │               self.details["missing_variables"] = missing_variables
│   │           if render_error:
│   │               self.details["render_error"] = render_error
│   │   
│   │   
│   │   class InvalidRecipientError(ValidationError):
│   │       """Invalid recipient email address."""
│   │   
│   │       code = "INVALID_RECIPIENT"
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           recipient: Optional[str] = None,
│   │           reason: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, field="recipient", value=recipient, **kwargs)
│   │   
│   │           if reason:
│   │               self.details["reason"] = reason
│   │   
│   │   
│   │   class PreferencesNotFoundError(NotFoundError):
│   │       """Notification preferences not found."""
│   │   
│   │       code = "PREFERENCES_NOT_FOUND"
│   │   
│   │       def __init__(self, message: str, *, user_id: Optional[str] = None, **kwargs):
│   │           super().__init__(
│   │               message, resource="notification_preferences", resource_id=user_id, **kwargs
│   │           )
│   │   
│   │   
│   │   class EmailProviderError(InfrastructureError):
│   │       """Email provider API error."""
│   │   
│   │       code = "EMAIL_PROVIDER_ERROR"
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           provider: Optional[str] = None,
│   │           provider_error_code: Optional[str] = None,
│   │           provider_message: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, service=provider, **kwargs)
│   │   
│   │           if provider_error_code:
│   │               self.details["provider_error_code"] = provider_error_code
│   │           if provider_message:
│   │               self.details["provider_message"] = provider_message
│   │   
│   │   
│   │   class UnsubscribedError(ConflictError):
│   │       """Recipient has unsubscribed."""
│   │   
│   │       code = "UNSUBSCRIBED"
│   │   
│   │       def __init__(
│   │           self,
│   │           message: str = "Recipient has unsubscribed from notifications",
│   │           *,
│   │           user_id: Optional[str] = None,
│   │           notification_type: Optional[str] = None,
│   │           unsubscribed_at: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │   
│   │           if user_id:
│   │               self.details["user_id"] = user_id
│   │           if notification_type:
│   │               self.details["notification_type"] = notification_type
│   │           if unsubscribed_at:
│   │               self.details["unsubscribed_at"] = unsubscribed_at
│   │   ```
│   │   
│   ├── profile.py
│   │   
│   │   ```py
│   │   
│   │   # -------------------------------
│   │   # shared/errors/profile.py
│   │   # -------------------------------
│   │   
│   │   """Profile service specific errors."""
│   │   
│   │   from typing import Optional
│   │   from .base import NotFoundError, ConflictError, DomainError
│   │   
│   │   
│   │   class ProfileNotFoundError(NotFoundError):
│   │       """User profile not found."""
│   │       
│   │       code = "PROFILE_NOT_FOUND"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           user_id: Optional[str] = None,
│   │           profile_id: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, resource="profile", **kwargs)
│   │           
│   │           if user_id:
│   │               self.details["user_id"] = user_id
│   │           if profile_id:
│   │               self.details["profile_id"] = profile_id
│   │   
│   │   
│   │   class ProfileAlreadyExistsError(ConflictError):
│   │       """Profile already exists for this user."""
│   │       
│   │       code = "PROFILE_ALREADY_EXISTS"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           user_id: Optional[str] = None,
│   │           existing_profile_id: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(
│   │               message,
│   │               conflicting_resource="profile",
│   │               **kwargs
│   │           )
│   │           
│   │           if user_id:
│   │               self.details["user_id"] = user_id
│   │           if existing_profile_id:
│   │               self.details["existing_profile_id"] = existing_profile_id
│   │   
│   │   
│   │   class ProfileCreationFailedError(DomainError):
│   │       """Failed to create profile."""
│   │       
│   │       code = "PROFILE_CREATION_FAILED"
│   │       status = 422
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           user_id: Optional[str] = None,
│   │           reason: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │           
│   │           if user_id:
│   │               self.details["user_id"] = user_id
│   │           if reason:
│   │               self.details["reason"] = reason
│   │   ```
│   │   
│   ├── selfie.py
│   │   
│   │   ```py
│   │   
│   │   # -------------------------------
│   │   # shared/errors/selfie.py
│   │   # -------------------------------
│   │   
│   │   """Selfie service specific errors."""
│   │   
│   │   from typing import Optional, List
│   │   from .base import NotFoundError, ValidationError
│   │   
│   │   
│   │   class SelfieNotFoundError(NotFoundError):
│   │       """Selfie not found."""
│   │       
│   │       code = "SELFIE_NOT_FOUND"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           selfie_id: Optional[str] = None,
│   │           user_id: Optional[str] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, resource="selfie", resource_id=selfie_id, **kwargs)
│   │           
│   │           if user_id:
│   │               self.details["user_id"] = user_id
│   │   
│   │   
│   │   class InvalidImageFormatError(ValidationError):
│   │       """Image format not supported."""
│   │       
│   │       code = "INVALID_IMAGE_FORMAT"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           provided_format: Optional[str] = None,
│   │           supported_formats: Optional[List[str]] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │           
│   │           if provided_format:
│   │               self.details["provided_format"] = provided_format
│   │           if supported_formats:
│   │               self.details["supported_formats"] = supported_formats
│   │   
│   │   
│   │   class ImageTooLargeError(ValidationError):
│   │       """Image exceeds size limit."""
│   │       
│   │       code = "IMAGE_TOO_LARGE"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           size_bytes: Optional[int] = None,
│   │           max_size_bytes: Optional[int] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │           
│   │           if size_bytes:
│   │               self.details["size_bytes"] = size_bytes
│   │           if max_size_bytes:
│   │               self.details["max_size_bytes"] = max_size_bytes
│   │   
│   │   
│   │   class ImageTooSmallError(ValidationError):
│   │       """Image below minimum dimensions."""
│   │       
│   │       code = "IMAGE_TOO_SMALL"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str,
│   │           *,
│   │           width: Optional[int] = None,
│   │           height: Optional[int] = None,
│   │           min_width: Optional[int] = None,
│   │           min_height: Optional[int] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │           
│   │           if width:
│   │               self.details["width"] = width
│   │           if height:
│   │               self.details["height"] = height
│   │           if min_width:
│   │               self.details["min_width"] = min_width
│   │           if min_height:
│   │               self.details["min_height"] = min_height
│   │   
│   │   
│   │   class NoFaceDetectedError(ValidationError):
│   │       """No face detected in image."""
│   │       
│   │       code = "NO_FACE_DETECTED"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str = "No face detected in the image",
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │   
│   │   
│   │   class MultipleFacesDetectedError(ValidationError):
│   │       """Multiple faces detected."""
│   │       
│   │       code = "MULTIPLE_FACES_DETECTED"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str = "Multiple faces detected in the image",
│   │           *,
│   │           face_count: Optional[int] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │           
│   │           if face_count:
│   │               self.details["face_count"] = face_count
│   │   
│   │   
│   │   class PoorImageQualityError(ValidationError):
│   │       """Image quality too low for analysis."""
│   │       
│   │       code = "POOR_IMAGE_QUALITY"
│   │       
│   │       def __init__(
│   │           self,
│   │           message: str = "Image quality too low for analysis",
│   │           *,
│   │           quality_score: Optional[float] = None,
│   │           min_quality_score: Optional[float] = None,
│   │           quality_issues: Optional[List[str]] = None,
│   │           **kwargs
│   │       ):
│   │           super().__init__(message, **kwargs)
│   │           
│   │           if quality_score is not None:
│   │               self.details["quality_score"] = quality_score
│   │           if min_quality_score is not None:
│   │               self.details["min_quality_score"] = min_quality_score
│   │           if quality_issues:
│   │               self.details["quality_issues"] = quality_issues
│   │   ```
│   │   
│   └── utils.py
│       
│       ```py
│       # -------------------------------
│       # shared/errors/utils.py
│       # -------------------------------
│       
│       """
│       Utility functions for error handling and classification.
│       
│       This module provides helpers for wrapping external errors,
│       classifying HTTP errors, and determining retry behavior.
│       """
│       
│       from typing import Type, Callable, TypeVar
│       import httpx
│       import asyncio
│       from functools import wraps
│       
│       from .base import (
│           GlamBaseError,
│           InfrastructureError,
│           RequestTimeoutError,
│           ServiceUnavailableError,
│           RateLimitedError,
│       )
│       from .infrastructure import UpstreamServiceError
│       
│       T = TypeVar("T")
│       
│       
│       def wrap_external_error(
│           error_class: Type[GlamBaseError], message: str, *, cause: Exception, **kwargs
│       ) -> GlamBaseError:
│           """
│           Wrap an external exception in a domain-specific error.
│       
│           This preserves the original exception chain while providing
│           a clean domain error for upper layers.
│       
│           Args:
│               error_class: The error class to wrap with
│               message: Human-readable error message
│               cause: The original exception
│               **kwargs: Additional arguments for the error class
│       
│           Returns:
│               Instance of error_class with proper cause chain
│           """
│           return error_class(message, cause=cause, **kwargs)
│       
│       
│       def classify_http_error(
│           exc: httpx.HTTPError, *, service_name: str = "upstream"
│       ) -> InfrastructureError | RateLimitedError:
│           """
│           Classify HTTP errors into appropriate infrastructure errors.
│       
│           Args:
│               exc: The HTTP exception to classify
│               service_name: Name of the upstream service
│       
│           Returns:
│               Appropriate InfrastructureError subclass
│           """
│           if isinstance(exc, httpx.TimeoutException):
│               return RequestTimeoutError(
│                   f"Request to {service_name} timed out", cause=exc, operation="http_request"
│               )
│       
│           if isinstance(exc, httpx.HTTPStatusError):
│               status = exc.response.status_code
│       
│               if status == 429:
│                   # Extract retry-after if available
│                   retry_after = exc.response.headers.get("Retry-After")
│                   return RateLimitedError(
│                       f"Rate limited by {service_name}",
│                       cause=exc,
│                       retry_after=int(retry_after) if retry_after else None,
│                   )
│       
│               if status == 503:
│                   return ServiceUnavailableError(
│                       f"{service_name} is temporarily unavailable", cause=exc
│                   )
│       
│               if 500 <= status < 600:
│                   return UpstreamServiceError(
│                       f"{service_name} returned {status}",
│                       cause=exc,
│                       upstream_service=service_name,
│                       upstream_status=status,
│                       endpoint=str(exc.request.url),
│                   )
│       
│               # 4xx errors - usually client errors, not retryable
│               return UpstreamServiceError(
│                   f"{service_name} rejected request: {status}",
│                   cause=exc,
│                   upstream_service=service_name,
│                   upstream_status=status,
│                   endpoint=str(exc.request.url),
│                   retryable=False,
│               )
│       
│           # Generic connection errors
│           return InfrastructureError(
│               f"Failed to connect to {service_name}", cause=exc, service=service_name
│           )
│       
│       
│       def is_retryable_error(exc: Exception) -> bool:
│           """
│           Determine if an error should be retried.
│       
│           Args:
│               exc: The exception to check
│       
│           Returns:
│               True if the error is retryable
│           """
│           if isinstance(exc, InfrastructureError):
│               return exc.retryable
│       
│           # Specific exceptions that are retryable
│           retryable_types = (
│               asyncio.TimeoutError,
│               ConnectionError,
│               RequestTimeoutError,
│           )
│       
│           return isinstance(exc, retryable_types)
│       
│       
│       def with_error_mapping(
│           mappings: dict[Type[Exception], Type[GlamBaseError]],
│           *,
│           default_error: Type[GlamBaseError] = InfrastructureError,
│           default_message: str = "Operation failed",
│       ):
│           """
│           Decorator to automatically map exceptions to domain errors.
│       
│           Example:
│               @with_error_mapping({
│                   FileNotFoundError: NotFoundError,
│                   PermissionError: ForbiddenError,
│               })
│               async def read_file(path: str):
│                   ...
│       
│           Args:
│               mappings: Dict mapping exception types to error classes
│               default_error: Error class for unmapped exceptions
│               default_message: Default message for unmapped errors
│           """
│       
│           def decorator(func: Callable[..., T]) -> Callable[..., T]:
│               @wraps(func)
│               async def async_wrapper(*args, **kwargs):
│                   try:
│                       result = func(*args, **kwargs)
│                       if asyncio.iscoroutine(result):
│                           return await result
│                       return result
│                   except Exception as exc:
│                       # Check if we have a mapping for this exception
│                       for exc_type, error_class in mappings.items():
│                           if isinstance(exc, exc_type):
│                               raise error_class(
│                                   str(exc) or default_message, cause=exc
│                               ) from exc
│       
│                       # No mapping found - use default
│                       if isinstance(exc, GlamBaseError):
│                           # Already our error type - let it bubble
│                           raise
│       
│                       raise default_error(str(exc) or default_message, cause=exc) from exc
│       
│               @wraps(func)
│               def sync_wrapper(*args, **kwargs):
│                   try:
│                       return func(*args, **kwargs)
│                   except Exception as exc:
│                       # Same logic as async version
│                       for exc_type, error_class in mappings.items():
│                           if isinstance(exc, exc_type):
│                               raise error_class(
│                                   str(exc) or default_message, cause=exc
│                               ) from exc
│       
│                       if isinstance(exc, GlamBaseError):
│                           raise
│       
│                       raise default_error(str(exc) or default_message, cause=exc) from exc
│       
│               # Return appropriate wrapper based on function type
│               if asyncio.iscoroutinefunction(func):
│                   return async_wrapper  # type: ignore
│               else:
│                   return sync_wrapper
│       
│           return decorator
│       ```
│       
├── events/
│   ├── catalog/
│   │   ├── __init__.py
│   │   └── types.py
│   │       
│   │       ```py
│   │       from pydantic import BaseModel, Field
│   │       from typing import Dict, Optional, List, Any
│   │       
│   │       from uuid import UUID
│   │       
│   │       
│   │       from ..base import EventWrapper
│   │       
│   │       class CatalogCommands:
│   │           """Catalog command types"""
│   │           CATALOG_SYNC_INITIAL = "cmd.catalog.sync_initial"
│   │           CATALOG_SYNC_UPDATE = "cmd.catalog.sync_update"
│   │           CATALOG_PROCESS_IMAGES = "cmd.catalog.process_images"
│   │           CATALOG_ANALYZE_ITEMS = "cmd.catalog.analyze_items"
│   │           CATALOG_ENRICH_WITH_AI = "cmd.catalog.enrich_with_ai"
│   │           
│   │       class CatalogEvents:
│   │           """Catalog event types"""
│   │           CATALOG_SYNC_STARTED = "evt.catalog.sync_started"
│   │           CATALOG_SYNC_COMPLETED = "evt.catalog.sync_completed"
│   │           CATALOG_SYNC_FAILED = "evt.catalog.sync_failed"
│   │           CATALOG_IMAGES_PROCESSED = "evt.catalog.images_processed"
│   │           CATALOG_ITEMS_ANALYZED = "evt.catalog.items_analyzed"
│   │           CATALOG_AI_ENRICHED = "evt.catalog.ai_enriched"
│   │           
│   │       class CatalogSyncInitialPayload(BaseModel):
│   │           """Payload for initial catalog sync command"""
│   │           merchant_id: UUID
│   │           source: str  # e.g. "shopify", "woocommerce"
│   │           sync_type: str  # e.g. "full", "incremental"
│   │           webhook_url: Optional[str] = None  # Optional webhook for updates
│   │           
│   │       class CatalogSyncUpdatePayload(BaseModel):
│   │           """Payload for catalog sync update command"""
│   │           merchant_id: UUID
│   │           items: List[Dict[str, Any]]  # List of items to update
│   │           job_id: Optional[str] = None  # Optional job ID for tracking
│   │           
│   │       class CatalogProcessImagesPayload(BaseModel):
│   │           """Payload for processing catalog images"""
│   │           merchant_id: UUID
│   │           items: List[Dict[str, Any]]  # List of items with images to process
│   │           job_id: Optional[str] = None  # Optional job ID for tracking
│   │           
│   │       class CatalogAnalyzeItemsPayload(BaseModel):
│   │           """Payload for analyzing catalog items"""
│   │           merchant_id: UUID
│   │           items: List[Dict[str, Any]]  # List of items to analyze
│   │           job_id: Optional[str] = None  # Optional job ID for tracking
│   │           
│   │       class CatalogEnrichWithAIPayload(BaseModel):
│   │           """Payload for enriching catalog with AI"""
│   │           merchant_id: UUID
│   │           items: List[Dict[str, Any]]  # List of items to enrich
│   │           job_id: Optional[str] = None  # Optional job ID for tracking
│   │           
│   │       class CatalogSyncInitialCommand(EventWrapper):
│   │           """Command to initiate initial catalog sync"""
│   │           subject: str = CatalogCommands.CATALOG_SYNC_INITIAL
│   │           data: CatalogSyncInitialPayload
│   │           
│   │       class CatalogSyncUpdateCommand(EventWrapper):
│   │           """Command to update catalog with new items"""
│   │           subject: str = CatalogCommands.CATALOG_SYNC_UPDATE
│   │           data: CatalogSyncUpdatePayload
│   │           
│   │       class CatalogProcessImagesCommand(EventWrapper):
│   │           """Command to process catalog images"""
│   │           subject: str = CatalogCommands.CATALOG_PROCESS_IMAGES
│   │           data: CatalogProcessImagesPayload
│   │           
│   │       class CatalogAnalyzeItemsCommand(EventWrapper):
│   │           """Command to analyze catalog items"""
│   │           subject: str = CatalogCommands.CATALOG_ANALYZE_ITEMS
│   │           data: CatalogAnalyzeItemsPayload
│   │           
│   │       class CatalogEnrichWithAICommand(EventWrapper):
│   │           """Command to enrich catalog with AI"""
│   │           subject: str = CatalogCommands.CATALOG_ENRICH_WITH_AI
│   │           data: CatalogEnrichWithAIPayload
│   │           
│   │       class CatalogSyncStartedEvent(EventWrapper):
│   │           """Event emitted when catalog sync starts"""
│   │           subject: str = CatalogEvents.CATALOG_SYNC_STARTED
│   │           data: Dict[str, Any] = Field(default_factory=dict)  # Additional metadata if needed
│   │           
│   │       class CatalogSyncCompletedEvent(EventWrapper):
│   │           """Event emitted when catalog sync completes"""
│   │           subject: str = CatalogEvents.CATALOG_SYNC_COMPLETED
│   │           data: Dict[str, Any] = Field(default_factory=dict)  # Additional metadata if needed
│   │           
│   │       class CatalogSyncFailedEvent(EventWrapper):
│   │           """Event emitted when catalog sync fails"""
│   │           subject: str = CatalogEvents.CATALOG_SYNC_FAILED
│   │           data: Dict[str, Any] = Field(default_factory=dict)  # Additional metadata if needed
│   │           error_message: Optional[str] = None  # Optional error message for failure
│   │           
│   │       class CatalogImagesProcessedEvent(EventWrapper):
│   │           """Event emitted when catalog images are processed"""
│   │           subject: str = CatalogEvents.CATALOG_IMAGES_PROCESSED
│   │           data: Dict[str, Any] = Field(default_factory=dict)  # Additional metadata if needed
│   │           
│   │       class CatalogItemsAnalyzedEvent(EventWrapper):
│   │           """Event emitted when catalog items are analyzed"""
│   │           subject: str = CatalogEvents.CATALOG_ITEMS_ANALYZED
│   │           data: Dict[str, Any] = Field(default_factory=dict)  # Additional metadata if needed
│   │           
│   │       class CatalogAIEnrichedEvent(EventWrapper):
│   │           """Event emitted when catalog items are enriched with AI"""
│   │           subject: str = CatalogEvents.CATALOG_AI_ENRICHED
│   │           data: Dict[str, Any] = Field(default_factory=dict)  # Additional metadata if needed
│   │           enrichment_details: Optional[Dict[str, Any]] = None  # Optional details about the enrichment process
│   │           
│   │           
│   │       ```
│   │       
│   ├── credit/
│   │   ├── __init__.py
│   │   └── types.py
│   │       
│   │       ```py
│   │       # shared/events/notification/types.py
│   │       from pydantic import BaseModel, Field
│   │       from typing import Dict, List, Any
│   │       from datetime import datetime
│   │       from uuid import UUID
│   │       
│   │       from ..base import EventWrapper  # Now generic
│   │       from ..context import EventContext  # Keep this!
│   │       
│   │       class CreditEvents:
│   │           "Credit command types"
│   │           ORDER_UPDATED = "evt.order.updated" 
│   │           ACCOUNT_CREATED = "evt.account.created"
│   │           SUBSCRIPTION_RENEWED = "evt.subscription.renewed"
│   │           MERCHANT_CREATED="evt.merchant.created"
│   │       ```
│   │       
│   ├── notification/
│   │   ├── __init__.py
│   │   ├── helpers.py
│   │   │   
│   │   │   ```py
│   │   │   # shared/events/notification/helpers.py
│   │   │   from datetime import datetime, timezone
│   │   │   import hashlib
│   │   │   from typing import Dict, Optional, List, Any
│   │   │   from uuid import UUID
│   │   │   
│   │   │   from ..context import EventContext
│   │   │   from .types import (
│   │   │       Recipient,
│   │   │       SendEmailBulkCommand,
│   │   │       SendEmailCommand,
│   │   │   )
│   │   │   
│   │   │   
│   │   │   def create_send_email_command(
│   │   │       merchant_id: UUID,
│   │   │       merchant_domain: str,
│   │   │       recipient_email: str,
│   │   │       notification_type: str,
│   │   │       dynamic_content: Dict[str, Any],
│   │   │       unsubscribe_token: str,
│   │   │       idempotency_key: Optional[str] = None,
│   │   │       correlation_id: Optional[str] = None,
│   │   │       metadata: Optional[Dict[str, Any]] = None,
│   │   │   ) -> Dict[str, Any]:
│   │   │       """
│   │   │       Create a send email command with idempotency support and context.
│   │   │   
│   │   │       If no idempotency_key provided, generates one based on:
│   │   │       - merchant_id + notification_type + recipient_email + timestamp (hourly bucket)
│   │   │       This prevents duplicate emails within the same hour.
│   │   │       """
│   │   │       if not idempotency_key:
│   │   │           # Create deterministic key with hourly bucket to prevent duplicates
│   │   │           hour_bucket = datetime.now(timezone.utc).strftime("%Y%m%d%H")
│   │   │           key_data = f"{merchant_id}:{notification_type}:{recipient_email}:{hour_bucket}"
│   │   │           idempotency_key = f"send_{hashlib.sha256(key_data.encode()).hexdigest()[:16]}"
│   │   │   
│   │   │       # Create event context
│   │   │       context = EventContext.create(
│   │   │           subject="cmd.notification.send_email",
│   │   │           source_service=(
│   │   │               metadata.get("source_service", "unknown") if metadata else "unknown"
│   │   │           ),
│   │   │           correlation_id=correlation_id,
│   │   │           idempotency_key=idempotency_key,
│   │   │           metadata=metadata,
│   │   │       )
│   │   │   
│   │   │       recipient = Recipient(
│   │   │           merchant_id=merchant_id,
│   │   │           merchant_domain=merchant_domain,
│   │   │           email=recipient_email,
│   │   │           unsubscribe_token=unsubscribe_token,
│   │   │           dynamic_content=dynamic_content,
│   │   │       )
│   │   │   
│   │   │       command = SendEmailCommand.create_from_context(
│   │   │           context=context, notification_type=notification_type, recipient=recipient
│   │   │       )
│   │   │   
│   │   │       return command.to_event_dict()
│   │   │   
│   │   │   
│   │   │   def create_bulk_email_command(
│   │   │       notification_type: str,
│   │   │       recipients: List[Dict[str, Any]],
│   │   │       idempotency_key: Optional[str] = None,
│   │   │       correlation_id: Optional[str] = None,
│   │   │       metadata: Optional[Dict[str, Any]] = None,
│   │   │   ) -> Dict[str, Any]:
│   │   │       """
│   │   │       Create a bulk email command with idempotency support and context.
│   │   │   
│   │   │       If no idempotency_key provided, generates one based on:
│   │   │       - notification_type + recipient_count + timestamp
│   │   │       """
│   │   │       if not idempotency_key:
│   │   │           # Create unique key for this bulk operation
│   │   │           timestamp = datetime.now(timezone.utc).isoformat()
│   │   │           key_data = f"bulk:{notification_type}:{len(recipients)}:{timestamp}"
│   │   │           idempotency_key = f"bulk_{hashlib.sha256(key_data.encode()).hexdigest()[:16]}"
│   │   │   
│   │   │       # Create event context
│   │   │       context = EventContext.create(
│   │   │           subject="cmd.notification.bulk_send",
│   │   │           source_service=(
│   │   │               metadata.get("source_service", "unknown") if metadata else "unknown"
│   │   │           ),
│   │   │           correlation_id=correlation_id,
│   │   │           idempotency_key=idempotency_key,
│   │   │           metadata=metadata,
│   │   │       )
│   │   │   
│   │   │       # Convert recipient dicts to typed objects
│   │   │       typed_recipients = [
│   │   │           Recipient(
│   │   │               merchant_id=r["merchant_id"],
│   │   │               merchant_domain=r["merchant_domain"],
│   │   │               email=r["email"],
│   │   │               unsubscribe_token=r["unsubscribe_token"],
│   │   │               dynamic_content=r.get("dynamic_content", {}),
│   │   │           )
│   │   │           for r in recipients
│   │   │       ]
│   │   │   
│   │   │       command = SendEmailBulkCommand.create_from_context(
│   │   │           context=context,
│   │   │           notification_type=notification_type,
│   │   │           recipients=typed_recipients,
│   │   │       )
│   │   │   
│   │   │       return command.to_event_dict()
│   │   │   ```
│   │   │   
│   │   └── types.py
│   │       
│   │       ```py
│   │       # shared/events/notification/types.py
│   │       from pydantic import BaseModel, Field
│   │       from typing import Dict, List, Any
│   │       from datetime import datetime
│   │       from uuid import UUID
│   │       
│   │       from ..base import EventWrapper  # Now generic
│   │       from ..context import EventContext  # Keep this!
│   │       
│   │       
│   │       class NotificationCommands:
│   │           """Notification command types"""
│   │       
│   │           NOTIFICATION_SEND_EMAIL = "cmd.notification.send_email"
│   │           NOTIFICATION_SEND_BULK = "cmd.notification.bulk_send"
│   │       
│   │       
│   │       class NotificationEvents:
│   │           """Notification event types"""
│   │       
│   │           NOTIFICATION_EMAIL_SENT = "evt.notification.email.sent"
│   │           NOTIFICATION_EMAIL_FAILED = "evt.notification.email.failed"
│   │           NOTIFICATION_BULK_SEND_COMPLETED = "evt.notification.bulk_send.completed"
│   │       
│   │       
│   │       class Recipient(BaseModel):
│   │           merchant_id: UUID
│   │           merchant_domain: str
│   │           email: str
│   │           unsubscribe_token: str
│   │           dynamic_content: Dict[str, Any] = Field(default_factory=dict)
│   │       
│   │       
│   │       class SendEmailCommandPayload(BaseModel):
│   │           """Payload for sending a single email"""
│   │       
│   │           notification_type: str
│   │           recipient: Recipient
│   │       
│   │       
│   │       class SendEmailBulkCommandPayload(BaseModel):
│   │           """Payload for sending bulk emails by notification type"""
│   │       
│   │           notification_type: str
│   │           recipients: List[Recipient]
│   │       
│   │       
│   │       # Event payloads remain the same
│   │       class EmailSentEventPayload(BaseModel):
│   │           """Payload for NOTIFICATION_EMAIL_SENT event"""
│   │       
│   │           notification_id: UUID
│   │           merchant_id: UUID
│   │           notification_type: str
│   │           provider: str
│   │           provider_message_id: str
│   │           sent_at: datetime
│   │           metadata: Dict[str, Any] = Field(default_factory=dict)
│   │       
│   │           class Config:
│   │               json_encoders = {UUID: str, datetime: lambda v: v.isoformat()}
│   │       
│   │       
│   │       class EmailFailedEventPayload(BaseModel):
│   │           """Payload for NOTIFICATION_EMAIL_FAILED event"""
│   │       
│   │           notification_id: UUID
│   │           merchant_id: UUID
│   │           notification_type: str
│   │           error: str
│   │           error_code: str
│   │           retry_count: int
│   │           will_retry: bool
│   │           failed_at: datetime
│   │       
│   │       
│   │       class BulkCompletedEventPayload(BaseModel):
│   │           """Payload for bulk send completion"""
│   │       
│   │           bulk_job_id: UUID
│   │           notification_type: str
│   │           total_recipients: int
│   │           total_sent: int
│   │           total_failed: int
│   │           total_skipped: int
│   │           duration_seconds: float
│   │           completed_at: datetime
│   │       
│   │       
│   │       # Enhanced command wrappers that integrate with EventContext
│   │       # The only change is adding the Generic type parameter [PayloadType]
│   │       class SendEmailCommand(EventWrapper[SendEmailCommandPayload]):
│   │           """Command to send a single email"""
│   │       
│   │           subject: str = NotificationCommands.NOTIFICATION_SEND_EMAIL
│   │       
│   │           @classmethod
│   │           def create_from_context(
│   │               cls,
│   │               context: EventContext,  # Still using context!
│   │               notification_type: str,
│   │               recipient: Recipient,
│   │           ) -> "SendEmailCommand":
│   │               """Create command with context"""
│   │               return cls(
│   │                   subject=cls.subject,
│   │                   idempotency_key=context.idempotency_key,
│   │                   event_id=context.event_id,
│   │                   correlation_id=context.correlation_id,
│   │                   timestamp=context.timestamp,
│   │                   metadata=context.metadata,
│   │                   data=SendEmailCommandPayload(
│   │                       notification_type=notification_type, recipient=recipient
│   │                   ),
│   │               )
│   │       
│   │       
│   │       class SendEmailBulkCommand(EventWrapper[SendEmailBulkCommandPayload]):
│   │           """Command to send bulk emails with same notification type"""
│   │       
│   │           subject: str = NotificationCommands.NOTIFICATION_SEND_BULK
│   │       
│   │           @classmethod
│   │           def create_from_context(
│   │               cls,
│   │               context: EventContext,  # Still using context!
│   │               notification_type: str,
│   │               recipients: List[Recipient],
│   │           ) -> "SendEmailBulkCommand":
│   │               """Create bulk command with context"""
│   │               return cls(
│   │                   subject=cls.subject,
│   │                   idempotency_key=context.idempotency_key,
│   │                   event_id=context.event_id,
│   │                   correlation_id=context.correlation_id,
│   │                   timestamp=context.timestamp,
│   │                   metadata=context.metadata,
│   │                   data=SendEmailBulkCommandPayload(
│   │                       notification_type=notification_type, recipients=recipients
│   │                   ),
│   │               )
│   │       
│   │       
│   │       # Enhanced event wrappers
│   │       class EmailSentEvent(EventWrapper[EmailSentEventPayload]):
│   │           """Event emitted when an email is successfully sent"""
│   │       
│   │           subject: str = NotificationEvents.NOTIFICATION_EMAIL_SENT
│   │       
│   │           @classmethod
│   │           def create_from_context(
│   │               cls,
│   │               context: EventContext,  # Still using context!
│   │               payload: EmailSentEventPayload,
│   │           ) -> "EmailSentEvent":
│   │               """Create event with context"""
│   │               return cls(
│   │                   subject=cls.subject,
│   │                   idempotency_key=context.idempotency_key,
│   │                   event_id=context.event_id,
│   │                   correlation_id=context.correlation_id,
│   │                   timestamp=context.timestamp,
│   │                   metadata=context.metadata,
│   │                   data=payload,
│   │               )
│   │       
│   │       
│   │       class EmailDeliveryFailedEvent(EventWrapper[EmailFailedEventPayload]):
│   │           """Event emitted when email delivery fails"""
│   │       
│   │           subject: str = NotificationEvents.NOTIFICATION_EMAIL_FAILED
│   │       
│   │           @classmethod
│   │           def create_from_context(
│   │               cls,
│   │               context: EventContext,  # Still using context!
│   │               payload: EmailFailedEventPayload,
│   │           ) -> "EmailDeliveryFailedEvent":
│   │               """Create event with context"""
│   │               return cls(
│   │                   subject=cls.subject,
│   │                   idempotency_key=context.idempotency_key,
│   │                   event_id=context.event_id,
│   │                   correlation_id=context.correlation_id,
│   │                   timestamp=context.timestamp,
│   │                   metadata=context.metadata,
│   │                   data=payload,
│   │               )
│   │       
│   │       
│   │       class BulkSendCompletedEvent(EventWrapper[BulkCompletedEventPayload]):
│   │           """Event emitted when a bulk email send operation completes"""
│   │       
│   │           subject: str = NotificationEvents.NOTIFICATION_BULK_SEND_COMPLETED
│   │       
│   │           @classmethod
│   │           def create_from_context(
│   │               cls,
│   │               context: EventContext,  # Still using context!
│   │               payload: BulkCompletedEventPayload,
│   │           ) -> "BulkSendCompletedEvent":
│   │               """Create event with context"""
│   │               return cls(
│   │                   subject=cls.subject,
│   │                   idempotency_key=context.idempotency_key,
│   │                   event_id=context.event_id,
│   │                   correlation_id=context.correlation_id,
│   │                   timestamp=context.timestamp,
│   │                   metadata=context.metadata,
│   │                   data=payload,
│   │               )
│   │       ```
│   │       
│   ├── scheduler/
│   │   ├── __init__.py
│   │   └── types.py
│   │       
│   │       ```py
│   │       # shared/events/scheduler/types.py
│   │       from pydantic import BaseModel, Field
│   │       from typing import Dict, List, Any, Optional
│   │       from datetime import datetime
│   │       from uuid import UUID
│   │       from enum import Enum
│   │       
│   │       from shared.events.base import EventWrapper
│   │       from shared.events.context import EventContext
│   │       
│   │       
│   │       class ScheduleType(str, Enum):
│   │           """Schedule type enumeration"""
│   │           CRON = "cron"
│   │           INTERVAL = "interval"
│   │           ONE_TIME = "one_time"
│   │           IMMEDIATE = "immediate"
│   │       
│   │       
│   │       class ExecutionStatus(str, Enum):
│   │           """Execution status enumeration"""
│   │           PENDING = "pending"
│   │           RUNNING = "running"
│   │           SUCCESS = "success"
│   │           FAILED = "failed"
│   │           SKIPPED = "skipped"
│   │       
│   │       
│   │       class SchedulerCommands:
│   │           """Scheduler command types"""
│   │           SCHEDULE_CREATE = "cmd.scheduler.schedule.create"
│   │           SCHEDULE_UPDATE = "cmd.scheduler.schedule.update"
│   │           SCHEDULE_DELETE = "cmd.scheduler.schedule.delete"
│   │           SCHEDULE_PAUSE = "cmd.scheduler.schedule.pause"
│   │           SCHEDULE_RESUME = "cmd.scheduler.schedule.resume"
│   │           SCHEDULE_TRIGGER = "cmd.scheduler.schedule.trigger"
│   │           EXECUTE_IMMEDIATE = "cmd.scheduler.execute.immediate"
│   │       
│   │       
│   │       class SchedulerEvents:
│   │           """Scheduler event types"""
│   │           SCHEDULE_CREATED = "evt.scheduler.schedule.created"
│   │           SCHEDULE_UPDATED = "evt.scheduler.schedule.updated"
│   │           SCHEDULE_DELETED = "evt.scheduler.schedule.deleted"
│   │           SCHEDULE_PAUSED = "evt.scheduler.schedule.paused"
│   │           SCHEDULE_RESUMED = "evt.scheduler.schedule.resumed"
│   │           SCHEDULE_TRIGGERED = "evt.scheduler.schedule.triggered"
│   │           EXECUTION_STARTED = "evt.scheduler.execution.started"
│   │           EXECUTION_COMPLETED = "evt.scheduler.execution.completed"
│   │           EXECUTION_FAILED = "evt.scheduler.execution.failed"
│   │       
│   │       
│   │       # Command Payloads
│   │       class CreateScheduleCommandPayload(BaseModel):
│   │           """Payload for creating a schedule"""
│   │           name: str
│   │           description: Optional[str] = None
│   │           
│   │           # Scheduling configuration
│   │           schedule_type: ScheduleType
│   │           cron_expression: Optional[str] = None
│   │           interval_seconds: Optional[int] = None
│   │           scheduled_at: Optional[datetime] = None
│   │           timezone: str = "UTC"
│   │           
│   │           # Execution configuration
│   │           target_command: str
│   │           command_payload: Dict[str, Any] = Field(default_factory=dict)
│   │           
│   │           # Metadata
│   │           tags: List[str] = Field(default_factory=list)
│   │           priority: int = Field(default=5, ge=1, le=10)
│   │           max_retries: int = Field(default=3, ge=0)
│   │           retry_delay_seconds: int = Field(default=300, ge=0)
│   │           
│   │           # Optional fields
│   │           created_by: Optional[str] = None
│   │       
│   │       
│   │       class UpdateScheduleCommandPayload(BaseModel):
│   │           """Payload for updating a schedule"""
│   │           schedule_id: UUID
│   │           name: Optional[str] = None
│   │           description: Optional[str] = None
│   │           
│   │           # Scheduling configuration
│   │           schedule_type: Optional[ScheduleType] = None
│   │           cron_expression: Optional[str] = None
│   │           interval_seconds: Optional[int] = None
│   │           scheduled_at: Optional[datetime] = None
│   │           timezone: Optional[str] = None
│   │           
│   │           # Execution configuration
│   │           target_command: Optional[str] = None
│   │           command_payload: Optional[Dict[str, Any]] = None
│   │           
│   │           # Metadata
│   │           tags: Optional[List[str]] = None
│   │           priority: Optional[int] = Field(None, ge=1, le=10)
│   │           max_retries: Optional[int] = Field(None, ge=0)
│   │           retry_delay_seconds: Optional[int] = Field(None, ge=0)
│   │           
│   │           # State
│   │           is_active: Optional[bool] = None
│   │       
│   │       
│   │       class DeleteScheduleCommandPayload(BaseModel):
│   │           """Payload for deleting a schedule"""
│   │           schedule_id: UUID
│   │           hard_delete: bool = False  # If true, delete permanently; if false, soft delete
│   │       
│   │       
│   │       class PauseScheduleCommandPayload(BaseModel):
│   │           """Payload for pausing a schedule"""
│   │           schedule_id: UUID
│   │           reason: Optional[str] = None
│   │       
│   │       
│   │       class ResumeScheduleCommandPayload(BaseModel):
│   │           """Payload for resuming a schedule"""
│   │           schedule_id: UUID
│   │       
│   │       
│   │       class TriggerScheduleCommandPayload(BaseModel):
│   │           """Payload for triggering a schedule immediately"""
│   │           schedule_id: UUID
│   │           override_payload: Optional[Dict[str, Any]] = None
│   │       
│   │       
│   │       class ExecuteImmediateCommandPayload(BaseModel):
│   │           """Payload for immediate execution without scheduling"""
│   │           target_command: str
│   │           command_payload: Dict[str, Any] = Field(default_factory=dict)
│   │           priority: int = Field(default=5, ge=1, le=10)
│   │       
│   │       
│   │       # Event Payloads
│   │       class ScheduleCreatedEventPayload(BaseModel):
│   │           """Payload for SCHEDULE_CREATED event"""
│   │           schedule_id: UUID
│   │           name: str
│   │           schedule_type: ScheduleType
│   │           target_command: str
│   │           next_run_at: Optional[datetime]
│   │           created_by: str
│   │           created_at: datetime
│   │       
│   │       
│   │       class ScheduleUpdatedEventPayload(BaseModel):
│   │           """Payload for SCHEDULE_UPDATED event"""
│   │           schedule_id: UUID
│   │           updated_fields: List[str]
│   │           next_run_at: Optional[datetime]
│   │           updated_by: str
│   │           updated_at: datetime
│   │       
│   │       
│   │       class ScheduleDeletedEventPayload(BaseModel):
│   │           """Payload for SCHEDULE_DELETED event"""
│   │           schedule_id: UUID
│   │           deleted_by: str
│   │           deleted_at: datetime
│   │           hard_delete: bool
│   │       
│   │       
│   │       class SchedulePausedEventPayload(BaseModel):
│   │           """Payload for SCHEDULE_PAUSED event"""
│   │           schedule_id: UUID
│   │           reason: Optional[str]
│   │           paused_by: str
│   │           paused_at: datetime
│   │       
│   │       
│   │       class ScheduleResumedEventPayload(BaseModel):
│   │           """Payload for SCHEDULE_RESUMED event"""
│   │           schedule_id: UUID
│   │           next_run_at: Optional[datetime]
│   │           resumed_by: str
│   │           resumed_at: datetime
│   │       
│   │       
│   │       class ScheduleTriggeredEventPayload(BaseModel):
│   │           """Payload for SCHEDULE_TRIGGERED event"""
│   │           schedule_id: UUID
│   │           execution_id: UUID
│   │           triggered_by: str
│   │           triggered_at: datetime
│   │       
│   │       
│   │       class ExecutionStartedEventPayload(BaseModel):
│   │           """Payload for EXECUTION_STARTED event"""
│   │           execution_id: UUID
│   │           schedule_id: UUID
│   │           schedule_name: str
│   │           target_command: str
│   │           scheduled_for: datetime
│   │           started_at: datetime
│   │           attempt_number: int = 1
│   │       
│   │       
│   │       class ExecutionCompletedEventPayload(BaseModel):
│   │           """Payload for EXECUTION_COMPLETED event"""
│   │           execution_id: UUID
│   │           schedule_id: UUID
│   │           status: ExecutionStatus
│   │           duration_ms: int
│   │           completed_at: datetime
│   │           next_run_at: Optional[datetime]
│   │       
│   │       
│   │       class ExecutionFailedEventPayload(BaseModel):
│   │           """Payload for EXECUTION_FAILED event"""
│   │           execution_id: UUID
│   │           schedule_id: UUID
│   │           error_message: str
│   │           error_type: Optional[str]
│   │           will_retry: bool
│   │           retry_at: Optional[datetime]
│   │           attempt_number: int
│   │           failed_at: datetime
│   │       ```
│   │       
│   ├── webhook/
│   ├── __init__.py
│   │   
│   │   ```py
│   │   # shared/events/__init__.py
│   │   """
│   │   Shared event handling module for glam-app microservices.
│   │   
│   │   This module provides:
│   │   - Event type definitions and stream mapping
│   │   - Base publisher/subscriber classes
│   │   - Event context management
│   │   - Stream configuration
│   │   """
│   │   
│   │   from .base import (
│   │       Streams,
│   │       EventWrapper,
│   │       # EventDefinition removed
│   │   )
│   │   
│   │   from .context import (
│   │       EventContext,
│   │       EventPayload,
│   │       EventContextManager,
│   │   )
│   │   
│   │   from .base_publisher import DomainEventPublisher
│   │   from .base_subscriber import DomainEventSubscriber
│   │   
│   │   from .mappers import (
│   │       # EVENT_REGISTRY removed
│   │       SERVICE_STREAM_MAP,
│   │       get_stream_subjects,
│   │       get_stream_for_service,
│   │       get_stream_from_subject,
│   │   )
│   │   
│   │   __all__ = [
│   │       # Base types
│   │       "Streams",
│   │       "EventWrapper",
│   │       # Context management
│   │       "EventContext",
│   │       "EventPayload",
│   │       "EventContextManager",
│   │       # Publishers/Subscribers
│   │       "DomainEventPublisher",
│   │       "DomainEventSubscriber",
│   │       # Mappers
│   │       "SERVICE_STREAM_MAP",
│   │       "get_stream_subjects",
│   │       "get_stream_for_service",
│   │       "get_stream_from_subject",
│   │   ]
│   │   ```
│   │   
│   ├── base.py
│   │   
│   │   ```py
│   │   # shared/events/base.py
│   │   from pydantic import BaseModel, Field
│   │   from typing import Dict, Optional, List, Any, TypeVar, Generic
│   │   from datetime import datetime
│   │   from uuid import UUID
│   │   from enum import Enum
│   │   
│   │   from .context import EventContext
│   │   
│   │   # Generic type for event data
│   │   TData = TypeVar("TData", bound=BaseModel)
│   │   
│   │   
│   │   class Streams(str, Enum):
│   │       """JetStream streams organized by domain"""
│   │   
│   │       # Core Business Domains
│   │       CATALOG = "CATALOG"  # Catalog management
│   │       MERCHANT = "MERCHANT"  # Merchant management
│   │       BILLING = "BILLING"  # Billing management
│   │       CREDIT = "CREDIT"  # Credit management
│   │   
│   │       PROFILE = "PROFILE"  # Merchant users profiles
│   │   
│   │       # Platform Services
│   │       NOTIFICATION = "NOTIFICATION"  # Notification delivery
│   │       ANALYTICS = "ANALYTICS"  # Analytics and reporting
│   │       WEBHOOKS = "WEBHOOKS"  # Webhook delivery
│   │       SCHEDULER = "SCHEDULER"  # Scheduled jobs
│   │   
│   │       # AI Services
│   │       AI_PROCESSING = "AI_PROCESSING"  # AI/ML processing tasks
│   │   
│   │   
│   │   class EventWrapper(BaseModel, Generic[TData]):
│   │       """Base wrapper for all events with context support"""
│   │   
│   │       subject: str
│   │       idempotency_key: Optional[str] = None
│   │   
│   │       # Context fields
│   │       event_id: Optional[str] = None
│   │       trace_id: Optional[str] = None
│   │       correlation_id: Optional[str] = None
│   │       timestamp: Optional[datetime] = None
│   │       metadata: Optional[Dict[str, Any]] = None
│   │   
│   │       # Data field is now generic
│   │       data: TData
│   │   
│   │       class Config:
│   │           json_encoders = {UUID: str, datetime: lambda v: v.isoformat()}
│   │   
│   │       @classmethod
│   │       def from_context(
│   │           cls, context: EventContext, data: TData, subject: Optional[str] = None
│   │       ) -> "EventWrapper[TData]":
│   │           """Create EventWrapper from EventContext and data"""
│   │           if not subject:
│   │               subject = context.subject
│   │   
│   │           return cls(
│   │               subject=subject,
│   │               idempotency_key=context.idempotency_key,
│   │               event_id=context.event_id,
│   │               trace_id=context.trace_id,
│   │               correlation_id=context.correlation_id,
│   │               timestamp=context.timestamp,
│   │               metadata=context.metadata,
│   │               data=data,
│   │           )
│   │   
│   │       def to_event_dict(self) -> Dict[str, Any]:
│   │           """Convert to event dictionary for publishing"""
│   │           return {
│   │               "event_id": self.event_id,
│   │               "trace_id": self.trace_id,
│   │               "subject": self.subject,
│   │               "correlation_id": self.correlation_id,
│   │               "idempotency_key": self.idempotency_key,
│   │               "timestamp": self.timestamp.isoformat() if self.timestamp else None,
│   │               "metadata": self.metadata or {},
│   │               "payload": (
│   │                   self.data.model_dump() if isinstance(self.data, BaseModel) else {}
│   │               ),
│   │           }
│   │   ```
│   │   
│   ├── base_publisher.py
│   │   
│   │   ```py
│   │   from typing import Dict, Any, Optional
│   │   
│   │   from .base import Streams
│   │   from .mappers import get_stream_from_subject, get_stream_subjects
│   │   from shared.messaging.publisher import JetStreamEventPublisher
│   │   
│   │   
│   │   class DomainEventPublisher(JetStreamEventPublisher):
│   │       """
│   │       Smart publisher that:
│   │       1. Auto-configures streams based on domain
│   │       2. Validates events belong to the correct stream
│   │       3. Provides domain-specific helpers
│   │       """
│   │   
│   │       # Concrete classes just set these two properties
│   │       domain_stream: Optional[Streams] = None
│   │       service_name_override: Optional[str] = None
│   │   
│   │       @property
│   │       def stream_name(self) -> str:
│   │           """Auto-determined from domain_stream"""
│   │           if not self.domain_stream:
│   │               raise NotImplementedError("domain_stream must be set")
│   │           return self.domain_stream.value
│   │   
│   │       @property
│   │       def subjects(self) -> list[str]:
│   │           """Auto-determined from domain_stream"""
│   │           if not self.domain_stream:
│   │               raise NotImplementedError("domain_stream must be set")
│   │           return get_stream_subjects(self.domain_stream)
│   │   
│   │       @property
│   │       def service_name(self) -> str:
│   │           """Uses override or defaults"""
│   │           return self.service_name_override or "unknown-service"
│   │   
│   │       def _validate_subject(self, subject: str):
│   │           """
│   │           Ensures you can't accidentally publish a billing event
│   │           from the catalog service
│   │           """
│   │           try:
│   │               # Try to infer stream from event type
│   │               inferred_stream = get_stream_from_subject(subject)
│   │               if inferred_stream != self.domain_stream:
│   │                   raise ValueError(
│   │                       f"Event {subject} belongs to {inferred_stream}, "
│   │                       f"not {self.domain_stream}"
│   │                   )
│   │           except ValueError:
│   │               # For events not following standard pattern, check prefix matches subjects
│   │               if not any(
│   │                   subject.startswith(subj.replace("*", "")) for subj in self.subjects
│   │               ):
│   │                   raise ValueError(
│   │                       f"Event {subject} doesn't match any subject pattern "
│   │                       f"for stream {self.domain_stream}"
│   │                   )
│   │   
│   │       async def publish_event(
│   │           self,
│   │           subject: str,
│   │           payload: Dict[str, Any],
│   │           subject: Optional[str] = None,
│   │           correlation_id: Optional[str] = None,
│   │           idempotency_key: Optional[str] = None,
│   │           metadata: Optional[Dict[str, Any]] = None,
│   │       ) -> str:
│   │           """Override with same signature, adds validation"""
│   │           self._validate_subject(subject)
│   │   
│   │           # Call parent with all parameters
│   │           return await super().publish_event(
│   │               subject=subject,
│   │               payload=payload,
│   │               subject=subject,
│   │               correlation_id=correlation_id,
│   │               idempotency_key=idempotency_key,
│   │               metadata=metadata,
│   │           )
│   │   
│   │       async def publish_command(
│   │           self, command_type: str, payload: Dict[str, Any], **kwargs
│   │       ) -> str:
│   │           """Override to add validation"""
│   │           # Ensure command format
│   │           if not command_type.startswith("cmd."):
│   │               command_type = f"cmd.{command_type}"
│   │   
│   │           self._validate_subject(command_type)
│   │   
│   │           return await super().publish_command(command_type, payload, **kwargs)
│   │   
│   │       async def publish_event_response(
│   │           self, subject: str, payload: Dict[str, Any], **kwargs
│   │       ) -> str:
│   │           """Override to add validation"""
│   │           # Ensure event format
│   │           if not subject.startswith("evt."):
│   │               subject = f"evt.{subject}"
│   │   
│   │           self._validate_subject(subject)
│   │   
│   │           return await super().publish_event_response(subject, payload, **kwargs)
│   │   ```
│   │   
│   ├── base_subscriber.py
│   │   
│   │   ```py
│   │   # File: shared/shared/events/base_subscriber.py
│   │   from .mappers import get_stream_from_subject
│   │   from shared.messaging.subscriber import JetStreamEventSubscriber
│   │   
│   │   
│   │   class DomainEventSubscriber(JetStreamEventSubscriber):
│   │       """
│   │       Smart subscriber that:
│   │       1. Auto-determines stream from event type
│   │       2. Provides helpers for cross-domain subscriptions
│   │       3. Validates event structure
│   │       """
│   │   
│   │       @property
│   │       def stream_name(self) -> str:
│   │           """Auto-determine stream from event type"""
│   │           try:
│   │               stream = get_stream_from_subject(self.subject)
│   │               return stream.value
│   │           except ValueError:
│   │               # For events not following standard pattern, must override
│   │               return self._stream_name_override()
│   │   
│   │       def _stream_name_override(self) -> str:
│   │           """Override when subscribing to events not following standard pattern"""
│   │           raise NotImplementedError(
│   │               f"Event {self.subject} doesn't follow standard pattern, "
│   │               f"must override _stream_name_override"
│   │           )
│   │   ```
│   │   
│   ├── context.py
│   │   
│   │   ```py
│   │   # shared/events/context.py
│   │   """
│   │   Standardized event context management for all services.
│   │   """
│   │   from uuid7 import uuid7
│   │   from uuid import uuid4
│   │   from typing import Dict, Any, Optional, TypeVar, Generic
│   │   from dataclasses import dataclass
│   │   from datetime import datetime, timezone
│   │   
│   │   from ..api.correlation import set_correlation_context, get_correlation_context
│   │   from shared.api.tracing import set_trace_context, get_trace_context
│   │   
│   │   T = TypeVar("T")
│   │   
│   │   @dataclass
│   │   class EventContext:
│   │       """Standard context for all events across the platform"""
│   │   
│   │       event_id: str
│   │       trace_id: str
│   │       subject: str
│   │       correlation_id: Optional[str]
│   │       timestamp: datetime
│   │       source_service: str
│   │       idempotency_key: Optional[str] = None
│   │       metadata: Optional[Dict[str, Any]] = None
│   │   
│   │       def __post_init__(self):
│   │           if self.metadata is None:
│   │               self.metadata = {}
│   │   
│   │       @classmethod
│   │       def from_event(cls, event: Dict[str, Any]) -> "EventContext":
│   │           """Extract context from incoming event"""
│   │           return cls(
│   │               event_id=event.get("event_id", ""),
│   │               trace_id=event.get("trace_id", ""),
│   │               subject=event.get("subject", ""),
│   │               correlation_id=event.get("correlation_id"),
│   │               timestamp=datetime.fromisoformat(
│   │                   event.get("timestamp", datetime.now(timezone.utc).isoformat())
│   │               ),
│   │               source_service=event.get("metadata", {}).get("source_service", "unknown"),
│   │               idempotency_key=event.get("idempotency_key"),
│   │               metadata=event.get("metadata", {}),
│   │           )
│   │   
│   │       @classmethod
│   │       def create(
│   │           cls,
│   │           *,
│   │           subject: str,
│   │           source_service: str,
│   │           correlation_id: Optional[str] = None,
│   │           idempotency_key: Optional[str] = None,
│   │           metadata: Optional[Dict[str, Any]] = None,
│   │           trace_id: Optional[str] = None,
│   │       ) -> "EventContext":
│   │           """Create new event context"""
│   │           if not correlation_id:
│   │               correlation_id = get_correlation_context()
│   │           if not trace_id:
│   │               trace_id = get_trace_context() or correlation_id or str(uuid7())
│   │           return cls(
│   │               event_id=str(uuid4()),
│   │               trace_id=trace_id,
│   │               subject=subject,
│   │               correlation_id=correlation_id,
│   │               timestamp=datetime.now(timezone.utc),
│   │               source_service=source_service,
│   │               idempotency_key=idempotency_key,
│   │               metadata=metadata or {},
│   │           )
│   │   
│   │       def to_dict(self) -> Dict[str, Any]:
│   │           """Convert context to dictionary for logging or serialization"""
│   │           return {
│   │               "event_id": self.event_id,
│   │               "trace_id": self.trace_id,
│   │               "subject": self.subject,
│   │               "correlation_id": self.correlation_id,
│   │               "timestamp": self.timestamp.isoformat(),
│   │               "source_service": self.source_service,
│   │               "idempotency_key": self.idempotency_key,
│   │               "metadata": self.metadata,  # keep nested
│   │           }
│   │   
│   │       def apply_correlation(self):
│   │           """Apply correlation and trace context to async context"""
│   │           if self.correlation_id:
│   │               set_correlation_context(self.correlation_id)
│   │           if self.trace_id:
│   │               set_trace_context(self.trace_id)
│   │   
│   │       def create_response_context(
│   │           self,
│   │           response_subject: str,
│   │           response_service: str,
│   │           idempotency_key: Optional[str] = None,
│   │           additional_metadata: Optional[Dict[str, Any]] = None,
│   │       ) -> "EventContext":
│   │           """Create context for response events, preserving correlation chain"""
│   │           metadata = {
│   │               "triggered_by": self.event_id,
│   │               "original_source": self.source_service,
│   │               **(additional_metadata or {}),
│   │           }
│   │   
│   │           return EventContext.create(
│   │               subject=response_subject,
│   │               source_service=response_service,
│   │               correlation_id=self.correlation_id,
│   │               trace_id=self.trace_id,
│   │               idempotency_key=idempotency_key,
│   │               metadata=metadata,
│   │           )
│   │   
│   │   
│   │   @dataclass
│   │   class EventPayload(Generic[T]):
│   │       """Wrapper for typed event payloads with context"""
│   │   
│   │       context: EventContext
│   │       data: T
│   │   
│   │       @property
│   │       def correlation_id(self) -> Optional[str]:
│   │           return self.context.correlation_id
│   │   
│   │       def to_event_dict(self) -> Dict[str, Any]:
│   │           """Convert to event dictionary for publishing"""
│   │           return {
│   │               "event_id": self.context.event_id,
│   │               "subject": self.context.subject,
│   │               "correlation_id": self.context.correlation_id,
│   │               "idempotency_key": self.context.idempotency_key,
│   │               "timestamp": self.context.timestamp.isoformat(),
│   │               "metadata": {
│   │                   "source_service": self.context.source_service,
│   │                   **(self.context.metadata or {}),
│   │               },
│   │               "payload": self.data if isinstance(self.data, dict) else {},
│   │           }
│   │   
│   │   
│   │   class EventContextManager:
│   │       """
│   │       Manages event context throughout processing lifecycle.
│   │       This can be used by any service that processes events.
│   │       """
│   │   
│   │       def __init__(self, logger=None):
│   │           self.logger = logger
│   │   
│   │       def extract_context(self, event: Dict[str, Any]) -> EventContext:
│   │           """Extract and validate event context"""
│   │           context = EventContext.from_event(event)
│   │   
│   │           # Validate required fields
│   │           if not context.event_id and self.logger:
│   │               self.logger.warning("Event missing event_id", extra={"event": event})
│   │   
│   │           if not context.subject:
│   │               raise ValueError("Event missing required subject")
│   │   
│   │           # Apply correlation context
│   │           context.apply_correlation()
│   │   
│   │           # Log event reception
│   │           if self.logger:
│   │               self.logger.info(
│   │                   f"Processing {context.subject} event", extra=context.to_dict()
│   │               )
│   │   
│   │           return context
│   │   
│   │       def log_event_completion(
│   │           self,
│   │           context: EventContext,
│   │           success: bool,
│   │           duration_ms: float,
│   │           error: Optional[Exception] = None,
│   │           additional_data: Optional[Dict[str, Any]] = None,
│   │       ):
│   │           """Log event processing completion"""
│   │           if not self.logger:
│   │               return
│   │   
│   │           log_data = {
│   │               **context.to_dict(),
│   │               "success": success,
│   │               "duration_ms": duration_ms,
│   │               **(additional_data or {}),
│   │           }
│   │   
│   │           if success:
│   │               self.logger.info(f"Completed processing {context.subject}", extra=log_data)
│   │           else:
│   │               self.logger.error(
│   │                   f"Failed to process {context.subject}: {error}",
│   │                   extra={
│   │                       **log_data,
│   │                       "error_type": type(error).__name__ if error else "Unknown",
│   │                   },
│   │                   exc_info=error,
│   │               )
│   │   ```
│   │   
│   └── mappers.py
│       
│       ```py
│       # shared/events/mappers.py
│       from typing import List
│       from .base import Streams
│       
│       # Simple service to stream mapping
│       SERVICE_STREAM_MAP = {
│           "analytics-service": Streams.ANALYTICS,
│           "billing-service": Streams.BILLING,
│           "catalog-ai-service": Streams.AI_PROCESSING,
│           "catalog-connector": Streams.CATALOG,
│           "catalog-service": Streams.CATALOG,
│           "credit-service": Streams.CREDIT,
│           "merchant-service": Streams.MERCHANT,
│           "notification-service": Streams.NOTIFICATION,
│           "profile-ai-selfie": Streams.AI_PROCESSING,
│           "profile-service": Streams.PROFILE,
│           "scheduler-service": Streams.SCHEDULER,
│           "webhook-service": Streams.WEBHOOKS,
│       }
│       
│       
│       def get_stream_subjects(stream: Streams) -> List[str]:
│           """Get all subjects for a stream"""
│           stream_prefix = stream.value.lower()
│           return [f"cmd.{stream_prefix}.*", f"evt.{stream_prefix}.*"]
│       
│       
│       def get_stream_for_service(service_name: str) -> Streams:
│           """Get the stream for a service"""
│           if service_name not in SERVICE_STREAM_MAP:
│               raise ValueError(f"Unknown service: {service_name}")
│           return SERVICE_STREAM_MAP[service_name]
│       
│       
│       def get_stream_from_subject(subject: str) -> Streams:
│           """
│           Infer stream from event type.
│           Event types follow pattern: cmd.domain.* or evt.domain.*
│           """
│           parts = subject.split(".")
│           if len(parts) >= 2 and parts[0] in ["cmd", "evt"]:
│               domain = parts[1].upper()
│               if hasattr(Streams, domain):
│                   return Streams[domain]
│       
│           raise ValueError(f"Cannot infer stream from event type: {subject}")
│       ```
│       
├── mappers/
│   ├── __init__.py
│   │   
│   │   ```py
│   │   # shared/mappers/__init__.py
│   │   
│   │   """Shared mapper infrastructure for all services."""
│   │   
│   │   from .base import (
│   │       BaseMapper,
│   │   )
│   │   
│   │   __all__ = [
│   │       "BaseMapper",
│   │   ]
│   │   ```
│   │   
│   ├── base.py
│   │   
│   │   ```py
│   │   # File: shared/shared/mappers/base.py
│   │   from __future__ import annotations
│   │   
│   │   from typing import TypeVar, Generic, List, Dict, Any, Optional
│   │   from abc import ABC, abstractmethod
│   │   from pydantic import BaseModel
│   │   
│   │   # Type variables
│   │   ModelT = TypeVar("ModelT")
│   │   CreateT = TypeVar("CreateT", bound=BaseModel)
│   │   UpdateT = TypeVar("UpdateT", bound=BaseModel)
│   │   ResponseT = TypeVar("ResponseT", bound=BaseModel)
│   │   
│   │   class BaseMapper(Generic[ModelT, CreateT, UpdateT, ResponseT], ABC):
│   │       """
│   │       Universal mapper for all entities with optional update support.
│   │       
│   │       Required methods (always implement):
│   │       - create_to_model: Convert create schema to model
│   │       - model_to_response: Convert model to basic response
│   │       
│   │       Optional methods (implement only if your entity needs them):
│   │       - update_to_dict: For updatable entities  
│   │       - model_to_detail_response: For entities with detailed views
│   │       """
│   │       __slots__ = ()  # Memory optimization
│   │       
│   │       @abstractmethod
│   │       def create_to_model(self, create_schema: CreateT, **kwargs) -> ModelT:
│   │           """Convert create schema to model instance."""
│   │           pass
│   │       
│   │       @abstractmethod
│   │       def model_to_response(self, model: ModelT) -> ResponseT:
│   │           """Convert model instance to basic response schema."""
│   │           pass
│   │       
│   │       @abstractmethod
│   │       def models_to_responses(self, models: List[ModelT]) -> List[ResponseT]:
│   │           """Convert list of models to list of response schemas."""
│   │           return [self.model_to_response(model) for model in models]
│   │       
│   │       @abstractmethod
│   │       def update_to_dict(self, update_schema: UpdateT) -> Dict[str, Any]:
│   │           """
│   │           Optional: Convert update schema to dict for partial updates.
│   │           Only implement this method if your entity supports updates.
│   │           """
│   │           return {
│   │               k: v for k, v in update_schema.model_dump(exclude_none=True).items()
│   │           }
│   │   ```
│   │   
│   └── crud_mapper.py
│       
│       ```py
│       # mappers/generic.py
│       from __future__ import annotations
│       from typing import TypeVar, Generic, List
│       from pydantic import BaseModel, TypeAdapter
│       from sqlalchemy.orm import DeclarativeBase
│       from sqlalchemy import inspect
│       
│       ModelT = TypeVar('ModelT', bound=DeclarativeBase)
│       InT    = TypeVar('InT',    bound=BaseModel)
│       PatchT = TypeVar('PatchT', bound=BaseModel | None)
│       OutT   = TypeVar('OutT',   bound=BaseModel)
│       
│       class CRUDMapper(Generic[ModelT, InT, PatchT, OutT]):
│           """Bidirectional bridge between SQLAlchemy models and Pydantic DTOs."""
│       
│           model_cls: type[ModelT]
│           out_schema: type[OutT]
│       
│           # ---------- CREATE ---------- #
│           def to_model(self, dto: InT, **extra) -> ModelT:
│               return self.model_cls(**dto.model_dump(), **extra)
│       
│           # ---------- PATCH ---------- #
│           def patch_model(self, obj: ModelT, patch: PatchT) -> None:
│               if patch is None:
│                   return
│               for field, value in patch.model_dump(exclude_unset=True).items():
│                   setattr(obj, field, value)
│               
│       
│           # ---------- READ ---------- #
│           def to_out(self, obj: ModelT) -> OutT:
│               # from_attributes=True must be set on OutT
│               return self.out_schema.model_validate(obj)
│       
│           def list_to_out(self, objs: List[ModelT]) -> List[OutT]:
│               # faster validation for big lists
│               ta = TypeAdapter(List[self.out_schema])  # type: ignore[arg-type]
│               return ta.validate_python(objs)
│       
│           # ---------- HELPER ---------- #
│           @staticmethod
│           def is_dirty(obj: ModelT) -> bool:
│               return bool(inspect(obj).attrs.modified)
│       ```
│       
├── messaging/
│   ├── __init__.py
│   ├── dependencies.py
│   │   
│   │   ```py
│   │   # shared/messaging/dependencies.py
│   │   from typing import Any, Dict, Literal, get_args
│   │   
│   │   # Define all valid dependency keys across the platform
│   │   DepKeys = Literal[
│   │       "credit_service",
│   │       "credit_transaction_service", 
│   │       "notification_service",
│   │       "logger",
│   │       # Test-only keys
│   │       "test_service",
│   │   ]
│   │   
│   │   class ServiceDependencies:
│   │       """Service-scoped dependency container with typed key constraints"""
│   │       
│   │       def __init__(self):
│   │           self._deps: Dict[str, Any] = {}
│   │       
│   │       def register(self, key: DepKeys, instance: Any) -> None:
│   │           """Register a dependency - prevents accidental shadowing"""
│   │           if key in self._deps:
│   │               raise RuntimeError(f"Dependency '{key}' already registered")
│   │           self._deps[key] = instance
│   │       
│   │       def get(self, key: DepKeys) -> Any:
│   │           """Get a dependency with type-constrained keys"""
│   │           if key not in self._deps:
│   │               available = list(self._deps.keys())
│   │               raise KeyError(
│   │                   f"Dependency '{key}' not registered. Available: {available}"
│   │               )
│   │           return self._deps[key]
│   │       
│   │       def has(self, key: DepKeys) -> bool:
│   │           """Check if dependency is registered"""
│   │           return key in self._deps
│   │       
│   │       def clear(self) -> None:
│   │           """Clear all dependencies (for testing)"""
│   │           self._deps.clear()
│   │           
│   │       def register_many(self, **kwargs) -> None:
│   │           """Bulk register dependencies for cleaner lifecycle code"""
│   │           for key, instance in kwargs.items():
│   │               if key not in get_args(DepKeys):
│   │                   raise ValueError(f"Invalid dependency key: {key}")
│   │               self.register(key, instance)
│   │   ```
│   │   
│   ├── jetstream_wrapper.py
│   │   
│   │   ```py
│   │   # shared/messaging/jetstream_wrapper.py
│   │   from typing import List, Optional, Dict, Type, Any, TypeVar, cast
│   │   import os
│   │   import asyncio
│   │   
│   │   import nats
│   │   from nats.aio.client import Client
│   │   from nats.js import JetStreamContext
│   │   
│   │   from shared.messaging.publisher import JetStreamEventPublisher
│   │   from shared.messaging.subscriber import JetStreamEventSubscriber
│   │   from .dependencies import ServiceDependencies, DepKeys
│   │   
│   │   T = TypeVar('T', bound=JetStreamEventPublisher)
│   │   
│   │   class JetStreamWrapper:
│   │       """JetStream wrapper with service-scoped dependency injection"""
│   │       
│   │       def __init__(self, logger: Optional[Any] = None):
│   │           self._client: Optional[Client] = None
│   │           self._js: Optional[JetStreamContext] = None
│   │           self._publishers: Dict[str, JetStreamEventPublisher] = {}
│   │           self._subscribers: List[JetStreamEventSubscriber] = []
│   │           self._tasks: List[asyncio.Task] = []
│   │           self.logger = logger
│   │           self.dependencies = ServiceDependencies()
│   │       
│   │       async def __aenter__(self):
│   │           """Async context manager entry"""
│   │           return self
│   │       
│   │       async def __aexit__(self, exc_type, exc_val, exc_tb):
│   │           """Guaranteed cleanup even if caller forgets close()"""
│   │           await self.close()
│   │       
│   │       @property
│   │       def client(self) -> Client:
│   │           """Get the NATS client (for backward compatibility)"""
│   │           if not self._client:
│   │               raise Exception("NATS client not connected")
│   │           return self._client
│   │       
│   │       @property
│   │       def js(self) -> JetStreamContext:
│   │           """Get the JetStream context"""
│   │           if not self._js:
│   │               raise Exception("JetStream not initialized")
│   │           return self._js
│   │       
│   │       @property
│   │       def publishers(self) -> Dict[str, JetStreamEventPublisher]:
│   │           """Get all registered publishers"""
│   │           return self._publishers
│   │       
│   │       async def connect(self, servers: List[str]):
│   │           """Connect to NATS and initialize JetStream"""
│   │           try:
│   │               # Connection options
│   │               options = {
│   │                   "servers": servers,
│   │                   "max_reconnect_attempts": -1,
│   │                   "reconnect_time_wait": 2,
│   │               }
│   │               
│   │               # Add authentication if provided
│   │               if user := os.getenv("NATS_USER"):
│   │                   options["user"] = user
│   │                   options["password"] = os.getenv("NATS_PASSWORD", "")
│   │               
│   │               self._client = await nats.connect(**options)
│   │               self._js = self._client.jetstream()
│   │               
│   │               if self.logger:
│   │                   self.logger.info(f"Connected to NATS with JetStream at {servers}")
│   │               
│   │           except Exception as e:
│   │               if self.logger:
│   │                   self.logger.error(f"Failed to connect to NATS: {e}")
│   │               raise
│   │       
│   │       async def close(self):
│   │           """Enhanced cleanup with task cancellation"""
│   │           # Cancel all subscriber tasks
│   │           for task in self._tasks:
│   │               task.cancel()
│   │           
│   │           # Wait for graceful shutdown
│   │           if self._tasks:
│   │               await asyncio.gather(*self._tasks, return_exceptions=True)
│   │           
│   │           # Stop subscribers
│   │           for subscriber in self._subscribers:
│   │               try:
│   │                   await subscriber.stop()
│   │               except Exception as e:
│   │                   if self.logger:
│   │                       self.logger.error(f"Error stopping subscriber: {e}")
│   │           
│   │           # Close NATS connection
│   │           if self._client and not self._client.is_closed:
│   │               await self._client.close()
│   │               if self.logger:
│   │                   self.logger.info("NATS connection closed")
│   │       
│   │       def is_connected(self) -> bool:
│   │           """Health check helper for /healthz endpoints"""
│   │           return bool(self._client and self._client.is_connected)
│   │       
│   │       def register_dependency(self, key: DepKeys, instance: Any) -> None:
│   │           """
│   │           Register a dependency for this wrapper's subscribers.
│   │           Typical keys: see shared/messaging/dependencies.DepKeys.
│   │           """
│   │           self.dependencies.register(key, instance)
│   │           if self.logger:
│   │               self.logger.debug(f"Registered dependency: {key}")
│   │       
│   │       def create_publisher(self, publisher_class: Type[T]) -> T:
│   │           """Create and cache a publisher instance"""
│   │           class_name = publisher_class.__name__
│   │           
│   │           if class_name not in self._publishers:
│   │               if not self._client or not self._js:
│   │                   raise Exception("Must connect to NATS before creating publishers")
│   │               
│   │               self._publishers[class_name] = publisher_class(self._client, self._js, self.logger)
│   │               if self.logger:
│   │                   self.logger.info(f"Created publisher: {class_name}")
│   │           
│   │           return cast(T, self._publishers[class_name])
│   │       
│   │       def get_publisher(self, publisher_class: Type[T]) -> Optional[T]:
│   │           """Get a publisher by class type"""
│   │           publisher = self._publishers.get(publisher_class.__name__)
│   │           return cast(T, publisher) if publisher else None
│   │       
│   │       def create_subscriber(self, subscriber_class: Type[JetStreamEventSubscriber]) -> JetStreamEventSubscriber:
│   │           """Create subscriber with wrapper access for dependencies"""
│   │           if not self._client or not self._js:
│   │               raise Exception("Must connect to NATS before creating subscribers")
│   │           
│   │           # Pass wrapper reference so subscriber can access dependencies
│   │           subscriber = subscriber_class(self._client, self._js, self.logger, self)
│   │           self._subscribers.append(subscriber)
│   │           if self.logger:
│   │               self.logger.info(f"Created subscriber: {subscriber_class.__name__}")
│   │           
│   │           return subscriber
│   │       
│   │       async def start_subscriber(self, subscriber_class: Type[JetStreamEventSubscriber]):
│   │           """Create and start a subscriber in the background"""
│   │           subscriber = self.create_subscriber(subscriber_class)
│   │           task = asyncio.create_task(subscriber.listen())
│   │           self._tasks.append(task)  # Track for proper cleanup
│   │           
│   │           if self.logger:
│   │               self.logger.info(f"Started subscriber: {subscriber_class.__name__}")
│   │           
│   │           return task
│   │   ```
│   │   
│   ├── publisher.py
│   │   
│   │   ```py
│   │   # File: shared/shared/messaging/publisher.py
│   │   import json
│   │   import uuid
│   │   from abc import ABC, abstractmethod
│   │   from typing import Dict, Any, Optional
│   │   from datetime import datetime, timezone
│   │   
│   │   from nats.aio.client import Client
│   │   from nats.js import JetStreamContext
│   │   from nats.js.api import StreamConfig, RetentionPolicy, StorageType
│   │   
│   │   from shared.api.tracing import get_trace_context
│   │   
│   │   
│   │   class JetStreamEventPublisher(ABC):
│   │       """
│   │       JetStream publisher specifically for structured events.
│   │       Combines base functionality with event structure in one class.
│   │       """
│   │   
│   │       @property
│   │       @abstractmethod
│   │       def stream_name(self) -> str:
│   │           """The JetStream stream name to publish to."""
│   │           pass
│   │   
│   │       @property
│   │       @abstractmethod
│   │       def subjects(self) -> list[str]:
│   │           """List of subjects this stream should handle."""
│   │           pass
│   │   
│   │       @property
│   │       @abstractmethod
│   │       def service_name(self) -> str:
│   │           """The name of the service publishing events."""
│   │           pass
│   │   
│   │       @property
│   │       def service_version(self) -> str:
│   │           """The version of the service."""
│   │           return "1.0.0"
│   │   
│   │       def __init__(
│   │           self, client: Client, js: JetStreamContext, logger: Optional[Any] = None
│   │       ):
│   │           self.client = client
│   │           self.js = js
│   │           self._stream_created = False
│   │           self.logger = logger or self._get_default_logger()
│   │   
│   │       def _get_default_logger(self):
│   │           """Get a default logger if none provided"""
│   │           import logging
│   │   
│   │           return logging.getLogger(self.__class__.__name__)
│   │   
│   │       async def ensure_stream(self) -> None:
│   │           """Ensure the stream exists with default configuration."""
│   │           if self._stream_created:
│   │               return
│   │   
│   │           stream_config = StreamConfig(
│   │               name=self.stream_name,
│   │               subjects=self.subjects,
│   │               retention=RetentionPolicy.LIMITS,
│   │               max_age=7 * 24 * 60 * 60,  # 7 days
│   │               max_msgs_per_subject=100000,
│   │               storage=StorageType.FILE,
│   │               duplicate_window=60,
│   │               allow_rollup_hdrs=True,
│   │           )
│   │   
│   │           try:
│   │               await self.js.stream_info(self.stream_name)
│   │               if self.logger:
│   │                   self.logger.info(f"Using existing stream: {self.stream_name}")
│   │           except:
│   │               await self.js.add_stream(stream_config)
│   │               if self.logger:
│   │                   self.logger.info(f"Created new stream: {self.stream_name}")
│   │   
│   │           self._stream_created = True
│   │   
│   │       async def publish_event(
│   │           self,
│   │           subject: str,
│   │           payload: Dict[str, Any],
│   │           subject: Optional[str] = None,
│   │           correlation_id: Optional[str] = None,
│   │           trace_id: Optional[str] = None,
│   │           idempotency_key: Optional[str] = None,
│   │           metadata: Optional[Dict[str, Any]] = None,
│   │       ) -> str:
│   │           """Publish a structured event."""
│   │           await self.ensure_stream()
│   │   
│   │           # Generate IDs
│   │           event_id = str(uuid.uuid4())
│   │           if not correlation_id:
│   │               correlation_id = str(uuid.uuid4())
│   │           if not idempotency_key:
│   │               idempotency_key = event_id
│   │   
│   │           trace_id = trace_id or (get_trace_context() or correlation_id)
│   │   
│   │           # Build event
│   │           event = {
│   │               "event_id": event_id,
│   │               "subject": subject,
│   │               "correlation_id": correlation_id,
│   │               "idempotency_key": idempotency_key,
│   │               "timestamp": datetime.now(timezone.utc).isoformat(),
│   │               "metadata": {
│   │                   "source_service": self.service_name,
│   │                   "version": self.service_version,
│   │                   **(metadata or {}),
│   │               },
│   │               "payload": payload,
│   │           }
│   │   
│   │           # Publish
│   │           subject = subject or subject
│   │           headers = {"Nats-Msg-Id": idempotency_key}
│   │   
│   │           try:
│   │               ack = await self.js.publish(
│   │                   subject, json.dumps(event).encode("utf-8"), headers=headers
│   │               )
│   │               if self.logger:
│   │                   self.logger.debug(f"Published {subject} to {subject} (seq: {ack.seq})")
│   │   
│   │               if ack.duplicate:
│   │                   self.logger.info(
│   │                       f"Duplicate message detected and ignored",
│   │                       extra={"idempotency_key": idempotency_key},
│   │                   )
│   │               return event_id
│   │   
│   │           except Exception as e:
│   │               if self.logger:
│   │                   self.logger.critical(
│   │                       f"Failed to publish to {subject}: {e}", exc_info=True
│   │                   )
│   │               raise
│   │   
│   │       async def publish_command(
│   │           self, command_type: str, payload: Dict[str, Any], **kwargs
│   │       ) -> str:
│   │           """Publish a command event."""
│   │           if not command_type.startswith("cmd."):
│   │               command_type = f"cmd.{command_type}"
│   │   
│   │           idempotency_key = kwargs.pop("idempotency_key", None)
│   │   
│   │           return await self.publish_event(
│   │               subject=command_type,
│   │               payload=payload,
│   │               idempotency_key=idempotency_key,
│   │               **kwargs,
│   │           )
│   │   
│   │       async def publish_event_response(
│   │           self, subject: str, payload: Dict[str, Any], **kwargs
│   │       ) -> str:
│   │           """Publish an event response."""
│   │           if not subject.startswith("evt."):
│   │               subject = f"evt.{subject}"
│   │   
│   │           # Ensure idempotency key is set
│   │           idempotency_key = kwargs.pop("idempotency_key", None)
│   │   
│   │           return await self.publish_event(
│   │               subject=subject, payload=payload, idempotency_key=idempotency_key, **kwargs
│   │           )
│   │   ```
│   │   
│   └── subscriber.py
│       
│       ```py
│       # shared/messaging/subscriber.py
│       import json
│       from abc import ABC, abstractmethod
│       from typing import Dict, Any, Optional, TYPE_CHECKING
│       import asyncio
│       
│       from nats.aio.client import Client
│       from nats.js import JetStreamContext
│       from nats.js.api import ConsumerConfig, DeliverPolicy, AckPolicy
│       from .dependencies import DepKeys
│       
│       if TYPE_CHECKING:
│           from .jetstream_wrapper import JetStreamWrapper
│       
│       
│       class JetStreamEventSubscriber(ABC):
│           """
│           JetStream subscriber specifically for structured events.
│           Combines base functionality with event handling in one class.
│           """
│       
│           @property
│           @abstractmethod
│           def stream_name(self) -> str:
│               """The JetStream stream name to subscribe to."""
│               pass
│       
│           @property
│           @abstractmethod
│           def subject(self) -> str:
│               """The subject pattern to subscribe to."""
│               pass
│       
│           @property
│           @abstractmethod
│           def durable_name(self) -> str:
│               """The durable consumer name."""
│               pass
│       
│           @property
│           @abstractmethod
│           def subject(self) -> str:
│               """The expected subject for validation."""
│               pass
│       
│           def __init__(
│               self,
│               client: Client,
│               js: JetStreamContext,
│               logger: Optional[Any] = None,
│               wrapper: Optional["JetStreamWrapper"] = None,
│           ):
│               """Constructor with optional wrapper for dependency access"""
│               self.client = client
│               self.js = js
│               self.logger = logger or self._get_default_logger()
│               self._subscription = None
│       
│               # Access to service dependencies via wrapper
│               self._wrapper = wrapper
│       
│               # Debug initialization
│               self.logger.debug(f"Initialized {self.__class__.__name__}")
│               self.logger.debug(f"JetStream context: {self.js is not None}")
│               self.logger.debug(
│                   f"NATS client connected: {self.client.is_connected if self.client else False}"
│               )
│       
│           def _get_default_logger(self):
│               """Get a default logger if none provided"""
│               import logging
│       
│               return logging.getLogger(self.__class__.__name__)
│       
│           def get_dependency(self, key: DepKeys) -> Any:
│               """Get a service dependency with type-constrained keys"""
│               if not self._wrapper:
│                   raise RuntimeError(
│                       f"Cannot access dependency '{key}' - wrapper not provided. "
│                       "This usually means the subscriber was created manually instead of via JetStreamWrapper."
│                   )
│               return self._wrapper.dependencies.get(key)
│       
│           def dep(self, key: DepKeys) -> Any:
│               """Shorter alias for get_dependency"""
│               return self.get_dependency(key)
│       
│           @abstractmethod
│           async def on_event(
│               self, event: Dict[str, Any], headers: Optional[Dict[str, str]] = None
│           ) -> None:
│               """Process the validated event - access dependencies via self.get_dependency()"""
│               pass
│       
│           async def on_error(self, error: Exception, event: Dict[str, Any]) -> bool:
│               """Handle processing errors with safety net for dependency access"""
│               try:
│                   self.logger.error(f"Error processing {self.subject}: {error}")
│               except Exception as log_error:
│                   # Fallback if even logging fails - avoid print() in production
│                   try:
│                       import sys
│       
│                       sys.stderr.write(
│                           f"Critical error in subscriber {self.__class__.__name__}: {error}\n"
│                       )
│                       sys.stderr.write(f"Additionally, logging failed: {log_error}\n")
│                       sys.stderr.flush()
│                   except Exception:
│                       # Last resort - this should never happen but protects against closed stderr
│                       pass
│               return False  # Default: retry
│       
│           async def listen(self) -> None:
│               """Subscribe and process messages with exponential backoff on failures"""
│       
│               # Consumer config
│               consumer_config = ConsumerConfig(
│                   durable_name=self.durable_name,
│                   deliver_policy=DeliverPolicy.ALL,
│                   ack_policy=AckPolicy.EXPLICIT,
│                   max_deliver=3,
│                   ack_wait=30,
│                   filter_subject=self.subject,
│               )
│       
│               # Create or bind consumer
│               try:
│                   await self.js.consumer_info(self.stream_name, self.durable_name)
│                   self.logger.info(f"Using existing consumer: {self.durable_name}")
│               except:
│                   await self.js.add_consumer(self.stream_name, config=consumer_config)
│                   self.logger.info(f"Created new consumer: {self.durable_name}")
│       
│               # Subscribe
│               try:
│                   self._subscription = await self.js.pull_subscribe(
│                       self.subject, durable=self.durable_name, stream=self.stream_name
│                   )
│       
│                   if self._subscription is None:
│                       raise Exception("Failed to create subscription")
│       
│                   self.logger.info(f"Listening on {self.stream_name}/{self.subject}")
│       
│                   error_count = 0
│                   max_errors = 5
│       
│                   # Process messages with exponential backoff
│                   while True:
│                       try:
│                           messages = await self._subscription.fetch(batch=10, timeout=1)
│                           error_count = 0  # Reset on success
│       
│                           for msg in messages:
│                               await self._process_message(msg)
│       
│                       except asyncio.TimeoutError:
│                           continue  # Normal - no messages
│                       except Exception as e:
│                           error_count += 1
│       
│                           if error_count > max_errors:
│                               self.logger.error("Too many errors, stopping subscriber")
│                               break
│       
│                           # Exponential backoff with jitter
│                           backoff = min(60, 2**error_count)
│                           self.logger.warning(
│                               f"Error #{error_count}, backing off {backoff}s: {e}"
│                           )
│                           await asyncio.sleep(backoff)
│       
│               except Exception as e:
│                   self.logger.error(f"Failed to create subscription: {e}")
│                   raise
│       
│           async def _process_message(self, msg) -> None:
│               """Process a single message with error handling"""
│               try:
│                   # Parse message
│                   try:
│                       data = json.loads(msg.data.decode("utf-8"))
│                   except json.JSONDecodeError as e:
│                       self.logger.error(f"Invalid JSON: {e}")
│                       await msg.ack()
│                       return
│       
│                   # Validate structure
│                   required = ["event_id", "subject", "timestamp", "payload"]
│                   if missing := [f for f in required if f not in data]:
│                       self.logger.error(f"Missing fields: {missing}")
│                       await msg.ack()
│                       return
│       
│                   # Validate event type
│                   if self.subject and data.get("subject") != self.subject:
│                       self.logger.warning(
│                           f"Wrong event type: expected {self.subject}, got {data.get('subject')}"
│                       )
│                       await msg.ack()
│                       return
│       
│                   # Extract headers
│                   headers = {}
│                   if msg.headers:
│                       headers = {
│                           k: v[0] if isinstance(v, list) else v
│                           for k, v in msg.headers.items()
│                       }
│       
│                   # Process event
│                   try:
│                       await self.on_event(data, headers)
│                       await msg.ack()
│                   except Exception as e:
│                       should_ack = await self.on_error(e, data)
│                       if should_ack:
│                           await msg.ack()
│       
│               except Exception as e:
│                   self.logger.critical(f"Fatal error processing message: {e}", exc_info=True)
│                   try:
│                       await msg.ack()  # Prevent poison messages
│                   except:
│                       pass
│       
│           async def stop(self) -> None:
│               """Stop listening"""
│               if self._subscription:
│                   try:
│                       await self._subscription.unsubscribe()
│                   except Exception as e:
│                       self.logger.error(f"Error unsubscribing: {e}")
│                   finally:
│                       self._subscription = None
│       ```
│       
├── metrics/
│   ├── __init__.py
│   │   
│   │   ```py
│   │   
│   │   # -------------------------------
│   │   # shared/metrics/__init__.py
│   │   # -------------------------------
│   │   
│   │   """Prometheus metrics utilities for microservices."""
│   │   
│   │   from .middleware import (
│   │       PrometheusMiddleware,
│   │       metrics_endpoint,
│   │       http_requests_total,
│   │       http_request_duration_seconds,
│   │       http_requests_in_progress,
│   │   )
│   │   
│   │   __all__ = [
│   │       "PrometheusMiddleware",
│   │       "metrics_endpoint",
│   │       "http_requests_total",
│   │       "http_request_duration_seconds",
│   │       "http_requests_in_progress",
│   │   ]
│   │   ```
│   │   
│   └── middleware.py
│       
│       ```py
│       # -------------------------------
│       # shared/metrics/middleware.py
│       # -------------------------------
│       
│       """
│       Prometheus metrics middleware for all services.
│       
│       Provides standard HTTP metrics and allows services to register
│       their own domain-specific metrics.
│       """
│       
│       import time
│       import re
│       from typing import Dict, Any, Optional, Callable
│       from fastapi import Request
│       from starlette.middleware.base import BaseHTTPMiddleware
│       from starlette.responses import Response
│       from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
│       
│       # Standard HTTP metrics for all services
│       http_requests_total = Counter(
│           'http_requests_total',
│           'Total HTTP requests',
│           ['service', 'method', 'endpoint', 'status']
│       )
│       
│       http_request_duration_seconds = Histogram(
│           'http_request_duration_seconds',
│           'HTTP request duration in seconds',
│           ['service', 'method', 'endpoint']
│       )
│       
│       http_requests_in_progress = Gauge(
│           'http_requests_in_progress',
│           'HTTP requests in progress',
│           ['service']
│       )
│       
│       
│       class PrometheusMiddleware(BaseHTTPMiddleware):
│           """Prometheus metrics collection middleware."""
│           
│           def __init__(self, app, service_name: str):
│               super().__init__(app)
│               self.service_name = service_name
│           
│           async def dispatch(self, request: Request, call_next):
│               # Skip metrics endpoint to avoid recursion
│               if request.url.path == "/metrics":
│                   return await call_next(request)
│               
│               # Get method and normalize path
│               method = request.method
│               path = self._normalize_path(request.url.path)
│               
│               # Track in-progress requests
│               http_requests_in_progress.labels(service=self.service_name).inc()
│               
│               # Time the request
│               start_time = time.time()
│               
│               try:
│                   response = await call_next(request)
│                   status_code = response.status_code
│                   
│                   # Record success metrics
│                   self._record_metrics(method, path, status_code, start_time)
│                   
│                   return response
│                   
│               except Exception as e:
│                   # Record failure metrics
│                   self._record_metrics(method, path, 500, start_time)
│                   raise
│               finally:
│                   http_requests_in_progress.labels(service=self.service_name).dec()
│           
│           def _record_metrics(self, method: str, path: str, status: int, start_time: float):
│               """Record HTTP metrics."""
│               http_requests_total.labels(
│                   service=self.service_name,
│                   method=method,
│                   endpoint=path,
│                   status=status
│               ).inc()
│               
│               http_request_duration_seconds.labels(
│                   service=self.service_name,
│                   method=method,
│                   endpoint=path
│               ).observe(time.time() - start_time)
│           
│           def _normalize_path(self, path: str) -> str:
│               """Normalize paths to prevent high cardinality."""
│               # Replace UUIDs
│               path = re.sub(
│                   r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}',
│                   '{id}',
│                   path
│               )
│               # Replace numeric IDs
│               path = re.sub(r'/\d+', '/{id}', path)
│               return path
│       
│       
│       async def metrics_endpoint(request: Request) -> Response:
│           """Endpoint to expose Prometheus metrics."""
│           return Response(
│               content=generate_latest(),
│               media_type=CONTENT_TYPE_LATEST
│           )
│       
│       ```
│       
├── utils/
│   ├── __init__.py
│   ├── idempotency_key_generator.py
│   │   
│   │   ```py
│   │   # shared/utils/idempotency.py
│   │   """Simple idempotency key generator."""
│   │   
│   │   from typing import Union, Optional
│   │   from uuid import UUID
│   │   
│   │   
│   │   def generate_idempotency_key(
│   │       system: str,
│   │       operation_type: str, 
│   │       identifier: Union[str, int, UUID],
│   │       extra: Optional[str] = None
│   │   ) -> str:
│   │       """
│   │       Generate idempotency key: SYSTEM_OPERATION_ID[_EXTRA]
│   │       
│   │       Examples:
│   │           generate_idempotency_key("SHOPIFY", "ORDER", "123456") 
│   │           → "SHOPIFY_ORDER_123456"
│   │           
│   │           generate_idempotency_key("STRIPE", "PAYMENT", "pi_abc123")
│   │           → "STRIPE_PAYMENT_pi_abc123"
│   │           
│   │           generate_idempotency_key("SHOPIFY", "ORDER", "123", "TESTSTORE")
│   │           → "SHOPIFY_ORDER_123_TESTSTORE"
│   │       """
│   │       # Normalize inputs
│   │       system = str(system).upper().replace('-', '_').replace('.', '_')
│   │       operation_type = str(operation_type).upper().replace('-', '_').replace('.', '_')
│   │       identifier = str(identifier)
│   │       
│   │       # Build key
│   │       parts = [system, operation_type, identifier]
│   │       
│   │       if extra:
│   │           parts.append(str(extra).upper().replace('-', '_').replace('.', '_'))
│   │       
│   │       return '_'.join(parts)
│   │   ```
│   │   
│   └── logger.py
│       
│       ```py
│       # shared/utils/logger.py
│       import logging
│       import logging.handlers
│       import os
│       import json
│       from datetime import datetime
│       from typing import Dict, Any
│       from pathlib import Path
│       from contextvars import ContextVar
│       
│       
│       # Context variable for request-scoped data
│       request_context: ContextVar[Dict[str, Any]] = ContextVar('request_context', default={})
│       
│       
│       class ServiceLogger:
│           """
│           Service-specific logger that automatically includes service name and request context.
│           """
│           
│           def __init__(self, service_name: str):
│               self.service_name = service_name
│               self._setup_logging()
│               self._logger = logging.getLogger(service_name)
│           
│           def _setup_logging(self):
│               """Configure logging for this service"""
│               env = os.getenv("APP_ENV", "dev").lower()
│               log_level = os.getenv("LOG_LEVEL", "INFO").upper()
│               print(f"Setting up logger for {self.service_name} in {env} environment with level {log_level}")
│               
│               # Create logs directory
│               Path("logs").mkdir(exist_ok=True)
│               
│               # Configure formatters based on environment
│               if env == "prod":
│                   formatter = JsonFormatter(self.service_name)
│               else:
│                   formatter = ConsoleFormatter(self.service_name)
│               
│               # Set up handlers
│               console_handler = logging.StreamHandler()
│               console_handler.setFormatter(formatter)
│               console_handler.setLevel(log_level)
│               
│               # Configure the service logger
│               logger = logging.getLogger(self.service_name)
│               logger.setLevel(log_level)
│               logger.addHandler(console_handler)
│               
│               # Add file handler for production
│               if env == "prod":
│                   file_handler = logging.handlers.RotatingFileHandler(
│                       f"logs/{self.service_name}.log",
│                       maxBytes=10485760,  # 10MB
│                       backupCount=5,
│                       encoding='utf8'
│                   )
│                   file_handler.setFormatter(formatter)
│                   file_handler.setLevel(logging.INFO)
│                   logger.addHandler(file_handler)
│               
│               # Prevent propagation to avoid duplicate logs
│               logger.propagate = False
│           
│           def set_request_context(self, **kwargs):
│               """Set request-scoped context (e.g., request_id, user_id)"""
│               ctx = request_context.get()
│               ctx.update(kwargs)
│               request_context.set(ctx)
│           
│           def clear_request_context(self):
│               """Clear request context"""
│               request_context.set({})
│           
│           def _log(self, level: int, msg: str, *args, **kwargs):
│               """Internal log method that adds context"""
│               # Get request context
│               ctx = request_context.get()
│               
│               # Add context to extra
│               extra = kwargs.get('extra', {})
│               extra.update(ctx)
│               kwargs['extra'] = extra
│               
│               self._logger.log(level, msg, *args, **kwargs)
│           
│           def debug(self, msg: str, *args, **kwargs):
│               self._log(logging.DEBUG, msg, *args, **kwargs)
│           
│           def info(self, msg: str, *args, **kwargs):
│               self._log(logging.INFO, msg, *args, **kwargs)
│           
│           def warning(self, msg: str, *args, **kwargs):
│               self._log(logging.WARNING, msg, *args, **kwargs)
│           
│           def error(self, msg: str, *args, **kwargs):
│               self._log(logging.ERROR, msg, *args, **kwargs)
│           
│           def critical(self, msg: str, *args, **kwargs):
│               self._log(logging.CRITICAL, msg, *args, **kwargs)
│       
│       
│       class ConsoleFormatter(logging.Formatter):
│           """Console formatter that includes service name and request context"""
│           
│           def __init__(self, service_name: str):
│               self.service_name = service_name
│               super().__init__()
│           
│           def format(self, record):
│               # Build context string from extra fields
│               context_parts = []
│               request_id = getattr(record, 'request_id', None)
│               if request_id is not None:
│                   context_parts.append(f"request_id={request_id}")
│               user_id = getattr(record, 'user_id', None)
│               if user_id is not None:
│                   context_parts.append(f"user_id={user_id}")
│               
│               # Add any other extra fields
│               for key in record.__dict__:
│                   if key not in ['name', 'msg', 'args', 'created', 'filename', 'funcName',
│                                 'levelname', 'levelno', 'lineno', 'module', 'msecs',
│                                 'pathname', 'process', 'processName', 'relativeCreated',
│                                 'thread', 'threadName', 'exc_info', 'exc_text', 'stack_info',
│                                 'message', 'getMessage', 'request_id', 'user_id']:
│                       context_parts.append(f"{key}={getattr(record, key)}")
│               
│               # Format: 2024-01-15 10:30:45 - funding-service - INFO - [request_id=123] Processing order
│               timestamp = datetime.fromtimestamp(record.created).strftime('%Y-%m-%d %H:%M:%S')
│               
│               message = f"{timestamp} - {self.service_name} - {record.levelname}"
│               if context_parts:
│                   message += f" - [{' '.join(context_parts)}]"
│               message += f" - {record.getMessage()}"
│               
│               if record.exc_info:
│                   message += '\n' + self.formatException(record.exc_info)
│               
│               return message
│       
│       
│       class JsonFormatter(logging.Formatter):
│           """JSON formatter for production"""
│           
│           def __init__(self, service_name: str):
│               self.service_name = service_name
│               super().__init__()
│           
│           def format(self, record):
│               log_data = {
│                   "timestamp": datetime.utcnow().isoformat(),
│                   "service": self.service_name,
│                   "level": record.levelname,
│                   "message": record.getMessage(),
│                   "environment": os.getenv("APP_ENV", "dev"),
│               }
│               
│               # Add request context from extra
│               request_id = getattr(record, 'request_id', None)
│               if request_id is not None:
│                   log_data['request_id'] = request_id
│               user_id = getattr(record, 'user_id', None)
│               if user_id is not None:
│                   log_data['user_id'] = user_id
│               
│               # Add any other extra fields
│               for key in record.__dict__:
│                   if key not in ['name', 'msg', 'args', 'created', 'filename', 'funcName',
│                                 'levelname', 'levelno', 'lineno', 'module', 'msecs',
│                                 'pathname', 'process', 'processName', 'relativeCreated',
│                                 'thread', 'threadName', 'exc_info', 'exc_text', 'stack_info',
│                                 'message', 'getMessage', 'request_id', 'user_id']:
│                       log_data[key] = getattr(record, key)
│               
│               # Add exception if present
│               if record.exc_info:
│                   log_data['exception'] = self.formatException(record.exc_info)
│               
│               return json.dumps(log_data)
│       
│       
│       # Factory function to create service logger
│       def create_logger(service_name: str) -> ServiceLogger:
│           """
│           Create a logger for a specific service.
│           
│           Args:
│               service_name: Name of the service
│               
│           Returns:
│               ServiceLogger instance
│           """
│           return ServiceLogger(service_name)
│       
│       
│       # ============ USAGE ============
│       
│       """
│       USAGE:
│       
│       1. In your service initialization (main.py or app.py):
│       ```python
│       from shared.utils.logger import create_logger
│       
│       # Create service-specific logger
│       logger = create_logger("funding-service")
│       ```
│       
│       2. Basic logging:
│       ```python
│       logger.info("Service started")
│       logger.error("Connection failed", extra={"host": "localhost", "port": 5432})
│       ```
│       
│       3. In FastAPI middleware or request handler:
│       ```python
│       @app.middleware("http")
│       async def add_request_context(request: Request, call_next):
│           # Set request context for all logs in this request
│           logger.set_request_context(
│               request_id=request.headers.get("X-Request-ID", str(uuid.uuid4())),
│               method=request.method,
│               path=request.url.path
│           )
│           
│           logger.info("Request started")
│           response = await call_next(request)
│           logger.info("Request completed", extra={"status_code": response.status_code})
│           
│           # Clear context after request
│           logger.clear_request_context()
│           return response
│       ```
│       
│       4. In any route or service method:
│       ```python
│       @app.post("/api/orders")
│       async def create_order(order: Order, user_id: str = Depends(get_current_user)):
│           # Add user context
│           logger.set_request_context(user_id=user_id)
│           
│           logger.info("Creating order", extra={"order_id": order.id})
│           # ... business logic ...
│           logger.info("Order created successfully")
│       ```
│       
│       5. Output examples:
│       
│       Development:
│       2024-01-15 10:30:45 - funding-service - INFO - [request_id=abc123 user_id=456] - Creating order
│       
│       Production (JSON):
│       {"timestamp": "2024-01-15T10:30:45.123Z", "service": "funding-service", "level": "INFO", "message": "Creating order", "request_id": "abc123", "user_id": "456", "order_id": "789"}
│       """
│       ```
│       
└── __init__.py
tests/
└── __init__.py
.python-version
poetry.lock
pyproject.toml

```toml
# shared/pyproject.toml
[tool.poetry]
name = "shared"
version = "0.1.0"
description = "Shared utilities for GLAM system services"
authors = ["GLAM Team <team@glam.com>"]

[tool.poetry.dependencies]
python = "^3.11"
nats-py = "^2.6.0"
pydantic = "^2.5.0"
python-json-logger = "^2.0.7"
redis = "^5.0.1"
tenacity = "^8.2.3"
alembic = "^1.16.2"
pydantic-settings = "^2.10.1"
asyncio = "^3.4.3"
python-dotenv = "^1.1.1"
uuid7 = "^0.1.0"

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"
```

README.md

```md
# GLAM Shared Package

Shared utilities and infrastructure components for GLAM microservices platform.

## Features

- **Standardized API Responses**: Consistent response format across all services
- **Database Integration**: SQLAlchemy models, repository pattern, and session management  
- **Event-Driven Architecture**: Publishers, subscribers, and event context management
- **Error Handling**: Comprehensive error hierarchy with automatic mapping
- **Distributed Tracing**: Correlation IDs and trace context propagation
- **Metrics & Monitoring**: Prometheus metrics with automatic HTTP tracking
- **Structured Logging**: Service-aware logging with request context
- **Configuration Management**: YAML configuration with environment overrides
- **NATS Messaging**: JetStream integration with dependency injection

## Quick Start

### Installation

```bash
# Add to your service
cd your-service/
poetry add ../shared

# Or if published to registry
poetry add glam-shared
```

### Basic Service Setup

```python
# src/main.py
from fastapi import FastAPI
from shared.api import setup_middleware, create_health_router
from shared.utils.logger import create_logger

# Create logger and app
logger = create_logger("your-service")
app = FastAPI(title="Your Service")

# Setup essential middleware
setup_middleware(app, service_name="your-service")

# Add health endpoint
app.include_router(create_health_router("your-service"), prefix="/api/v1")
```

### Configuration

```yaml
# config/services/your-service.yml
database:
  host: "localhost"
  port: 5432
  name: "your_service_db"
  user: "postgres"
  password: "password"

nats:
  servers: ["nats://localhost:4222"]
```

## Core Components

### API Module
- **Middleware**: Automatic request/response handling, tracing, metrics
- **Dependencies**: Pagination, request context, correlation IDs
- **Responses**: Standardized success, error, and paginated responses
- **Health Checks**: Built-in health endpoints

### Database Module
- **Base Models**: Async SQLAlchemy with automatic mixins
- **Repository Pattern**: Generic CRUD operations with extensibility
- **Session Management**: Async session handling with proper cleanup
- **Migrations**: Alembic integration utilities

### Event System
- **Publishers**: Domain-specific event publishing with validation
- **Subscribers**: Event processing with dependency injection
- **Context Management**: Correlation and trace propagation
- **Pre-built Events**: Common event types (notifications, billing, etc.)

### Error Handling
- **Error Hierarchy**: Structured domain and infrastructure errors
- **Automatic Mapping**: Convert external exceptions to domain errors
- **Service-Specific**: Ready-to-use errors for common domains
- **API Integration**: Automatic error response formatting

### Messaging (NATS)
- **JetStream Wrapper**: Simplified publisher/subscriber setup
- **Dependency Injection**: Service dependencies for event handlers
- **Stream Management**: Automatic stream creation and configuration
- **Health Monitoring**: Connection health checks

## Available Mixins & Base Classes

### Database Mixins
```python
# Automatic timestamps
class TimestampedMixin:
    created_at: Mapped[datetime]  # Auto-set
    updated_at: Mapped[datetime]  # Auto-updated

# Multi-tenant support  
class MerchantMixin:
    merchant_id: Mapped[UUID]      # Indexed
    merchant_domain: Mapped[str]   # Indexed

# Soft delete support
class SoftDeleteMixin:
    deleted_at: Mapped[datetime | None]
    is_deleted: Mapped[bool]       # Indexed
```

### Repository Base
```python
class Repository(Generic[T]):
    # Available methods (all async)
    async def save(self, instance: T) -> T
    async def find_by_id(self, id: UUID) -> T | None  
    async def find_all(self, **filters) -> list[T]
    async def delete_by_id(self, id: UUID) -> None
    # Override for custom queries
```

### Event Base Classes
```python
class DomainEventPublisher:
    # Set these properties
    domain_stream: Streams = Streams.YOUR_DOMAIN
    service_name_override: str = "your-service"
    
    # Available methods
    async def publish_event(subject: str, payload: dict) -> str
    async def publish_command(command_type: str, payload: dict) -> str

class DomainEventSubscriber:
    # Must implement
    async def on_event(self, event: dict, headers: dict) -> None
    
    # Available methods  
    def get_dependency(self, key: str) -> Any
```

## Error Types

### Common Domain Errors (Ready to Use)
- `NotFoundError` - Resource not found (404)
- `ValidationError` - Invalid input (422)
- `ConflictError` - Resource conflicts (409)
- `UnauthorizedError` - Authentication required (401)
- `ForbiddenError` - Insufficient permissions (403)

### Infrastructure Errors (Ready to Use)
- `DatabaseError` - Database operation failed
- `UpstreamServiceError` - External service failure
- `MessageBusError` - NATS/messaging failure
- `S3Error` - Storage operation failed

### Service-Specific Collections
```python
# Import pre-built error collections
from shared.errors import (
    # Catalog errors
    SyncInProgressError, ItemNotFoundError,
    
    # Profile errors  
    ProfileNotFoundError, ProfileAlreadyExistsError,
    
    # Notification errors
    TemplateNotFoundError, EmailProviderError
)
```

## Event Streams

Each service publishes to designated streams:

```python
# Available streams
Streams.CATALOG      # catalog-service, catalog-connector
Streams.MERCHANT     # merchant-service
Streams.BILLING      # billing-service  
Streams.CREDIT       # credit-service
Streams.PROFILE      # profile-service
Streams.NOTIFICATION # notification-service
Streams.AI_PROCESSING # AI services
Streams.WEBHOOKS     # webhook-service
Streams.SCHEDULER    # scheduler-service
Streams.ANALYTICS    # analytics-service
```

## Configuration

### Environment Variables

All services support these patterns:

```bash
# Database
{SERVICE}_DB_HOST=localhost
{SERVICE}_DB_PORT=5432
{SERVICE}_DB_NAME=service_db
{SERVICE}_DB_USER=postgres
{SERVICE}_DB_PASSWORD=password

# Messaging  
{SERVICE}_NATS_SERVERS=nats://localhost:4222

# Logging
{SERVICE}_LOG_LEVEL=INFO
APP_ENV=dev  # dev, staging, prod
```

### YAML Configuration
```yaml
# config/services/your-service.yml
service:
  name: "your-service"
  port: 8080

database:
  host: "localhost"
  port: 5432
  name: "your_service_db"

nats:
  servers: ["nats://localhost:4222"]
```

## Development

### Testing
```bash
# Run tests
poetry run pytest

# With coverage
poetry run pytest --cov=shared
```

### Code Quality
```bash
# Format code
poetry run black .
poetry run isort .

# Type checking  
poetry run mypy shared/

# Linting
poetry run ruff check shared/
```

### Documentation
```bash
# Generate API docs
poetry run pdoc shared --html
```

## Monitoring & Observability

### Built-in Metrics (Automatic)
- `http_requests_total` - Request counts by service/endpoint/status
- `http_request_duration_seconds` - Request timing histograms  
- `http_requests_in_progress` - Active request gauge

### Health Checks (Automatic)
- `GET /api/v1/health` - Service health with timestamp
- `GET /metrics` - Prometheus metrics endpoint

### Logging Features
- **Structured Logging**: JSON in production, console in development
- **Request Context**: Automatic request_id, correlation_id, trace_id
- **File Rotation**: Automatic log rotation in production
- **Environment-Aware**: Different formats per environment

## Architecture Patterns

The shared package enforces consistent patterns across services:

- **Event-Driven Design**: All inter-service communication through events
- **Repository Pattern**: Standardized data access with extensibility
- **Dependency Injection**: Clean service dependencies via messaging wrapper
- **Domain Errors**: Business logic errors separate from infrastructure
- **Request Tracing**: End-to-end request tracking with correlation IDs
- **Multi-Tenancy**: Built-in merchant isolation and indexing

## Usage Examples

### Complete Service Setup
```python
from shared.api import setup_middleware
from shared.database import create_database_config, DatabaseSessionManager
from shared.messaging import JetStreamWrapper

# Database setup
db_config = create_database_config("YOUR_SERVICE_")
db_manager = DatabaseSessionManager(db_config.database_url)
await db_manager.init()

# Messaging setup  
messaging = JetStreamWrapper(logger)
await messaging.connect(["nats://localhost:4222"])
publisher = messaging.create_publisher(YourPublisher)

# Middleware setup
setup_middleware(app, service_name="your-service")
```

### Event Publishing
```python
# Publish with automatic correlation
await publisher.publish_event(
    subject="evt.item.created",
    payload={"item_id": str(item.id)},
    correlation_id=get_correlation_context(),
    idempotency_key=generate_idempotency_key("INTERNAL", "ITEM_CREATED", item.id)
)
```

### Error Handling
```python
from shared.errors import NotFoundError, ValidationError

# Use specific error types
if not item:
    raise NotFoundError(
        f"Item {item_id} not found",
        resource="item",
        resource_id=str(item_id)
    )
```

## Dependencies

- **FastAPI** - Web framework integration
- **SQLAlchemy** - Async database ORM
- **Pydantic** - Data validation and serialization
- **NATS.py** - JetStream messaging
- **Prometheus Client** - Metrics collection
- **Alembic** - Database migrations
- **PyYAML** - Configuration management

## License

Copyright © 2025 GlamYouUp. All rights reserved.
```


================================================================================
Output includes file contents
================================================================================